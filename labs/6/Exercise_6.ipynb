{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6: Protein sequence generation with transformers\n",
    "\n",
    "In this exercise, you will delve into protein sequences, which are composed of amino acids. Understanding and predicting protein sequences is crucial in fields such as bioinformatics, drug discovery, and structural biology.\n",
    "\n",
    "Generative pretraining using transformer-based models has proven to be a powerful approach for natural language understanding and generation tasks. In this exercise, we will adapt this approach to the domain of protein sequences. You will learn how to build a transformer model capable of predicting the next amino acid in a protein sequence and then train it on a dataset of protein sequences.\n",
    "\n",
    "To do this, the following exercise is structured as follows:\n",
    "1. Protein Sequences: load the data and explore it\n",
    "2. Masked Self-Attention: implement masked self-attention\n",
    "3. Transformer Model: build a small transformer model\n",
    "4. Training: train a transformer model on the protein sequences to do *next-token prediction* (predict the next token, or \"word\", in a sequence)\n",
    "5. Inference and Generation: use the trained model to generate new protein sequences\n",
    "\n",
    "First, let's import some useful libraries. Note that you will have to write code in our custom modules!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import our custom modules\n",
    "from data import *\n",
    "from model import *\n",
    "from train import *\n",
    "from utils import *\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data: protein sequences\n",
    "\n",
    "Our data is made of protein sequences, where each protein is described by a sequence of Amino Acids (AA). Each unique amino acid is represented by a unique letter of the [alphabet](https://www.bioinformatics.org/sms/iupac.html).\n",
    "\n",
    "Download the data from [here](https://drive.google.com/file/d/1US0GzmJ2rcy4Uo0Kfo7KQOnPYoGIORCF/view?usp=drive_link), then extract it and load it below. \n",
    "\n",
    "The proteins are from eukaryotes, filtered to be of length 40-200 AA, and were originally taken from [UniProt](https://www.uniprot.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = load_json(\"data/sequences.json\")\n",
    "\n",
    "print(f\"Number of protein sequences: {len(sequences)}\")\n",
    "print(f\"A few examples:\", *sequences[:4], sep=\"\\n  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some data statistics\n",
    "# Lengths of the sequences\n",
    "seq_lens = [len(seq) for seq in sequences]\n",
    "print(f\"Min length: {min(seq_lens)}\")\n",
    "print(f\"Max length: {max(seq_lens)}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Sequence lengths (in Amino Acids)\")\n",
    "plt.hist(seq_lens, bins=100)\n",
    "plt.xlabel(\"Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Amino acids and frequencies\n",
    "aa_freq = get_token_freq(sequences)\n",
    "aa_sorted = sorted(aa_freq.keys(), key=lambda x: aa_freq[x], reverse=True)  # sorted by frequency\n",
    "print(f\"Number of different amino acids: {len(aa_freq)}\")\n",
    "print(f\"Total number of amino acids: {sum(aa_freq.values()):,}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Frequency of amino acids\")\n",
    "plt.bar(range(len(aa_freq)), [aa_freq[aa] for aa in aa_sorted], align=\"center\", tick_label=aa_sorted)\n",
    "plt.xlabel(\"Amino acid\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we must prepare the vocabulary, *i.e.*, all possible tokens that the model can take as input and predict. To do this, we consider each unique amino acid as a token, and we also add a few special tokens `<bos>`, `<eos>`, `<pad>`:\n",
    "* `<bos>`: begining of sequence. It is needed to generate the first AA, otherwise the initial input sequence would be empty!\n",
    "* `<eos>`: end of sequence. It declares when a protein sequence ends, so that our model can predict sequences of different lengths.\n",
    "* `<pad>`: padding token. It is used to indicate if a position is simply a padding instead of part of the sequence. It is useful to batch sequences of different length together, very similarly to batching images of different size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary by listing all amino acids (in order of frequency here), and adding the special tokens\n",
    "vocab = [\"<pad>\", \"<bos>\", \"<eos>\"] + aa_sorted\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Vocabulary:\", *vocab)\n",
    "\n",
    "# Build mapping from token to indices (and vice-versa)\n",
    "token2idx = {token: i for i, token in enumerate(vocab)}\n",
    "idx2token = {i: token for i, token in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Implement a dataset\n",
    "\n",
    "You are tasked with writing a PyTorch `Dataset` for the protein sequences. Fill in the `ProteinDataset` class as described in the `data.py` file.\n",
    "\n",
    "**Note:** the input and target sequence should be shifted by one token: for positiong $i^{\\mathrm{th}}$, we input the $i^{\\mathrm{th}}$ token and want to predict the $i^{\\mathrm{th}}+1$ token. For instance, consider the sentence \"Quelle belle journ√©e\", with the added tokens `<bos>` and `<eos>`:\n",
    "\n",
    "<img src=\"img/input_target.png\" width=\"450\" />\n",
    "\n",
    "You can then use the following cells to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the dataset\n",
    "n_seq = 10\n",
    "dataset = ProteinDataset(sequences[:n_seq], token2idx)\n",
    "assert len(dataset) == n_seq\n",
    "\n",
    "idx = 0\n",
    "input_ids, target_ids = dataset[idx]\n",
    "input_tokens, target_tokens = [idx2token[i] for i in input_ids], [idx2token[i] for i in target_ids]\n",
    "print(f\"Original sequence:                \", *sequences[idx])\n",
    "print(f\"Input sequence from dataset:\", *input_tokens)\n",
    "print(f\"Target sequence from dataset:     \", *target_tokens)\n",
    "assert input_tokens[0] == \"<bos>\" and target_tokens[-1] == \"<eos>\", \"Did you add the special tokens?\"\n",
    "assert sequences[idx] == \"\".join(input_tokens[1:]) == \"\".join(target_tokens[:-1]), \"The sequences are not equivalent.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the batch padding function with a dummy batch\n",
    "batch = [\n",
    "    [token2idx[i] for i in ['<bos>', 'A', 'B', 'C', '<eos>']],\n",
    "    [token2idx[i] for i in ['<bos>', 'A', 'B', '<eos>']],\n",
    "    [token2idx[i] for i in ['<bos>', 'A', '<eos>']],\n",
    "    [token2idx[i] for i in ['<bos>', 'A', 'B', 'C', 'D', '<eos>']],\n",
    "]\n",
    "batch2 = [(b[:-1], b[1:]) for b in batch]\n",
    "input_ids, target_ids = dataset.padding_batch(batch2)\n",
    "assert input_ids.shape == target_ids.shape, \"The input and target tensors should have the same shape.\"\n",
    "\n",
    "# Do you obtain what you expect? Is the padding correct?\n",
    "# Hint: all the inputs should start with <bos> and all targets should end with <eos> (except for the padding)\n",
    "print(\"Input batch:\")\n",
    "print(*[[idx2token[i] for i in x] for x in input_ids.tolist()], sep=\"\\n\")\n",
    "print(\"Target batch:\")\n",
    "print(*[[idx2token[i] for i in x] for x in target_ids.tolist()], sep=\"\\n\")\n",
    "assert (input_ids[0] == torch.tensor([1, 5, 25, 19, 0])).all(), \"The first input sequence is not correct.\"\n",
    "assert (target_ids[0] == torch.tensor([5, 25, 19, 2, 0])).all(), \"The first target sequence is not correct.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transformer model\n",
    "\n",
    "In this part, you will implement masked self-attention, a transformer layer, and finally a transformer network with these building blocks.\n",
    "\n",
    "Let's first start with masking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Masking\n",
    "\n",
    "Recall from your lecture about the transformer decoder: when computing the self-attention, we do not want a given position to have access to the next position otherwise the model can simply cheat by looking at the next token to know what to predict!\n",
    "\n",
    "Therefore, we will build a mask to hide these \"future positions\" when computing the attention.\n",
    "\n",
    "<img src=\"img/attention_mask.png\" width=\"350\" />\n",
    "\n",
    "Additionally, we will also mask the `<pad>` tokens as these are simply there to batch sequences together, but are not useful for our protein generation task.\n",
    "\n",
    "Fill the function `get_mask()` in the file `utils.py`, and test it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a dummy input without any padding, test the masking function\n",
    "input_ids = torch.ones(2, 5, dtype=torch.long).to(device)\n",
    "\n",
    "# Compute the attention mask (no padding here). Do you obtain what you expect?\n",
    "# Hint: the mask should be the same for each sequence of the batch (because there is no padding here)\n",
    "mask = get_mask(input_ids, token2idx[\"<pad>\"])\n",
    "print(mask)\n",
    "assert mask.shape == (2, 5, 5), \"The mask should be of shape (batch_size, seq_len, seq_len).\"\n",
    "assert (mask[0, 1] == torch.BoolTensor([True, True, False, False, False])).all(), \"The mask is not correct.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Masked self-attention\n",
    "\n",
    "Now that we have a function to compute a mask, let's use it in the computation of the self-attention.\n",
    "\n",
    "Here, you have to implement two modules:\n",
    "1. `ScaledDotProductAttention`: implement masked self-attention, given queries, keys, and values $Q$, $K$, and $V$, and $d_k$ the dimension of these vectors:\n",
    "$$\n",
    "\\mathrm{Attention}(Q,K,V)=\\mathrm{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "1. `SingleHeadAttention`: implement the full attention layer. Note: here we consider a **unique head** to simplify, so $h=1$, there is no concatenation, and the linear layers actually project from $d_k$ to $d_k$.\n",
    "\n",
    "<img src=\"img/self-attention.png\" width=\"600\"/>\n",
    "\n",
    "For the masking, the goal is to have an attention of $0$ for the masked position (where `mask==0`). A way of doing this is to replace the attention scores (pre-softmax) that are masked by $-\\infty$, then they will become $0$ after we apply the exponential within the softmax!   \n",
    "*Hint*: in python, you can use `float(\"-inf\")` for $-\\infty$.\n",
    "\n",
    "You can find both class in `model.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 4\n",
    "attention = SingleHeadAttention(d_model)\n",
    "q = torch.randn(2, 5, d_model).to(device)\n",
    "k = torch.randn(2, 5, d_model).to(device)\n",
    "v = torch.randn(2, 5, d_model).to(device)\n",
    "output = attention(q, k, v, mask)\n",
    "print(output.shape)\n",
    "assert output.shape == (2, 5, d_model), \"The output should be of shape (batch_size, seq_len, d_model).\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Transformer layer\n",
    "\n",
    "With the above module, write a transformer layer as shown below: (inputs enter at the bottom, outputs leave at the top)\n",
    "\n",
    "<img src=\"img/transformer_layer.png\" width=\"200\" />\n",
    "\n",
    "A few remarks:\n",
    "1. the input `x` of the layer is used as query, key, and value for the `SingleHeadAttention` module,\n",
    "2. for the feed-forward part, use two linear layers with a ReLU in between, the hidden dimension should be `feedforward_dim`\n",
    "3. the dropout is applied after the attention and the feed-forward layers, but before the residual connection and layer norm.\n",
    "\n",
    "Fill in the class `TransformerLayer` in `model.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 4\n",
    "feedforward_dim = 8\n",
    "transformer_layer = TransformerLayer(d_model, feedforward_dim, 0.1)\n",
    "input = torch.randn(2, 5, d_model).to(device)\n",
    "output = transformer_layer(input)\n",
    "print(output.shape)\n",
    "assert output.shape == (2, 5, d_model), \"The output should be of shape (batch_size, seq_len, d_model).\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Model\n",
    "\n",
    "Finally, let's implement a transformer model that we will train for next-token prediction.\n",
    "This model will need:\n",
    "1. Embedding layers: we need to embed both tokens and position. Here, we will use learnable embeddings for both, see [`nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html).\n",
    "2. Some transformer layers as defined before.\n",
    "3. A final output layer to predict token probability **logits** at each position, we use a simple linear layer for this. **Note** that the softmax will not be part of the model, unlike the drawing below.\n",
    "\n",
    "<img src=\"img/transformer.png\" width=\"200\" />\n",
    "\n",
    "Fill in the `__init__()` and `forward()` function of the `Transformer` class in `model.py`. You can leave `generate_sequence()` for later.\n",
    "\n",
    "**Note:** if you want to initialize a list of layers within a model, make sure to wrap them in a [`nn.ModuleList`](https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html), otherwise PyTorch will not find them correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 4\n",
    "feedforward_dim = 8\n",
    "num_layers = 2\n",
    "max_seq_len = 5\n",
    "transformer = Transformer(vocab_size, token2idx['<pad>'], d_model, feedforward_dim, num_layers, 0.1, \n",
    "                          device, max_seq_len, token2idx, idx2token)\n",
    "input_ids = torch.randint(3, 10, (2, 5), dtype=torch.long).to(device)\n",
    "output = transformer(input_ids)\n",
    "print(output.shape)\n",
    "assert output.shape == (2, 5, vocab_size), \"The output should be of shape (batch_size, seq_len, vocab_size).\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training\n",
    "\n",
    "Let's now train our transformer to generate protein sequences!\n",
    "\n",
    "We have implemented for you a training and evaluation function for you, in `train.py`. You can take a look to see how they work.\n",
    "\n",
    "Below, you can choose between a `DEBUG` mode to use a small model and a subset of the data, or `FULL` to use the entire data and a larger model. We advise you to stay in debug when completing the exercise, and only do the full at home if you are interested (you will need a GPU for this, *e.g.*, Google Colab).\n",
    "\n",
    "Note that in debug mode, we use only a few sequences. Therefore, you should expect that the model won't be able to generalize past the training set. In that mode, we advise you to simply try to overfit the training data to then see the generative capability of the model in what follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 202  # maximum length of 200 AA + 2 for <bos> and <eos> tokens\n",
    "TRAIN_RATIO = 0.8  # fraction of training samples\n",
    "VALID_RATIO = 0.1  # fraction of validation samples (and rest is test)\n",
    "MODE = \"DEBUG\"  #¬†DEBUG or FULL. Use DEBUG to implement the exercise, you may try FULL on colab (with some time in front of you!)\n",
    "\n",
    "# Hyperparameters\n",
    "config = {\n",
    "    \"DEBUG\": {  # you can expect ~40-50% acc on train with these hyperparameters (and ~10% on valid)\n",
    "        \"epochs\": 100,\n",
    "        \"batch_size\": 32,\n",
    "        \"subsample\" : 250,  # number of sequences to subsample for faster training\n",
    "        \"lr\": 1e-3,\n",
    "        \"d_model\": 64,\n",
    "        \"feedforward_dim\": 256,\n",
    "        \"num_layers\": 3,\n",
    "    },\n",
    "    \"FULL\": {  # you may try these hyperparameters on colab (around ~1h30 for 50% acc on train and 35% on valid)\n",
    "        \"epochs\": 100,\n",
    "        \"batch_size\": 32,\n",
    "        \"subsample\" : None,\n",
    "        \"lr\": 1e-4,\n",
    "        \"d_model\": 512,\n",
    "        \"feedforward_dim\": 2048,\n",
    "        \"num_layers\": 6,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Data\n",
    "train_sequences, valid_sequences, test_sequences = split_sequences(\n",
    "    sequences, TRAIN_RATIO, VALID_RATIO, subsample=config[MODE][\"subsample\"],\n",
    ")\n",
    "train_dataset = ProteinDataset(train_sequences, token2idx)\n",
    "valid_dataset = ProteinDataset(valid_sequences, token2idx)\n",
    "test_dataset = ProteinDataset(test_sequences, token2idx)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[MODE][\"batch_size\"], shuffle=True, collate_fn=train_dataset.padding_batch)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=config[MODE][\"batch_size\"], shuffle=False, collate_fn=valid_dataset.padding_batch)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config[MODE][\"batch_size\"], shuffle=False, collate_fn=test_dataset.padding_batch)\n",
    "\n",
    "# Model\n",
    "model = Transformer(\n",
    "    vocab_size, \n",
    "    pad_idx=token2idx['<pad>'],\n",
    "    d_model=config[MODE][\"d_model\"], \n",
    "    feedforward_dim=config[MODE][\"feedforward_dim\"], \n",
    "    num_layers=config[MODE][\"num_layers\"], \n",
    "    dropout=0.1, \n",
    "    device=device,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    token2idx=token2idx,\n",
    "    idx2token=idx2token\n",
    ")\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config[MODE][\"lr\"])\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=token2idx['<pad>']).to(device)  # ignore the padding in the loss computation\n",
    "\n",
    "# Training loop\n",
    "history = training_loop(\n",
    "    config[MODE][\"epochs\"], \n",
    "    model, \n",
    "    train_loader, \n",
    "    valid_loader, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    device, \n",
    "    token2idx['<pad>'], \n",
    "    vocab_size\n",
    ")\n",
    "\n",
    "fig, axs = plt.subplots(1, 3,  figsize=(12, 4))\n",
    "axs[0].set_title('Loss')\n",
    "axs[0].plot(history['loss'], label='train')\n",
    "axs[0].plot(history['val-loss'], label='valid')\n",
    "axs[1].set_title('Perplexity')\n",
    "axs[1].plot(history['ppl'], label='train')\n",
    "axs[1].plot(history['val-ppl'], label='valid')\n",
    "axs[2].set_title('Accuracy')\n",
    "axs[2].plot(history['acc'], label='train')\n",
    "axs[2].plot(history['val-acc'], label='valid')\n",
    "for ax in axs.flat:\n",
    "    ax.set(xlabel='Epoch')\n",
    "    ax.grid()\n",
    "    ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation\n",
    "\n",
    "In this section, we explore what our model has learned and its ability to generate new sequences.\n",
    "\n",
    "### 4.1. Visualize some next-token predictions\n",
    "\n",
    "First, let's see some prediction results on example protein sequences from the train and validation sets. This is the same setting as in training: to predict a token at position $t$, the model sees all previous tokens up to position $t-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the predictions of the model with the ground truth\n",
    "model.eval()\n",
    "for name, dataset in [(\"Training\", train_dataset), (\"Validation\", valid_dataset)]:\n",
    "    idx = np.random.choice(len(dataset))\n",
    "    input_ids, target_ids = dataset[idx]\n",
    "    with torch.no_grad():\n",
    "        logits = model(torch.LongTensor([input_ids]).to(device))\n",
    "    preds = torch.argmax(logits, dim=-1).squeeze(0).detach().cpu().numpy()\n",
    "    print(f\"\\n{name} protein sequence {idx}:\")\n",
    "    print(\"  - Target:   \", \" \".join([idx2token[i] for i in target_ids]))\n",
    "    print(\"  - Predicted:\", \" \".join([idx2token[i] for i in preds]))\n",
    "    print(\"  - Accuracy:\", np.mean(preds == target_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Generate new protein sequences\n",
    "\n",
    "Our model can also be used as a purely generative model, *i.e.*, it can generate new sequences from scratch! To do this:\n",
    "1. we initialize a sequence with the `<bos>` token\n",
    "2. use the model to predict the next token\n",
    "3. if the new predicted token is `<eos>`, or we have generated the maximum number of tokens, then the sequence is finished and we stop\n",
    "4. else, append this new token to the input sequence and go back to step 2\n",
    "\n",
    "Such a model that uses its previous output as input is called **auto-regressive**.\n",
    "\n",
    "Implement the function `Transformer.generate_sequence()` in `model.py` to do this auto-regressive generation! We will choose the next token in a probabilistic way, by sampling it based on the predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = model.generate_sequence()\n",
    "print(f\"Generated sequence of length {len(seq)}:\")\n",
    "print(*seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Test evaluation\n",
    "\n",
    "Finally, let's see the model's performance on the unseen test set! (In the `DEBUG` mode, the performace might again be poor.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = evaluate(model, test_loader, criterion, device, token2idx['<pad>'], vocab_size)\n",
    "test_ppl = math.exp(test_loss)\n",
    "print(f\"Test loss={test_loss:.4f} - Test ppl={test_ppl:.4f} - Test acc={test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs502",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
