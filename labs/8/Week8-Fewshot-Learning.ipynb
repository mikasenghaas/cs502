{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8: Dive into Few-Shot Learning ðŸš€\n",
    "\n",
    "In this exercise, you'll:\n",
    "\n",
    "1. Learn about the few-shot-learning concept and ways to tackle it.\n",
    "2. Get a walkthrough of the `Few-Shot-Bench` code base. This code base offers a structured way to compare various methods meta-learning methods an various datasets. Depending on the project you choose, this might be helpful.\n",
    "\n",
    "Let's get started and explore this together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "# 0.0 Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod, ABC\n",
    "import os\n",
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from fcnet import FCNet\n",
    "import display_utils\n",
    "import gdown\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://drive.google.com/u/0/uc?id=1a3IFmUMUXBH8trx_VWKZEGteRiotOkZS&export=download'\n",
    "\n",
    "if os.path.exists('swissprot.zip'):\n",
    "    print('File already downloaded.')\n",
    "else:\n",
    "    output = 'swissprot.zip'\n",
    "    gdown.download(url, output, quiet=False)\n",
    "    print('Download completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q swissprot.zip\n",
    "\n",
    "!rm -rf swissprot.zip\n",
    "clear_output()\n",
    "\n",
    "!mv data/swissprot/go-basic.obo ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_samples_using_ic, check_min_samples, get_mode_ids, encodings, get_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "\n",
    "# 1.0 [Protonet](https://arxiv.org/abs/1703.05175)\n",
    "\n",
    "Imagine trying to recognize and classify new objects after seeing them just once or a few times. It's a tough task for most machine learning models. This is called few-shot classification. Humans can often identify objects even after seeing them just once. So, how can we make machines do that too?\n",
    "\n",
    "Prototype networks work on a basic idea: objects of the same type should be similar to each other in some way. So, in a space where the data is mapped tp, objects from the same class are expected to cluster around a central point, or a 'prototype'. When a new object comes in, the network checks which prototype it's closest to and classifies it accordingly.\n",
    "\n",
    "This method not only works for cases where we have a few examples but also for zero-shot learning, where we have no prior examples at all! Instead, we use a high-level description to determine the prototype.\n",
    "\n",
    "In short, prototype networks help in classifying new objects by looking for similarities to known prototypes, making them both efficient and effective for tasks with limited data.\n",
    "\n",
    "## 1.1 Creating a Meta-Template for Few-Shot Tasks\n",
    "In this exercise, we'll focus on implementing the Protonet using the `Few-Shot-Bench` code base. This code base offers a structured way to compare various methods. It has a template designed to add new methods and assess their performance seamlessly.\n",
    "\n",
    "The template has a standard training and testing cycle for meta-learning techniques. To train and test our model, we have two primary tasks:\n",
    "\n",
    "Mapping our data to the embeddings space: This step is facilitated by the parse_features function. Through this function, we'll calculate the prototypes (or support) and queries (or embeddings) for each data point.\n",
    "\n",
    "Evaluating our model's performance: The correct function will help us with this. It measures how accurately our model classifies data.\n",
    "\n",
    "Your task is to complete the implementation of the `parse_features` and `correct` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaTemplate(nn.Module):\n",
    "    def __init__(self, backbone, n_way, n_support):\n",
    "        super(MetaTemplate, self).__init__()\n",
    "        self.n_way = n_way\n",
    "        self.n_support = n_support\n",
    "        self.n_query = -1  # (change depends on input)\n",
    "        self.feature = backbone\n",
    "        self.feat_dim = self.feature.final_feat_dim\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_forward(self, x, is_feature=False):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def set_forward_loss(self, x):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.feature.forward(x)\n",
    "        return out\n",
    "\n",
    "    def parse_feature(self, x):\n",
    "        '''\n",
    "        :param x: [n_way, n_support + n_query, **embedding_dim]\n",
    "        '''\n",
    "        x = Variable(x.to(self.device))\n",
    "        x = x.contiguous().view(self.n_way * (self.n_support + self.n_query), * x.size()[2:])\n",
    "        # Compute support and query feature.\n",
    "        z_all = ...\n",
    "\n",
    "        # Reverse the transformation to distribute the samples based on the dimensions of their individual categories and flatten the embeddings.\n",
    "        z_all = ...\n",
    "\n",
    "        # Extract the support and query features.\n",
    "        z_support = ...\n",
    "        z_query = ...\n",
    "\n",
    "        return z_support, z_query\n",
    "\n",
    "    def correct(self, x):\n",
    "        # Compute the predictions scores.\n",
    "        scores = ...\n",
    "\n",
    "        # Compute the top1 elements.\n",
    "        topk_scores, topk_labels = ...\n",
    "\n",
    "        # Detach the variables.\n",
    "        topk_ind = ...\n",
    "\n",
    "        # Create the category labels for the queries.\n",
    "        y_query = ...\n",
    "\n",
    "        # Compute number of elements that are correctly classified.\n",
    "        top1_correct = ...\n",
    "        return float(top1_correct), len(y_query)\n",
    "\n",
    "    def train_loop(self, epoch, train_loader, optimizer):\n",
    "        print_freq = 10\n",
    "\n",
    "        avg_loss = 0\n",
    "        for i, (x, _) in enumerate(train_loader):\n",
    "            self.n_query = x.size(1) - self.n_support\n",
    "            optimizer.zero_grad()\n",
    "            loss = self.set_forward_loss(x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss = avg_loss + loss.item()\n",
    "\n",
    "            if i % print_freq == 0:\n",
    "                print('Epoch {:d} | Batch {:d}/{:d} | Loss {:f}'.format(epoch, i, len(train_loader),\n",
    "                                                                        avg_loss / float(i + 1)))\n",
    "        return avg_loss/len(train_loader)\n",
    "    \n",
    "    def test_loop(self, epoch, test_loader, record=None, return_std=False):\n",
    "        acc_all = []\n",
    "\n",
    "        iter_num = len(test_loader)\n",
    "        for i, (x, _) in enumerate(test_loader):\n",
    "            self.n_query = x.size(1) - self.n_support\n",
    "            correct_this, count_this = self.correct(x)\n",
    "            acc_all.append(correct_this / count_this * 100)\n",
    "\n",
    "        acc_all = np.asarray(acc_all)\n",
    "        acc_mean = np.mean(acc_all)\n",
    "        acc_std = np.std(acc_all)\n",
    "        print(f'Epoch {epoch} | Test Acc = {acc_mean:4.2f}% +- {1.96 * acc_std / np.sqrt(iter_num):4.2f}%')\n",
    "\n",
    "        if return_std:\n",
    "            return acc_mean, acc_std\n",
    "        else:\n",
    "            return acc_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 ProtoNet Implementation\n",
    "Now we are ready to extend the MetaTemplate with the ProtoNet specific implementation.\n",
    "\n",
    "Your task is to write the code for two functions: `set_forward` and `set_forward_loss`.\n",
    "\n",
    "1. `set_forward`: This function should calculate the prototypes (referred to as `support`) and queries (referred to as `embeddings`) for every data point. After that, determine the **similarity** between these prototypes and queries.\n",
    "2. `set_forward_loss`: With this function, your goal is to calculate the model's loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtoNet(MetaTemplate):\n",
    "    def __init__(self, backbone, n_way, n_support):\n",
    "        super(ProtoNet, self).__init__(backbone, n_way, n_support)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def set_forward(self, x):\n",
    "        # Compute the prototypes (support) and queries (embeddings) for each datapoint.\n",
    "        # Remember that you implemented a function to compute this before.\n",
    "        z_support, z_query = ...\n",
    "            \n",
    "        # Compute the prototype.\n",
    "        z_support = z_support.contiguous().view(self.n_way, self.n_support, -1)\n",
    "        z_proto = ...\n",
    "        \n",
    "        # Format the queries for the similarity computation.\n",
    "        z_query = z_query.contiguous().view(self.n_way * self.n_query, -1)\n",
    "\n",
    "        # Compute similarity score based on the euclidean distance between prototypes and queries.\n",
    "        scores = ...\n",
    "        return scores\n",
    "\n",
    "    def set_forward_loss(self, x):\n",
    "        # Compute the similarity scores between the prototypes and the queries.\n",
    "        scores = ...\n",
    "        \n",
    "        # Create the category labels for the queries.\n",
    "        y_query = ...\n",
    "        y_query = Variable(y_query)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = ...\n",
    "        return loss\n",
    "\n",
    "def euclidean_dist( x, y):\n",
    "    # x: N x D\n",
    "    # y: M x D\n",
    "    n = x.size(0)\n",
    "    m = y.size(0)\n",
    "    d = x.size(1)\n",
    "    assert d == y.size(1)\n",
    "\n",
    "    x = x.unsqueeze(1).expand(n, m, d)\n",
    "    y = y.unsqueeze(0).expand(n, m, d)\n",
    "\n",
    "    return torch.pow(x - y, 2).sum(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "# 2.0 Dataset - [SwissProt](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC102476/)\n",
    "\n",
    "Now, let's dive into the world of proteins to test the power of the ProtoNet! ðŸ§¬\n",
    "\n",
    "Proteins are like intricate puzzles made up of smaller pieces known as amino acids. These tiny building blocks come together in a specific order to give a protein its unique structure and function in living organisms. It's like assembling LEGO blocks in a specific order to build a spaceship or a castle!\n",
    "\n",
    "Now, imagine we had a vast library of labels to describe each protein's function. Welcome to SwissProt dataset, which is annotated with Gene Ontology (GO) labels! The Gene Ontology Consortium (GOC), an international team of experts, crafted these labels to categorize the role, location, and function of genes across various species.\n",
    "\n",
    "The ontology includes over 47,000 terms (as of April 2018), each representing a specific characteristic or label. Think of these labels as descriptors that can be linked together. For instance, a protein might be described by its role in cell division, its location in the cell's nucleus, and its ability to bind to DNA. This results in a directed acyclic graph (DAG) structure of GO labels, where the nodes are the labels and the edges are the relationships between them. When a protein is linked to a specific label, it's also connected to all the labels that are ancestors in the DAG. This makes our machine learning challenge one of multi-label classification, meaning each protein can have multiple labels.\n",
    "\n",
    "For our exercise on fewshot learning, which mainly focuses on single-label classification, we need a method to assign just one label to each protein. We achieve this by choosing the most detailed label from the GO DAG for every protein. We've provided a procedure (get_samples_using_ic) that does this for you and also loads the dataset in the format you'll need.\n",
    "\n",
    "## 2.1 Creating a Dataset Template for the Few-Shot Task\n",
    "Let's keep our focus on our second objective: acquainting you with the `Few-Shot-Bench` code base. This library not only allows to measure the performance of various methods but also facilitates comparisons across multiple datasets. For this exercise, we'll start by examining the `FewShotDataset` template and SwissProt (SP) specialized version, the `SPDataset` before delving into the episodes data set for the SwissProt dataset, `SPSetDataset`, for the episodic training.\n",
    "\n",
    "**Note**: We're not expecting you to write any code for this. Instead, we want you to familiarize yourself with the code structure and understand the dataset's layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.check_init()\n",
    "\n",
    "    def check_init(self):\n",
    "        \"\"\"\n",
    "        Convenience function to check that the FewShotDataset is properly configured.\n",
    "        \"\"\"\n",
    "        required_attrs = ['_dataset_name', '_data_dir']\n",
    "        for attr in required_attrs:\n",
    "            if not hasattr(self, attr):\n",
    "                raise ValueError(f'FewShotDataset must have attribute {attr}.')\n",
    "\n",
    "        if not os.path.exists(self._data_dir):\n",
    "            raise ValueError(\n",
    "                f'{self._data_dir} does not exist yet. Please generate/download the dataset first.')\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def __getitem__(self, i):\n",
    "        return NotImplemented\n",
    "\n",
    "    @abstractmethod\n",
    "    def __len__(self):\n",
    "        return NotImplemented\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def dim(self):\n",
    "        return NotImplemented\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_data_loader(self, mode='train') -> DataLoader:\n",
    "        return NotImplemented\n",
    "\n",
    "    @property\n",
    "    def dataset_name(self):\n",
    "        \"\"\"\n",
    "        A string that identifies the dataset, e.g., 'swissprot'\n",
    "        \"\"\"\n",
    "        return self._dataset_name\n",
    "\n",
    "    @property\n",
    "    def data_dir(self):\n",
    "        return self._data_dir\n",
    "\n",
    "    def initialize_data_dir(self, root_dir):\n",
    "        os.makedirs(root_dir, exist_ok=True)\n",
    "        self._data_dir = os.path.join(root_dir, self._dataset_name)\n",
    "\n",
    "class SPDataset(FewShotDataset, ABC):\n",
    "    _dataset_name = 'swissprot'\n",
    "\n",
    "    def load_swissprot(self, level = 5, mode='train', min_samples = 20):\n",
    "        samples = get_samples_using_ic(root = self.data_dir)\n",
    "        samples = check_min_samples(samples, min_samples)\n",
    "        unique_ids = set(get_mode_ids(samples)[mode])\n",
    "        return [sample for sample in samples if sample.annot in unique_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Creating an Episode Dataset for the SwissProt Dataset\n",
    "\n",
    "\n",
    "In this exercise, you'll be working on creating an episode dataloader for the SwissProt dataset, which we need for training the ProtoNet model. Think of an episode dataset as a series of episodes, where each episode is made up of specific classes and their associated samples.\n",
    "\n",
    "Here's a breakdown:\n",
    "\n",
    "**Episode**: A group of n classes (n_way).\n",
    "\n",
    "**Each Class in an Episode**: Contains `k` support samples (`n_support`) used for training and `q` query (`n_query`) samples used for evaluating performance.\n",
    "\n",
    "Your task is to complete the episode dataloader `SPSetDataset` by creating a dataloaders for each category and implement the iterator function for the `EpisodicBatchSampler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROTDIM = 1280\n",
    "\n",
    "class SubDataset(Dataset):\n",
    "    def __init__(self, samples, data_dir):\n",
    "        self.samples = samples\n",
    "        self.encoder = encodings(data_dir)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        sample = self.samples[i]\n",
    "        return sample.input_seq, self.encoder[sample.annot]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return PROTDIM\n",
    "\n",
    "class EpisodicBatchSampler(object):\n",
    "    def __init__(self, n_classes, n_way, n_episodes):\n",
    "        self.n_classes = n_classes\n",
    "        self.n_way = n_way\n",
    "        self.n_episodes = n_episodes\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_episodes\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(self.n_episodes):\n",
    "            # Random sample the `n_way` classes for this episode.\n",
    "            # This is needed to return the indices for the selected category dataloaders for this episode.\n",
    "            yield ...\n",
    "\n",
    "class SPSetDataset(SPDataset):\n",
    "    def __init__(self, n_way, n_support, n_query, n_episode=100, root='./data', mode='train'):\n",
    "        self.initialize_data_dir(root)\n",
    "\n",
    "        self.n_way = n_way\n",
    "        self.n_episode = n_episode\n",
    "        min_samples = n_support + n_query\n",
    "        self.encoder = encodings(self.data_dir)\n",
    "\n",
    "        samples_all = self.load_swissprot(mode = mode, min_samples = min_samples)\n",
    "\n",
    "        self.categories = get_ids(samples_all) # Unique annotations\n",
    "        self.x_dim = PROTDIM\n",
    "\n",
    "        self.sub_dataloader = []\n",
    "\n",
    "        sub_data_loader_params = dict(batch_size=min_samples,\n",
    "                                      shuffle=True,\n",
    "                                      num_workers=0,  # use main thread only or may receive multiple batches\n",
    "                                      pin_memory=False)\n",
    "\n",
    "        # Create the sub datasets for each annotation of the categories and collect all the dataloaders in `self.sub_dataloader`.\n",
    "        ...\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # Return the next batch of the dataloader of the i-th category.\n",
    "        return ...\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.categories)\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self.x_dim\n",
    "\n",
    "    def get_data_loader(self) -> DataLoader:\n",
    "        sampler = EpisodicBatchSampler(len(self), self.n_way, self.n_episode)\n",
    "        data_loader_params = dict(batch_sampler=sampler, num_workers=0, pin_memory=True)\n",
    "        data_loader = torch.utils.data.DataLoader(self, **data_loader_params)\n",
    "        return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "\n",
    "# 3.0 Training and testing the model \n",
    "Now, we are going to train our Protonet model and see how it performs on the SwissProt dataset.\n",
    "\n",
    "**A heads up:** Loading the training dataset might take around 2 minutes and loading the test set about 40 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(n_way, n_support, n_query, n_train_episode):\n",
    "    # Load train dataset. Remember to use make use of functions defined in the `SPSetDataset`.\n",
    "    train_dataset = SPSetDataset(n_way, n_support, n_query, n_episode=n_train_episode, root='./data', mode='train')\n",
    "    train_loader = ...\n",
    "\n",
    "    # Load test dataset. Remember to use make use of functions defined in the `SPSetDataset`.\n",
    "    test_dataset = SPSetDataset(n_way, n_support, n_query, n_episode=100, root='./data', mode='test')\n",
    "    test_loader = ...\n",
    "\n",
    "    # Initialize a fully connected network `FCNet` in `fcnet.py` with two hidden layers of 512 units each as feature extractor.\n",
    "    backbone = ...\n",
    "\n",
    "    # Initialize model using the backbone and the optimizer.\n",
    "    model = ...\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    test_accs = []; train_losses = []\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "\n",
    "        # Implement training of the model. Remember to make use of functions defined in the `ProtoNet` and `MetaTemplate` class.\n",
    "        epoch_loss = ...\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        # Evaluate test performance for epoch. Remember to make use of functions defined in the `ProtoNet` and `MetaTemplate` class.\n",
    "        test_acc = ...\n",
    "        test_accs.append(test_acc)\n",
    "        print(f'Epoch {epoch} | Train Loss {epoch_loss} | Test Acc {test_acc}')\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 2.5))\n",
    "    ax1.plot(range(len(train_losses)), train_losses)\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Train Loss')\n",
    "    ax1.grid()\n",
    "\n",
    "    ax2.plot(range(len(test_accs)), test_accs)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Test Accuracy')\n",
    "    ax2.grid()\n",
    "    fig.suptitle(f\"n_way={n_way}, n_support={n_support}, n_query={n_query}, n_train_episode={n_train_episode}\")\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_way': 5, 'n_support': 5, 'n_query': 15, 'n_train_episode': 5}\n",
    "display_utils.sliders(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(**parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fewshotbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
