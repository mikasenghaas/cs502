{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQNjTEUVQTHW"
   },
   "source": [
    "# Exercise 4 - PyTorch Geometric Tutorial\n",
    "\n",
    "This exercise is to introduce you to **[PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/)** (PyG).\n",
    "\n",
    "PyG is a library built upon PyTorch to easily write and train Graph Neural Networks (GNNs).\n",
    "\n",
    "*Note*: you may not use PyTorch Geometric for Homework 2, except for data loading!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIsXMUa9RgS2"
   },
   "source": [
    "## 0. To install the required packages, run the cell below.\n",
    "\n",
    "**Note**: if you are running this notebook on your local machine and are using `conda`, you can comment out the `!pip install` lines below and instead run in your terminal:\n",
    "```bash\n",
    "conda install -y pyg -c pyg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ED57grjmQUVp",
    "outputId": "188fa068-f8b4-4c30-fc6a-ba8bd6fbacdc"
   },
   "outputs": [],
   "source": [
    "# Install required packages.\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dszt2RUHE7lW"
   },
   "source": [
    "# Node Classification with Graph Neural Networks\n",
    "\n",
    "Last week, you implemented GNNs to perform edge prediction by implementing a prediction head that performed a dot-product between the node embeddings. This week, we will explore another problem: node classification.\n",
    "\n",
    "<img src=\"img/node_class.png\" width=\"600\" />\n",
    "\n",
    "To demonstrate the abilities of the library, we make use of the [`torch_geometric.datasets.PPI`](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.PPI.html#torch_geometric.datasets.PPI) dataset, which contains **protein-protein interaction** networks from different human tissues. We have 20 networks for training, 2 networks for validation and 2 for testing. A node in each graph represents a protein and is described by a 50-dimensional feature vector reflecting different immunological signatures. The task is to predict protein roles in 121 different gene ontology sets. Thus, for each node $v$ we have label vector $y_v \\in \\{0, 1\\}^{121}$ leading to 121 binary classification problems.\n",
    "\n",
    "This is a different problem than you saw last week! Indeed, instead of having a single graph where we want to predict part of it, we here have multiple and want to do prediction on new sets of graphs.   \n",
    "$\\rightarrow$ We consider **inductive learning**, *i.e*, we are given the training set of graphs with ground-truth labels for each node in the graphs, and we want to infer the labels for the nodes of the test graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdcmzaLkR2aQ"
   },
   "source": [
    "## 1. Dataset and DataLoader\n",
    "\n",
    "First of all, load the train and test split of the PPI dataset and inspect them.\n",
    "[`torch_geometric.datasets.PPI`](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/datasets/ppi.html) and [`torch_geometric.loader.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) might be useful.\n",
    "\n",
    "To enable fast prototyping and testing let's use only the first 3 graphs for training. At home, you can use all 20 training graphs to get better performance.\n",
    "To easily create a subset of a dataset, you may want to use [`torch.utils.data.Subset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "imGrKO5YH11-",
    "outputId": "980999e0-40da-4c50-ba5f-af9f09bb36bb"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import PPI\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "dataset_train = PPI(...)  # TODO\n",
    "dataset_train = torch.utils.data.Subset(...)  # TODO\n",
    "trainloader = ...  # TODO\n",
    "dataset_test = PPI(...)  # TODO\n",
    "testloader = ...  # TODO\n",
    "\n",
    "print()\n",
    "print(f'Train set: {dataset_train.dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset_train)}')\n",
    "print(f'Number of features: {dataset_train.dataset.num_features}')\n",
    "print(f'Number of classes: {dataset_train.dataset.num_classes}')\n",
    "\n",
    "print()\n",
    "print(f'First element of the train set:\\n{dataset_train[0]}')\n",
    "print('===========================================================================================================')\n",
    "\n",
    "# Gather some statistics about the graph.\n",
    "print(f'Number of nodes: {dataset_train[0].num_nodes}')\n",
    "print(f'Number of edges: {dataset_train[0].num_edges}')\n",
    "print(f'Average node degree: {dataset_train[0].num_edges / dataset_train[0].num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {dataset_train[0].has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {dataset_train[0].has_self_loops()}')\n",
    "print(f'Is undirected: {dataset_train[0].is_undirected()}')\n",
    "\n",
    "\n",
    "print()\n",
    "print(f'First element of the test set:\\n{dataset_test[0]}')\n",
    "print('===========================================================================================================')\n",
    "\n",
    "# Gather some statistics about the graph.\n",
    "print(f'Number of nodes: {dataset_test[0].num_nodes}')\n",
    "print(f'Number of edges: {dataset_test[0].num_edges}')\n",
    "print(f'Average node degree: {dataset_test[0].num_edges / dataset_test[0].num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {dataset_test[0].has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {dataset_test[0].has_self_loops()}')\n",
    "print(f'Is undirected: {dataset_test[0].is_undirected()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the node embedding using `.x`, the edge indices using `.edge_index`, and the target class labels using `.y`. See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset_train[0]\n",
    "print(f'The node features have shape {data.x.shape}')\n",
    "print(f'The edge indices have shape {data.edge_index.shape}')\n",
    "print(f'The target labels have shape {data.y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gchkta4yTnLR"
   },
   "source": [
    "## 2. Simple Random prediction baseline\n",
    "\n",
    "As a very simple baseline, let's consider random predictions, *i.e.*, predicting 0 or 1 with probability 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwDzqDtVT0wS"
   },
   "outputs": [],
   "source": [
    "class SimpleRandomBaseline(torch.nn.Module):\n",
    "    \"\"\"Simple baseline that randomly assigns a label to each node.\"\"\"\n",
    "\n",
    "    def __init__(self, num_tasks):\n",
    "        \"\"\"\n",
    "        Initialize the baseline.\n",
    "        \n",
    "        Args:\n",
    "            num_tasks (int): Number of tasks (classes).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ...  # TODO\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Random prediction for each node.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input node features of shape (num_nodes, in_features).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Predicted labels of shape (num_nodes, num_tasks).\n",
    "        \"\"\"\n",
    "        return ...  # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gYmQ2WENUtuB"
   },
   "outputs": [],
   "source": [
    "m1 = SimpleRandomBaseline(dataset_train.dataset.num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d4pghcZVJ-1"
   },
   "source": [
    "## 3. Metric\n",
    "\n",
    "For each node, we have 121 binary classification problems. To assess the quality of the model, we will use micro-averaged F1 score, *i.e.*, the overall F1 score considering the total true/false positives/negatives. As a reminder, the F1 score for a single class is computed as follow:\n",
    "$$\n",
    "F_1 = \\frac{2 \\, \\mathrm{Precision} \\cdot \\mathrm{Recall}}{\\mathrm{Precision} + \\mathrm{Recall}} = \\frac{2TP}{2TP + FP + FN},\n",
    "$$\n",
    "with $TP$ the true positives, $FP$ false positives, and $FN$ false negatives. See more details on [Wikipedia](https://en.wikipedia.org/wiki/F-score#Dependence_of_the_F-score_on_class_imbalance).\n",
    "\n",
    "Below, compute the micro-averaged F1 score of the random baseline on the two graphs of the test set. You may use existing implementation, *e.g.*, [`sklearn.metrics.f1_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html).\n",
    "\n",
    "*Hint*: for the above random baseline, the F1 score should be around $\\sim0.37$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PCREeAQzUzrM"
   },
   "outputs": [],
   "source": [
    "# TODO: compute the micro-averaged F1 score on the random baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4p21_UrdORWZ"
   },
   "source": [
    "## 4. Simple linear model on raw node features\n",
    "\n",
    "As a slightly less naive baseline, let's consider a linear classifier operating directly on the raw node features.\n",
    "\n",
    "Implement such a model and compute its performance. You may use existing implementations, *e.g.*, [`sklearn.linear_model.LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) and [`sklearn.multioutput.MultiOutputClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) might be useful.\n",
    "\n",
    "*Hint*: for a simple linear model operating directly on the raw node features, the F1 score should be around $\\sim0.44$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115
    },
    "id": "Nlh4My7qOV46",
    "outputId": "a1f53052-1c3b-4425-be3b-fa8e0c4746bf"
   },
   "outputs": [],
   "source": [
    "# TODO: prepare and train a linear classifier on the node features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f1hF-KvjOY-H",
    "outputId": "dd457091-b9c8-4f27-a6fd-f14b1cfc801c"
   },
   "outputs": [],
   "source": [
    "# TODO: compute the micro-averaged F1 score on the linear classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IRdAELVKOl6"
   },
   "source": [
    "## 5. Multi-layer Perception Network on node features\n",
    "\n",
    "In theory, we should be able to infer the protein function solely based on its feature representation, without taking any information about interaction between proteins into account.\n",
    "\n",
    "Let's verify that by constructing a simple MLP that solely operates on input node features. Implement a simple MLP below, with two layers and ReLU activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "afXwPCA3KNoC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    \"\"\"Simple MLP with two hidden layers.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Initialize the MLP.\n",
    "        \n",
    "        Args:\n",
    "            in_channels (int): Number of input node features.\n",
    "            hidden_channels (int): Number of hidden units.\n",
    "            out_channels (int): Number of output features.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ...  # TODO\n",
    "\n",
    "    def forward(self, x, edge_index=None):\n",
    "        \"\"\"\n",
    "        Compute the node predictions.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input node features of shape (num_nodes, in_channels).\n",
    "            edge_index (LongTensor): Graph edge indices of shape (2, num_edges).\n",
    "                It is here for compatibility issue, but is not used.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Output node features of shape (num_nodes, out_channels).\n",
    "        \"\"\"\n",
    "        # edge_index is for compatibility, it's actually not used here.\n",
    "        ...  # TODO\n",
    "        return ...  # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_PO9EEHL7J6"
   },
   "source": [
    "Let's implement a `train` **function** to train a model and a `test` **function** to evaluate it.\n",
    "\n",
    "Do no hesitate to come back to the previous exercise if you need, a lot of this sort of implementation is similar throughout projects in PyTorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YgHcLXMLk4o"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, dataloader):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch on the dataloader.\n",
    "    \n",
    "    You should return the average loss over the dataloader (look into running loss).\n",
    "    \"\"\"\n",
    "    ...  # TODO\n",
    "    return ...  # TODO\n",
    "\n",
    "def test(model, dataloader):\n",
    "    \"\"\"Test the model on the dataloader and return the F1 score.\"\"\"\n",
    "    ...  # TODO\n",
    "    return ...  # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our MLP is defined by two linear layers and enhanced by [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html?highlight=relu#torch.nn.ReLU) non-linearity.\n",
    "Here, we first map the 50-dimensional feature vector to a hidden embedding (`hidden_channels=256`), while the second linear layer acts as a classifier that should map each low-dimensional node embedding to 121 binary classifiers.\n",
    "\n",
    "Test this simple MLP. As the loss function, we use the **binary cross entropy** (with logits) and as the optimizer, we use **Adam**.\n",
    "\n",
    "*Hint*: the layers above should give marginally better results than a simple linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZVV80sAim-NZ",
    "outputId": "bdc164f6-6b82-4b7c-f09e-18162d4c7f33",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = MLP(...)  # TODO\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()  # Define loss criterion.\n",
    "optimizer = torch.optim.Adam(...)  # TODO: Define optimizer.\n",
    "\n",
    "loss = []\n",
    "for epoch in range(1, 1001):\n",
    "    loss.append(\n",
    "        train(model, optimizer, criterion, trainloader)\n",
    "    )\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss[-1]:.4f}')\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title('Training loss')\n",
    "plt.plot(loss)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eK_FaaaPm-Pb",
    "outputId": "c39f0528-8868-498e-ff2c-51c32403b74a"
   },
   "outputs": [],
   "source": [
    "# Evaluate the performance of the model.\n",
    "train_acc = test(model, trainloader)\n",
    "print(f'Train F1: {train_acc:.4f}')\n",
    "\n",
    "test_acc = test(model, testloader)\n",
    "print(f'Test F1: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJatbWokRjNV"
   },
   "source": [
    "## 6. Graph Isomorphism Network on node features with protein-protein interaction information\n",
    "\n",
    "Now, let's use GNNs to make use of the graph structure! Let's start with [`torch.geometric.nn.GIN`](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.models.GIN.html#torch_geometric.nn.models.GIN), but do not hesitate to explore [`torch_geometric.nn`](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#models) to test various models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RVr8tLRUsrlL",
    "outputId": "d76893bf-f32d-4958-e4fa-674cbb29cb9e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GIN\n",
    "\n",
    "model = GIN(\n",
    "    in_channels=dataset_train.dataset.num_features, \n",
    "    hidden_channels=256, \n",
    "    num_layers=2, \n",
    "    out_channels=dataset_train.dataset.num_classes\n",
    ")\n",
    "print(model)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()  # Define loss criterion.\n",
    "optimizer = torch.optim.Adam(...)  # TODO: Define optimizer.\n",
    "\n",
    "loss = []\n",
    "for epoch in range(1, 1001):\n",
    "    loss.append(\n",
    "        train(model, optimizer, criterion, trainloader)\n",
    "    )\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss[-1]:.4f}')\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title('Training loss')\n",
    "plt.plot(loss)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qjj54cNRsrnL",
    "outputId": "c51f6163-d3e0-4d2f-d85a-8e5141cac1de"
   },
   "outputs": [],
   "source": [
    "# Evaluate the performance of the model.\n",
    "train_acc = test(model, trainloader)\n",
    "print(f'Train F1: {train_acc:.4f}')\n",
    "\n",
    "test_acc = test(model, testloader)\n",
    "print(f'Test F1: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the performances of the baseline to the MLP and GNN. How do they compare?\n",
    "\n",
    "*Hint*: as the GNN is more powerful, it may overfit more. However, you should still observe an improvement as it considers the connectivity between the nodes, *e.g.*, a test F1 score $>0.55$.\n",
    "\n",
    "We trained with only 3 graphs out of the 20 of the training set. At home and when you have time, try to increase the number of graphs you train on, and you should see an improvement of the performance on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paMH3_7ejSg4"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this exercise, you have seen how to apply GNNs to real-world problems using *PyTorch Geometric* library, and, in particular, how they can effectively be used for boosting a model's performance when graph structure is taken into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-q6Do4INLET"
   },
   "source": [
    "## (Optional) Exercises\n",
    "\n",
    "1. To achieve better model performance and to avoid overfitting, it is usually a good idea to select the best model based on an additional validation set.\n",
    "The `PPI` dataset provides a validation graph set as `split=\"val\"`. Try hyperparameter search!\n",
    "\n",
    "2. *PyTorch Geometric* has many different GNN architectures. Try to play with them!\n",
    "\n",
    "3. We trained only on 3 training graphs, but the whole dataset contains 20 graphs for the training. Try to train on the whole dataset to boost the performance!\n",
    "\n",
    "4. Visualize learnt node embeddings with respect one of the binary tasks to see whether embeddings have well clustered structure!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) GIN implementation\n",
    "Let's try to implement the GIN network ourselves in PyTorch.\n",
    "\n",
    "To do so, we need to implement:\n",
    "1. the [`GINConv`](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GINConv.html#torch_geometric.nn.conv.GINConv) convolutional layer,\n",
    "2. a prediction head operating at the node level, which we will implement with a simple linear transformation of the node embeddings,\n",
    "3. and the entire model which will consist in some `GINConv` followed by a prediction head."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `GINConv`, the formula in matrix form is given by:\n",
    "$$\n",
    "H^{(l+1)} = h_{\\Theta} \\left( \\left(\\mathbf{A} + (1+\\epsilon)\\,\\mathbf{I} \\right) \\cdot H^{(l)}\\right),\n",
    "$$\n",
    "where $h_{\\Theta}$ is an MLP with two layers and ReLU activation function, $\\mathbf{A}$ is the adjacency matrix, $\\mathbf{I}$ the identity matrix, and $H^{(l)}$ the node embeddings of layer $l$.\n",
    "\n",
    "Implement it below for `MyGINConv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGINConv(nn.Module):\n",
    "    \"\"\"GIN convolutional layer.\"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, epsilon=0.0):\n",
    "        \"\"\"\n",
    "        Initialize the GIN convolutional layer.\n",
    "        \n",
    "        Args:\n",
    "            in_features (int): number of input node features.\n",
    "            out_features (int): number of output node features.\n",
    "            epsilon (float): epsilon value. (default=0.0)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ...  # TODO\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"\n",
    "        Perform graph convolution operation.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input node features of shape (num_nodes, in_features).\n",
    "            adj (Tensor): Adjacency matrix of the graph, shape (num_nodes, num_nodes).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output node features after graph convolution, shape (num_nodes, out_features).\n",
    "        \"\"\"\n",
    "        ...  # TODO\n",
    "        return ...  # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the prediction head, we can directly make the predictions $\\hat{\\mathbf{y}}_v$ from the node embeddings $\\mathbf{h}_v^{(l)}$. Let's implement it by doing a simple linear transformation:\n",
    "$$\n",
    "\\hat{\\mathbf{y}}_v = \\mathbf{W}\\mathbf{h}_v^{(l)} + \\mathbf{b},\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearHead(nn.Module):\n",
    "    \"\"\"Prediction head using a linear transformation of the node embedding.\"\"\"\n",
    "\n",
    "    def __init__(self, in_features, num_tasks):\n",
    "        \"\"\"\n",
    "        Initialize the prediction head.\n",
    "\n",
    "        Args:\n",
    "            in_features (int): number of input node features.\n",
    "            num_tasks (int): number of prediction tasks (classes).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ...  # TODO\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Node-level prediction given the node embeddings.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input node features of shape (num_nodes, in_features).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Node predictions of shape (num_nodes, num_tasks).\n",
    "        \"\"\"\n",
    "        return ...  # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As last week, let's aggregate them into a single model that we call `MyGIN`.\n",
    "\n",
    "To be compatible with the code above, the model's forward method will take as input edge indices `edge_index`, however our `GINConv` above uses adjacency matrices.\n",
    "Therefore, we will simply convert the edge indices into an adjacency matrix in the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_index_to_adj(edge_index, num_nodes):\n",
    "    \"\"\"\n",
    "    Convert edge index to adjacency matrix.\n",
    "\n",
    "    Args:\n",
    "        edge_index (LongTensor): Graph edge indices of shape (2, num_edges).\n",
    "        num_nodes (int): Number of nodes in the graph.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Adjacency matrix of shape (num_nodes, num_nodes).\n",
    "    \"\"\"\n",
    "    adj = torch.zeros((num_nodes, num_nodes), device=edge_index.device)\n",
    "    adj[edge_index[0], edge_index[1]] = 1.0\n",
    "    return adj\n",
    "\n",
    "\n",
    "class MyGIN(nn.Module):\n",
    "    \"\"\"Re-implement the GIN network for node classification.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, hidden_channels, num_layers, out_channels, epsilon=0.0):\n",
    "        \"\"\"\n",
    "        Initialize the GNN model for node classification.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input node features.\n",
    "            hidden_channels (int): Number of hidden units.\n",
    "            num_layers (int): Number of GINConv layers.\n",
    "            out_channels (int): Number of prediction tasks (classes).\n",
    "            epsilon (float): Epsilon value for GINConv. (default=0.0)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ...  # TODO\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Perform forward pass for node classification.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input node features of shape (num_nodes, num_features).\n",
    "            adj (Tensor): Adjacency matrix of the graph, shape (num_nodes, num_nodes).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Predicted edge probabilities for each pair of nodes, shape (num_nodes, num_nodes).\n",
    "        \"\"\"\n",
    "        ...  # TODO\n",
    "\n",
    "        return ...  # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to use it above and see if it works! However, note that it will be slower than the PyTorch Geometric implementation, so you may want to reduce the epochs."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
