{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Exercise 1 - PyTorch Tutorial\n","\n","This exercise is to introduce you how to use PyTorch. The main goal of this exercise is to help you understand PyTorch’s library.\n","\n","Reference:\n","\n","[1] Official PyTorch Tutorial: https://pytorch.org/tutorials/"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Installation"]},{"cell_type":"markdown","metadata":{},"source":["The first step is to install the package [Numpy](https://numpy.org/install/) and [PyTorch](https://pytorch.org/). After installing them, you can test by importing the packages."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import torch\n","import numpy as np"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Tensors"]},{"cell_type":"markdown","metadata":{},"source":["In PyTorch, we use tensors to encode the inputs, outputs, and parameters of the models. Tensors are similar to NumPy’s ndarrays, except that tensors:\n","- Can run on GPUs or other hardware accelerators. \n","- Are optimized for automatic differentiation."]},{"cell_type":"markdown","metadata":{},"source":["### 2.1. Create a Tensor\n","\n","In this section, you will know how to initialize a tensor. We provide some examples, and you can play with them by changing the values and the shape of the tensors."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["data: \n"," [[1, 2], [3, 4]] \n","\n","tensor_data: \n"," tensor([[1, 2],\n","        [3, 4]]) \n","\n"]}],"source":["# Create from data\n","data = [[1, 2],[3, 4]]\n","tensor_data = torch.tensor(data)\n","\n","# You can see that the data type becomes tensor.\n","print(f\"data: \\n {data} \\n\")\n","print(f\"tensor_data: \\n {tensor_data} \\n\")\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["np_array: \n"," [[1 2]\n"," [3 4]] \n","\n","tensor_np: \n"," tensor([[1, 2],\n","        [3, 4]]) \n","\n"]}],"source":["# Create from NumPy array\n","np_array = np.array(data)\n","tensor_np = torch.from_numpy(np_array)\n","\n","print(f\"np_array: \\n {np_array} \\n\")\n","print(f\"tensor_np: \\n {tensor_np} \\n\")"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Ones Tensor: \n"," tensor([[1, 1],\n","        [1, 1]]) \n","\n","Random Tensor: \n"," tensor([[0.4986, 0.9636],\n","        [0.7148, 0.3094]]) \n","\n"]}],"source":["# Create from another tensor\n","tensor_ones = torch.ones_like(tensor_data)\n","print(f\"Ones Tensor: \\n {tensor_ones} \\n\")\n","\n","tensor_rand = torch.rand_like(tensor_data, dtype=torch.float)\n","print(f\"Random Tensor: \\n {tensor_rand} \\n\")"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Random Tensor: \n"," tensor([[0.7140, 0.8781, 0.2000],\n","        [0.8043, 0.4506, 0.8451]]) \n","\n","Ones Tensor: \n"," tensor([[1., 1., 1.],\n","        [1., 1., 1.]]) \n","\n","Zeros Tensor: \n"," tensor([[0., 0., 0.],\n","        [0., 0., 0.]])\n"]}],"source":["shape = (2,3,)\n","rand_tensor = torch.rand(shape)\n","ones_tensor = torch.ones(shape)\n","zeros_tensor = torch.zeros(shape)\n","\n","print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n","print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n","print(f\"Zeros Tensor: \\n {zeros_tensor}\")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of tensor: torch.Size([3, 4])\n","Datatype of tensor: torch.float32\n","Device tensor is stored on: cpu\n"]}],"source":["# Check the attributes of a Tensor\n","tensor_try = torch.rand(3,4)\n","\n","print(f\"Shape of tensor: {tensor_try.shape}\")\n","print(f\"Datatype of tensor: {tensor_try.dtype}\")\n","print(f\"Device tensor is stored on: {tensor_try.device}\")"]},{"cell_type":"markdown","metadata":{},"source":["### 2.2. Operations on Tensors\n","\n","This section is to show you how to apply the operations on tensors using the functions in PyTorch."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Device tensor is stored on: cpu\n","Device tensor is stored on: cpu\n"]}],"source":["# We move our tensor to the GPU if available\n","tensor_try = torch.rand(3,4)\n","print(f\"Device tensor is stored on: {tensor_try.device}\")\n","\n","if torch.cuda.is_available():\n","    tensor_try = tensor_try.to(\"cuda\")\n","\n","print(f\"Device tensor is stored on: {tensor_try.device}\")"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0.6922, 0.8034, 0.3391, 0.3146],\n","        [0.0505, 0.3010, 0.1598, 0.7561],\n","        [0.0208, 0.8315, 0.2758, 0.4926]])\n","First row: tensor([0.6922, 0.8034, 0.3391, 0.3146])\n","First column: tensor([0.6922, 0.0505, 0.0208])\n","Last column: tensor([0.3146, 0.7561, 0.4926])\n"]}],"source":["# Indexing and slicing:\n","tensor_try = torch.rand(3,4)\n","print(tensor_try)\n","\n","print(f\"First row: {tensor_try[0]}\")\n","print(f\"First column: {tensor_try[:, 0]}\")\n","print(f\"Last column: {tensor_try[:, -1]}\")\n","tensor_try[:,1] = 0"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0., 0., 0.],\n","        [0., 0., 0.],\n","        [1., 1., 1.],\n","        [1., 1., 1.],\n","        [2., 2., 2.],\n","        [2., 2., 2.]])\n","tensor([[0., 0., 0., 1., 1., 1., 2., 2., 2.],\n","        [0., 0., 0., 1., 1., 1., 2., 2., 2.]])\n"]}],"source":["# Concatenate tensors\n","# You can also check the shape of the tensors.\n","\n","tensor1 = torch.zeros(2,3)\n","tensor2 = torch.zeros(2,3) + 1\n","tensor3 = torch.zeros(2,3) + 2\n","\n","tensor_concat_0 = torch.cat([tensor1, tensor2, tensor3], dim=0)\n","print(tensor_concat_0)\n","\n","tensor_concat_1 = torch.cat([tensor1, tensor2, tensor3], dim=1)\n","print(tensor_concat_1)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor1: tensor([[1., 1., 1.],\n","        [1., 1., 1.]])\n","tensor2: tensor([[2., 2., 2.],\n","        [2., 2., 2.]])\n","Add: tensor([[3., 3., 3.],\n","        [3., 3., 3.]])\n","Minus: tensor([[-1., -1., -1.],\n","        [-1., -1., -1.]])\n","Element-wise product: tensor([[2., 2., 2.],\n","        [2., 2., 2.]])\n","Product: tensor([[6., 6.],\n","        [6., 6.]])\n"]}],"source":["# Arithmetic operations\n","tensor1 = torch.zeros(2,3) + 1\n","tensor2 = torch.zeros(2,3) + 2\n","print('tensor1:', tensor1)\n","print('tensor2:', tensor2)\n","     \n","tensor_add = tensor1 + tensor2\n","print('Add:', tensor_add)\n","\n","tensor_minus = tensor1 - tensor2\n","print('Minus:', tensor_minus)\n","\n","tensor_elementwise_product = tensor1 * tensor2\n","print('Element-wise product:', tensor_elementwise_product)\n","\n","tensor_product = tensor1 @ tensor2.T # .T is to get the transpose\n","print('Product:', tensor_product)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor: tensor([[1., 1., 1.],\n","        [1., 1., 1.]])\n","sum: tensor(6.)\n","sum along the first dimension: tensor([3., 3.])\n","mean: tensor(1.)\n","mean of the first dimension: tensor([1., 1.])\n"]}],"source":["# Aggregation\n","tensor_try = torch.zeros(2,3) + 1\n","print('tensor:', tensor_try)\n","\n","sum_try = tensor_try.sum()\n","print('sum:', sum_try)\n","sum_try = tensor_try.sum(dim = 1)\n","print('sum along the first dimension:', sum_try)\n","\n","mean = tensor_try.mean()\n","print('mean:', mean)\n","mean = tensor_try.mean(dim = 1)\n","print('mean of the first dimension:', mean)"]},{"cell_type":"markdown","metadata":{},"source":["### 2.3. Bridge with NumPy\n","\n","Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other."]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["NumPy: [1. 1. 1. 1. 1.]\n","Tensor: tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"]}],"source":["# Numpy array to Tensor\n","n = np.ones(5)\n","t = torch.from_numpy(n)\n","\n","print('NumPy:', n)\n","print('Tensor:', t)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Tensor: tensor([1., 1., 1., 1., 1.])\n","NumPy: [1. 1. 1. 1. 1.]\n"]}],"source":["# Tensor to Numpy array\n","\n","t = torch.ones(5)\n","n = t.numpy()\n","\n","print('Tensor:', t)\n","print('NumPy:', n)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# Can you convert a Tensor on GPU to Numpy array?\n","# Try this only when you have available GPU.\n","\n","if torch.cuda.is_available():\n","    t = torch.ones(5).to('cuda')\n","\n","    #n = t.numpy() # wrong\n","    n = t.cpu().numpy() # correct\n","\n","    print('Tensor:', t)\n","    print('NumPy:', n)"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Autograd and Automatic Differentiation\n","\n","### 3.1. Computing gradients automatically\n","In this section, we see how can we automatically get the gradients for the tensors.\n","\n","Here, we show a toy example. The model is $z = x * w + b$, where $x$ is the input, $w$ and $b$ are the parameters, and $z$ is the predicted output. Given a expected output $y$, we can calculate the 'difference' between $y$ and $z$, and then using the 'difference' to calculate the gradient of $w$ and $b$."]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Gradient function for z = <AddBackward0 object at 0x1276a32b0>\n","Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x104610580>\n","Gradient function of w = tensor([[0.0970, 0.4663],\n","        [0.0970, 0.4663],\n","        [0.0970, 0.4663]])\n","Gradient function of b = tensor([0.0970, 0.4663])\n"]}],"source":["x = torch.ones(3)  # input tensor\n","\n","w = torch.randn(3, 2, requires_grad=True) # trainable parameter w\n","b = torch.randn(2, requires_grad=True) # trainable parameter b\n","z = torch.matmul(x, w)+b # predicted output: z = x * w + b\n","\n","y = torch.zeros(2)  # expected output\n","\n","loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y) # loss: 'difference' between predicted output and expected output\n","\n","# The places are automatically reserved for the gradient.\n","print(f\"Gradient function for z = {z.grad_fn}\")\n","print(f\"Gradient function for loss = {loss.grad_fn}\")\n","\n","# Automatically get the gradient\n","loss.backward()\n","print(f\"Gradient function of w = {w.grad}\")\n","print(f\"Gradient function of b = {b.grad}\")"]},{"cell_type":"markdown","metadata":{},"source":["### 3.2. Disabling gradient tracking"]},{"cell_type":"markdown","metadata":{},"source":["We do not want to track the gradient in some conditions. For example, sometimes, we only want to train a part of a network. Under this condition, we prefer to disable gradient tracking of the part we do not want to train.\n","\n","Here, we show some ways to disable gradient tracking."]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n","False\n"]}],"source":["# Option 1: with torch.no_grad()\n","\n","z = torch.matmul(x, w)+b\n","print(z.requires_grad)\n","\n","with torch.no_grad():\n","    z = torch.matmul(x, w)+b\n","print(z.requires_grad)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n","True\n","False\n"]}],"source":["# Option 2: with .detach()\n","\n","z = torch.matmul(x, w)+b\n","print(z.requires_grad)\n","\n","z_det = z.detach()\n","print(z.requires_grad)\n","print(z_det.requires_grad)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["True\n","False\n"]},{"data":{"text/plain":["'\\nz = torch.matmul(x, w)+b\\nprint(z.requires_grad)\\n\\nz.requires_grad = False\\nprint(z.requires_grad)\\n'"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["# Option 3: .requires_grad = False\n","\n","w = torch.randn(3, 2, requires_grad=True)\n","print(w.requires_grad)\n","w.requires_grad = False\n","print(w.requires_grad)\n","\n","# You can release the code and try if you can disable the gradient of z using '.requires_grad = False'.\n","\"\"\"\n","z = torch.matmul(x, w)+b\n","print(z.requires_grad)\n","\n","z.requires_grad = False\n","print(z.requires_grad)\n","\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["## 4. Datasets and Dataloaders"]},{"cell_type":"markdown","metadata":{},"source":["In this section, we will go through the datasets and data loaders. \n","\n","### 4.1. Dataset\n","\n","Here, we show an example about how to load a dataset from Pytorch. \n","\n","In the following homework and project, you will try to design your dataset by inheriting from the Dataset class from PyTorch."]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["# you should have installed torchvision when installing pytorch using, e.g., 'pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117'\n","\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["# we try on FashionMNIST, a image dataset predefined in Pytorch\n","\n","training_data = datasets.FashionMNIST(\n","    root=\"data\", # the path which you download the data into.\n","    train=True, # it has a predefined data split. Set train=True to get the trainning data and train=False for the testing data.\n","    download=True, # you need to download the dataset in the first time you use it.\n","    transform=ToTensor() # apply transform to the images. Here we convert the images into tensors.\n",")"]},{"cell_type":"markdown","metadata":{},"source":["We use package matplotlib to visulize the data in the dataset. Here is the [installation](https://matplotlib.org/stable/users/installing/index.html)."]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 800x800 with 9 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# For a better visualization, we map labels from integer to string.\n","labels_map = {\n","    0: \"T-Shirt\",\n","    1: \"Trouser\",\n","    2: \"Pullover\",\n","    3: \"Dress\",\n","    4: \"Coat\",\n","    5: \"Sandal\",\n","    6: \"Shirt\",\n","    7: \"Sneaker\",\n","    8: \"Bag\",\n","    9: \"Ankle Boot\",\n","}\n","\n","figure = plt.figure(figsize=(8, 8))\n","cols, rows = 3, 3\n","\n","for i in range(1, cols * rows + 1):\n","    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n","\n","    # Get image and the corresponding label from the dataset\n","    img, label = training_data[sample_idx]\n","\n","    figure.add_subplot(rows, cols, i)\n","    plt.title(labels_map[label])\n","    plt.axis(\"off\")\n","    plt.imshow(img.squeeze(), cmap=\"gray\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### 4.2. Dataloader"]},{"cell_type":"markdown","metadata":{},"source":["The Dataset retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches” and reshuffle the data at every epoch. DataLoader is an iterable that abstracts this complexity for us in an easy API."]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","# We need to provide the dataloader a dataset, the batch_size, and a flag of reshuffle the data at every epoch or not.\n","train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Feature batch shape: torch.Size([64, 1, 28, 28])\n","Labels batch shape: torch.Size([64])\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Label: 9\n"]}],"source":["# We can get the minibatch from the dataloader easily\n","train_features, train_labels = next(iter(train_dataloader))\n","\n","# Try to change the batch_size in DataLoader in the previous block and see what will change accordingly.\n","print(f\"Feature batch shape: {train_features.size()}\")\n","print(f\"Labels batch shape: {train_labels.size()}\")\n","\n","# Check the 9th example in this minibatch\n","img = train_features[8].squeeze()\n","label = train_labels[8]\n","plt.imshow(img, cmap=\"gray\")\n","plt.show()\n","print(f\"Label: {label}\")"]},{"cell_type":"markdown","metadata":{},"source":["## 5. Neural Networks"]},{"cell_type":"markdown","metadata":{},"source":["### 5.1. Building a network\n","\n","Here we show a toy example of how to build a 2-layers MLP for FashionMNIST dataset."]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["# torch.nn is the main package to build the neural networks\n","from torch import nn"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cpu device\n"]}],"source":["# GPUs are prefered to train the models. So we need to first set the device.\n","# Note that we do not have to use GPUs for this exercise.\n","device = (\n","    \"cuda\"\n","    if torch.cuda.is_available()\n","    else \"cpu\"\n",")\n","print(f\"Using {device} device\")"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["# We define our network here.\n","# nn.Flatten() is to flatten an image into a tensor.\n","# nn.Sequential() is an ordered container of modules. For example, here it contains a linear layer, an activate function ReLU, and another linear layer in order.\n","\n","class NeuralNetwork(nn.Module):\n","    def __init__(self, hidden_dim = 128):\n","        super().__init__()\n","        self.flatten = nn.Flatten()\n","        self.linear_relu_stack = nn.Sequential(\n","            nn.Linear(28*28, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, 10),\n","        )\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        logits = self.linear_relu_stack(x)\n","        return logits"]},{"cell_type":"markdown","metadata":{},"source":["### 5.2. Using the model"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["NeuralNetwork(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear_relu_stack): Sequential(\n","    (0): Linear(in_features=784, out_features=128, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=128, out_features=10, bias=True)\n","  )\n",")\n"]}],"source":["# We set our model to be the NeuralNetwork. You can adapt the hyper-parameter hidden_dim.\n","\n","model = NeuralNetwork(hidden_dim = 128).to(device)\n","print(model)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Predicted class: tensor([9])\n"]}],"source":["# Inference from the model\n","# Set a random input sample\n","X = torch.rand(1, 28, 28, device=device)\n","\n","# Put the input sample into the model\n","logits = model(X)\n","\n","# Since the model output the logits, we need to convert it into the probability\n","pred_probab = nn.Softmax(dim=1)(logits)\n","\n","# we take the position of the maximum probability as our final prediction\n","y_pred = pred_probab.argmax(1)\n","\n","print(f\"Predicted class: {y_pred}\")"]},{"cell_type":"markdown","metadata":{},"source":["## 6. Loss function and optimizer\n","\n","To train a model is to optimize a model to an expected state. Thus, we first use a loss function to define the 'difference' between current state and expected state. And then, to minimize the 'difference', we use an optimizer to update the parameters in the model."]},{"cell_type":"markdown","metadata":{},"source":["### 6.1. Loss function\n","\n","There are some predefined [loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions) in Pytorch.\n","\n","MSE is the loss always used in regression tasks. Cross-entropy is the loss always used in classification tasks."]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(1.)\n"]}],"source":["# Example MSE loss\n","loss_fn = nn.MSELoss()\n","\n","input = torch.zeros(2, 3)\n","target = torch.ones(2, 3)\n","\n","output = loss_fn(input, target)\n","print(output)"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(5.5452)\n"]}],"source":["loss_fn = nn.CrossEntropyLoss()\n","\n","input = torch.zeros(3, 4)\n","target = torch.ones(3, 4)\n","\n","output = loss_fn(input, target)\n","print(output)"]},{"cell_type":"markdown","metadata":{},"source":["### 6.2. Optimizer\n","\n","Section 3 shows that we can automatically get the gradient for the tensors. However, the gradient is not the one that updates the tensors. To update the tensors, we need an optimizer.\n","\n","Here, we show how to setup an optimizer. We will show how to use it in Section 6.3.\n","\n","A fantastic introduction of different optimizers: [An overview of gradient descent optimization algorithms](https://www.ruder.io/optimizing-gradient-descent/).\n","\n","Note: Adam is the one that is most commonly used. However, sometimes it is not the best choice. Thus, you can always try Adam first to see what will happen."]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["# We need to provide an optimizer with the parameters need to be optimized.\n","\n","w = torch.randn(3, 2, requires_grad=True)\n","model = NeuralNetwork(hidden_dim = 128)\n","\n","learning_rate = 1e-3\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"]},{"cell_type":"markdown","metadata":{},"source":["### 6.3. Regularization\n","\n","Regularization is used mainly to prevent overfitting. The most common ways are dropout, L1 regularization, L2 regularization, and batch normalization. Here, we show examples of how to implement that regularization."]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0., 0., 2., 0., 0., 0., 2., 2.],\n","        [2., 0., 2., 2., 0., 2., 0., 2.]])\n"]}],"source":["# Dropout layer\n","\n","# Here, we use nn.Dropout to build a dropout layer whose dropout rate is 0.5\n","dropout_layer = nn.Dropout(p=0.5) \n","\n","# We test the dropout layer.\n","# You can run this block multiple times to see what happen.\n","# Question: Why the non-zero outputs are 2?\n","# Hint: check https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\n","\n","input = torch.ones(2, 8)\n","output = dropout_layer(input)\n","\n","print(output)"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(185.0943, grad_fn=<AddBackward0>)\n"]}],"source":["# L1 regularization\n","\n","# Total loss\n","loss = 0\n","\n","# Get the regularization loss\n","reg_loss = 0\n","for param in model.parameters():\n","    reg_loss += torch.norm(param, p=1)\n","\n","factor = 0.1 #lambda\n","loss += factor * reg_loss # Add the regularization loss to the total loss with lambda\n","\n","print(loss)"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["# L2 regularization\n","# We can directly apply L2 regularization through the optimizers from torch.optim\n","\n","# The parameter weight_decay is the lambda for L2 regularization\n","# For more details, please check: https://pytorch.org/docs/stable/generated/torch.optim.SGD.html\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=factor)"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["input_tensor: tensor([[-0.4823,  0.2671, -0.1072, -0.7679,  0.6126,  0.2214, -0.0138, -0.2162,\n","         -0.4373, -0.4126],\n","        [-1.2887, -2.1281, -1.2049, -0.7922, -0.4731, -0.3910, -0.3619,  1.0695,\n","         -0.8299,  0.9929],\n","        [-0.3110, -1.2361, -1.7180,  0.0182, -0.8234, -0.9196, -0.1193, -0.2521,\n","         -1.3063,  0.4873],\n","        [-0.8006, -0.8577, -2.3251, -0.0216,  0.0641,  1.7189,  1.0644, -0.4164,\n","         -2.4075, -0.9271],\n","        [-0.3998,  0.7391,  0.6330, -1.1693,  0.9909, -1.1954, -1.5032,  0.4731,\n","          0.3592,  0.4976]])\n","output_tensor: tensor([[ 0.4884,  0.8806,  0.7799, -0.4737,  0.8052,  0.3229,  0.2108, -0.6216,\n","          0.5289, -0.7766],\n","        [-1.7725, -1.4366, -0.2426, -0.5256, -0.8185, -0.2682, -0.2135,  1.6766,\n","          0.1026,  1.2440],\n","        [ 0.9686, -0.5737, -0.7206,  1.2086, -1.3424, -0.7784,  0.0822, -0.6858,\n","         -0.4148,  0.5170],\n","        [-0.4040, -0.2076, -1.2862,  1.1235, -0.0151,  1.7683,  1.5255, -0.9795,\n","         -1.6107, -1.5162],\n","        [ 0.7196,  1.3373,  1.4695, -1.3328,  1.3709, -1.0446, -1.6050,  0.6105,\n","          1.3940,  0.5318]])\n"]}],"source":["# Batch Normalization\n","# Check this link for different kind of normalization: https://scortex.io/batch-norm-folding-an-easy-way-to-improve-your-network-speed/\n","\n","# With Learnable Parameters\n","# m = nn.BatchNorm1d(10)\n","\n","# Without Learnable Parameters: set affine to False\n","m = nn.BatchNorm1d(10, affine=False)\n","\n","input_tensor = torch.randn(5, 10)\n","output_tensor = m(input_tensor)\n","\n","print('input_tensor:', input_tensor)\n","print('output_tensor:', output_tensor)"]},{"cell_type":"markdown","metadata":{},"source":["## 7. Training loop"]},{"cell_type":"markdown","metadata":{},"source":["### 7.1. An example of a complete training loop\n","\n","First, we import the packages. Then we initialize the hyper-parameters. After that, we can define dataset, dataloader, and model."]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","\n","# Hyper-parameters\n","learning_rate = 1e-3\n","batch_size = 64\n","epochs = 10\n","\n","# Build training and test dataset on FashionMNIST\n","training_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=True,\n","    download=True,\n","    transform=ToTensor()\n",")\n","\n","test_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=False,\n","    download=True,\n","    transform=ToTensor()\n",")\n","\n","# Setup the Dataloader for training\n","train_dataloader = DataLoader(training_data, batch_size=64)\n","test_dataloader = DataLoader(test_data, batch_size=64)\n","\n","# Create an MLP with 3 hidden layers\n","class NeuralNetwork(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.flatten = nn.Flatten()\n","        self.linear_relu_stack = nn.Sequential(\n","            nn.Linear(28*28, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 10),\n","        )\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        logits = self.linear_relu_stack(x)\n","        return logits\n","\n","model = NeuralNetwork()"]},{"cell_type":"markdown","metadata":{},"source":["After getting the model and data, we need to define our loss function and optimizer."]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["# Initialize the loss function with the cross-entropy loss\n","loss_fn = nn.CrossEntropyLoss()\n","\n","# Initialize the SGD optimizer with the predefined learning rate\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"]},{"cell_type":"markdown","metadata":{},"source":["After setting up everything, we can define our training and test loop.\n","\n","You may notice that we set model to train or test state using model.train() or model.eval(). If you are interested in that, you can try to find the answer to see why this is important for batch normalization and dropout layers. (Hint: batch normalization and dropout layers behave differently during training and evaluation.)"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["# Define a function of training loop\n","def train_loop(dataloader, model, loss_fn, optimizer):\n","\n","    size = len(dataloader.dataset)\n","\n","    # Set the model to train state\n","    model.train()\n","    \n","    # Load samples from the dataloader\n","    for batch, (X, y) in enumerate(dataloader):\n","\n","        # Compute prediction and loss\n","        pred = model(X)\n","        loss = loss_fn(pred, y)\n","\n","        # There are three basic steps for backpropagation:\n","        # 1. Set the gradient of trainable parameters to 0.\n","        optimizer.zero_grad()\n","        # 2. Automatically calculate the gradient of trainable parameters.\n","        loss.backward()\n","        # 3. Automatically update the trainable parameters using the gradient.\n","        optimizer.step()\n","\n","        if batch % 100 == 0:\n","            loss, current = loss.item(), (batch + 1) * len(X)\n","            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n","\n","# Define a function of test loop\n","def test_loop(dataloader, model, loss_fn):\n","    # Set the model to evaluation mode - important for batch normalization and dropout layers\n","    # Unnecessary in this situation but added for best practices\n","    model.eval()\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    test_loss, correct = 0, 0\n","\n","    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n","    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            pred = model(X)\n","            test_loss += loss_fn(pred, y).item()\n","            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","\n","    test_loss /= num_batches\n","    correct /= size\n","    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"]},{"cell_type":"markdown","metadata":{},"source":["We have defined everything and can start to train our model!"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1\n","-------------------------------\n","loss: 2.308683  [   64/60000]\n"]},{"name":"stdout","output_type":"stream","text":["loss: 2.293364  [ 6464/60000]\n","loss: 2.275410  [12864/60000]\n","loss: 2.275458  [19264/60000]\n","loss: 2.256159  [25664/60000]\n","loss: 2.228498  [32064/60000]\n","loss: 2.237523  [38464/60000]\n","loss: 2.197548  [44864/60000]\n","loss: 2.197942  [51264/60000]\n","loss: 2.182804  [57664/60000]\n","Test Error: \n"," Accuracy: 42.6%, Avg loss: 2.168865 \n","\n","Epoch 2\n","-------------------------------\n","loss: 2.172488  [   64/60000]\n","loss: 2.164914  [ 6464/60000]\n","loss: 2.108518  [12864/60000]\n","loss: 2.136184  [19264/60000]\n","loss: 2.084756  [25664/60000]\n","loss: 2.023540  [32064/60000]\n","loss: 2.064564  [38464/60000]\n","loss: 1.976681  [44864/60000]\n","loss: 1.983860  [51264/60000]\n","loss: 1.933932  [57664/60000]\n","Test Error: \n"," Accuracy: 55.7%, Avg loss: 1.919952 \n","\n","Epoch 3\n","-------------------------------\n","loss: 1.942967  [   64/60000]\n","loss: 1.921244  [ 6464/60000]\n","loss: 1.803498  [12864/60000]\n","loss: 1.857641  [19264/60000]\n","loss: 1.744177  [25664/60000]\n","loss: 1.691247  [32064/60000]\n","loss: 1.735581  [38464/60000]\n","loss: 1.618418  [44864/60000]\n","loss: 1.651378  [51264/60000]\n","loss: 1.560267  [57664/60000]\n","Test Error: \n"," Accuracy: 58.7%, Avg loss: 1.561941 \n","\n","Epoch 4\n","-------------------------------\n","loss: 1.623018  [   64/60000]\n","loss: 1.593003  [ 6464/60000]\n","loss: 1.437075  [12864/60000]\n","loss: 1.516747  [19264/60000]\n","loss: 1.390905  [25664/60000]\n","loss: 1.383373  [32064/60000]\n","loss: 1.412668  [38464/60000]\n","loss: 1.318884  [44864/60000]\n","loss: 1.362127  [51264/60000]\n","loss: 1.266311  [57664/60000]\n","Test Error: \n"," Accuracy: 62.4%, Avg loss: 1.284441 \n","\n","Epoch 5\n","-------------------------------\n","loss: 1.359495  [   64/60000]\n","loss: 1.343695  [ 6464/60000]\n","loss: 1.173125  [12864/60000]\n","loss: 1.281922  [19264/60000]\n","loss: 1.151041  [25664/60000]\n","loss: 1.177080  [32064/60000]\n","loss: 1.205223  [38464/60000]\n","loss: 1.128905  [44864/60000]\n","loss: 1.174113  [51264/60000]\n","loss: 1.091758  [57664/60000]\n","Test Error: \n"," Accuracy: 64.1%, Avg loss: 1.108524 \n","\n","Epoch 6\n","-------------------------------\n","loss: 1.177103  [   64/60000]\n","loss: 1.181241  [ 6464/60000]\n","loss: 0.994522  [12864/60000]\n","loss: 1.134426  [19264/60000]\n","loss: 0.998612  [25664/60000]\n","loss: 1.036305  [32064/60000]\n","loss: 1.077041  [38464/60000]\n","loss: 1.006708  [44864/60000]\n","loss: 1.052391  [51264/60000]\n","loss: 0.983147  [57664/60000]\n","Test Error: \n"," Accuracy: 65.4%, Avg loss: 0.994641 \n","\n","Epoch 7\n","-------------------------------\n","loss: 1.050686  [   64/60000]\n","loss: 1.075594  [ 6464/60000]\n","loss: 0.871707  [12864/60000]\n","loss: 1.036039  [19264/60000]\n","loss: 0.901931  [25664/60000]\n","loss: 0.935783  [32064/60000]\n","loss: 0.993297  [38464/60000]\n","loss: 0.926523  [44864/60000]\n","loss: 0.968801  [51264/60000]\n","loss: 0.911525  [57664/60000]\n","Test Error: \n"," Accuracy: 67.0%, Avg loss: 0.917420 \n","\n","Epoch 8\n","-------------------------------\n","loss: 0.958367  [   64/60000]\n","loss: 1.002688  [ 6464/60000]\n","loss: 0.784426  [12864/60000]\n","loss: 0.966767  [19264/60000]\n","loss: 0.838049  [25664/60000]\n","loss: 0.861770  [32064/60000]\n","loss: 0.934900  [38464/60000]\n","loss: 0.872557  [44864/60000]\n","loss: 0.908770  [51264/60000]\n","loss: 0.860975  [57664/60000]\n","Test Error: \n"," Accuracy: 68.3%, Avg loss: 0.862359 \n","\n","Epoch 9\n","-------------------------------\n","loss: 0.887662  [   64/60000]\n","loss: 0.948807  [ 6464/60000]\n","loss: 0.719980  [12864/60000]\n","loss: 0.915614  [19264/60000]\n","loss: 0.793430  [25664/60000]\n","loss: 0.805860  [32064/60000]\n","loss: 0.890957  [38464/60000]\n","loss: 0.834768  [44864/60000]\n","loss: 0.863798  [51264/60000]\n","loss: 0.823250  [57664/60000]\n","Test Error: \n"," Accuracy: 69.3%, Avg loss: 0.821057 \n","\n","Epoch 10\n","-------------------------------\n","loss: 0.831654  [   64/60000]\n","loss: 0.906252  [ 6464/60000]\n","loss: 0.670577  [12864/60000]\n","loss: 0.876284  [19264/60000]\n","loss: 0.760379  [25664/60000]\n","loss: 0.762841  [32064/60000]\n","loss: 0.855487  [38464/60000]\n","loss: 0.806696  [44864/60000]\n","loss: 0.828957  [51264/60000]\n","loss: 0.793487  [57664/60000]\n","Test Error: \n"," Accuracy: 70.6%, Avg loss: 0.788538 \n","\n","Done!\n"]}],"source":["for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train_loop(train_dataloader, model, loss_fn, optimizer)\n","    test_loop(test_dataloader, model, loss_fn)\n","print(\"Done!\")"]},{"cell_type":"markdown","metadata":{},"source":["### 7.2. Save and Load the Model"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["# save the model\n","torch.save(model.state_dict(), 'model_weights.pth')"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Before loading the pretrained model:\n","Test Error: \n"," Accuracy: 10.0%, Avg loss: 2.303590 \n","\n","After loading the pretrained model:\n","Test Error: \n"," Accuracy: 70.6%, Avg loss: 0.788538 \n","\n"]}],"source":["# create a new model\n","model_new = NeuralNetwork()\n","print('Before loading the pretrained model:')\n","test_loop(test_dataloader, model_new, loss_fn)\n","\n","# load the model\n","model_new.load_state_dict(torch.load('model_weights.pth'))\n","print('After loading the pretrained model:')\n","test_loop(test_dataloader, model_new, loss_fn)"]},{"cell_type":"markdown","metadata":{},"source":["### 7.3. Practice here!"]},{"cell_type":"markdown","metadata":{},"source":["Let's do some practice here. The goal is to make us more familiar with the content we learn in this exercise. Also, you can play with the framework. For example, changing datasets, adapting hyper-parameters, modifying the structure of the neural network."]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["NeuralNetwork(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear_relu_stack): Sequential(\n","    (0): Linear(in_features=784, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=512, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=512, out_features=10, bias=True)\n","  )\n",")\n"]}],"source":["import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","\n","# Hyper-parameters\n","learning_rate = 1e-3\n","batch_size = 64\n","epochs = 10\n","\n","# ToDo: build training and test dataset on FashionMNIST\n","training_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=True,\n","    transform=ToTensor()\n",")\n","\n","test_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=False,\n","    transform=ToTensor()\n",")\n","\n","# ToDo: Setup the Dataloader for training\n","train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n","\n","# ToDo: Create an MLP with 3 hidden layers\n","class NeuralNetwork(nn.Module):\n","    def __init__(self, input_dim=28*28, hidden_dim=512, output_dim=10):\n","        super().__init__()\n","        self.flatten = nn.Flatten()\n","\n","        self.linear_relu_stack = nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, output_dim),\n","        )\n","\n","    def forward(self, x):\n","        x = self.flatten(x)\n","        logits = self.linear_relu_stack(x)\n","        return logits\n","\n","model = NeuralNetwork()\n","print(model)"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["# cross entropy loss for multiclass classification\n","loss_fn = nn.CrossEntropyLoss()\n","\n","# stochastic gradient descent for optimisation\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["# Define a function of training loop\n","def train_loop(dataloader, model, loss_fn, optimizer):\n","    size = len(dataloader.dataset)\n","\n","    # set model to train stage\n","    model.train()\n","    for batch, (X, y) in enumerate(dataloader):\n","        logits = model(X)\n","        loss = loss_fn(logits, y)\n","\n","        # clear out gradients and backprop\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if batch % 100 == 0:\n","            loss, current = loss.item(), (batch + 1) * len(X)\n","            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n","\n","# Define a function of test loop\n","def test_loop(dataloader, model, loss_fn):\n","    model.eval()\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    test_loss, correct = 0, 0\n","\n","    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n","    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            pred = model(X)\n","            test_loss += loss_fn(pred, y).item()\n","            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","\n","    test_loss /= num_batches\n","    correct /= size\n","    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1\n","-------------------------------\n","loss: 2.310869  [   64/60000]\n","loss: 0.615386  [ 6464/60000]\n","loss: 0.584368  [12864/60000]\n","loss: 0.280094  [19264/60000]\n","loss: 0.371964  [25664/60000]\n","loss: 0.502756  [32064/60000]\n","loss: 0.333657  [38464/60000]\n","loss: 0.485438  [44864/60000]\n","loss: 0.407937  [51264/60000]\n","loss: 0.329369  [57664/60000]\n","Test Error: \n"," Accuracy: 84.2%, Avg loss: 0.426191 \n","\n","Epoch 2\n","-------------------------------\n","loss: 0.274815  [   64/60000]\n","loss: 0.555414  [ 6464/60000]\n","loss: 0.144800  [12864/60000]\n","loss: 0.304131  [19264/60000]\n","loss: 0.229512  [25664/60000]\n","loss: 0.634857  [32064/60000]\n","loss: 0.372951  [38464/60000]\n","loss: 0.403404  [44864/60000]\n","loss: 0.230826  [51264/60000]\n","loss: 0.474959  [57664/60000]\n","Test Error: \n"," Accuracy: 85.8%, Avg loss: 0.400640 \n","\n","Epoch 3\n","-------------------------------\n","loss: 0.300520  [   64/60000]\n","loss: 0.551017  [ 6464/60000]\n","loss: 0.160366  [12864/60000]\n","loss: 0.339756  [19264/60000]\n","loss: 0.382805  [25664/60000]\n","loss: 0.405441  [32064/60000]\n","loss: 0.443296  [38464/60000]\n","loss: 0.261772  [44864/60000]\n","loss: 0.518097  [51264/60000]\n","loss: 0.391945  [57664/60000]\n","Test Error: \n"," Accuracy: 85.8%, Avg loss: 0.396444 \n","\n","Epoch 4\n","-------------------------------\n","loss: 0.283346  [   64/60000]\n","loss: 0.267790  [ 6464/60000]\n","loss: 0.209310  [12864/60000]\n","loss: 0.271407  [19264/60000]\n","loss: 0.397774  [25664/60000]\n","loss: 0.309618  [32064/60000]\n","loss: 0.306209  [38464/60000]\n","loss: 0.281165  [44864/60000]\n","loss: 0.377659  [51264/60000]\n","loss: 0.313201  [57664/60000]\n","Test Error: \n"," Accuracy: 87.7%, Avg loss: 0.345192 \n","\n","Epoch 5\n","-------------------------------\n","loss: 0.217473  [   64/60000]\n","loss: 0.290167  [ 6464/60000]\n","loss: 0.296967  [12864/60000]\n","loss: 0.392459  [19264/60000]\n","loss: 0.254007  [25664/60000]\n","loss: 0.272166  [32064/60000]\n","loss: 0.213808  [38464/60000]\n","loss: 0.262050  [44864/60000]\n","loss: 0.188276  [51264/60000]\n","loss: 0.258832  [57664/60000]\n","Test Error: \n"," Accuracy: 87.3%, Avg loss: 0.344938 \n","\n","Epoch 6\n","-------------------------------\n","loss: 0.484436  [   64/60000]\n","loss: 0.168563  [ 6464/60000]\n","loss: 0.219674  [12864/60000]\n","loss: 0.237197  [19264/60000]\n","loss: 0.283112  [25664/60000]\n","loss: 0.409209  [32064/60000]\n","loss: 0.234716  [38464/60000]\n","loss: 0.239502  [44864/60000]\n","loss: 0.195338  [51264/60000]\n","loss: 0.275042  [57664/60000]\n","Test Error: \n"," Accuracy: 87.7%, Avg loss: 0.348499 \n","\n","Epoch 7\n","-------------------------------\n","loss: 0.242575  [   64/60000]\n","loss: 0.259508  [ 6464/60000]\n","loss: 0.247793  [12864/60000]\n","loss: 0.176717  [19264/60000]\n","loss: 0.438929  [25664/60000]\n","loss: 0.197306  [32064/60000]\n","loss: 0.235234  [38464/60000]\n","loss: 0.182076  [44864/60000]\n","loss: 0.211330  [51264/60000]\n","loss: 0.201937  [57664/60000]\n","Test Error: \n"," Accuracy: 88.8%, Avg loss: 0.318856 \n","\n","Epoch 8\n","-------------------------------\n","loss: 0.348579  [   64/60000]\n","loss: 0.193693  [ 6464/60000]\n","loss: 0.160848  [12864/60000]\n","loss: 0.320283  [19264/60000]\n","loss: 0.197321  [25664/60000]\n","loss: 0.240211  [32064/60000]\n","loss: 0.263401  [38464/60000]\n","loss: 0.289500  [44864/60000]\n","loss: 0.155923  [51264/60000]\n","loss: 0.177924  [57664/60000]\n","Test Error: \n"," Accuracy: 87.7%, Avg loss: 0.361951 \n","\n","Epoch 9\n","-------------------------------\n","loss: 0.178591  [   64/60000]\n","loss: 0.377162  [ 6464/60000]\n","loss: 0.318902  [12864/60000]\n","loss: 0.087191  [19264/60000]\n","loss: 0.183134  [25664/60000]\n","loss: 0.322823  [32064/60000]\n","loss: 0.320588  [38464/60000]\n","loss: 0.266397  [44864/60000]\n","loss: 0.158697  [51264/60000]\n","loss: 0.152830  [57664/60000]\n","Test Error: \n"," Accuracy: 88.6%, Avg loss: 0.327619 \n","\n","Epoch 10\n","-------------------------------\n","loss: 0.283200  [   64/60000]\n","loss: 0.245137  [ 6464/60000]\n","loss: 0.242204  [12864/60000]\n","loss: 0.220150  [19264/60000]\n","loss: 0.201807  [25664/60000]\n","loss: 0.213412  [32064/60000]\n","loss: 0.395349  [38464/60000]\n","loss: 0.220752  [44864/60000]\n","loss: 0.269952  [51264/60000]\n","loss: 0.261146  [57664/60000]\n","Test Error: \n"," Accuracy: 89.0%, Avg loss: 0.326776 \n","\n","Done!\n"]}],"source":["for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train_loop(train_dataloader, model, loss_fn, optimizer)\n","    test_loop(test_dataloader, model, loss_fn)\n","print(\"Done!\")"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["# save the model\n","torch.save(model.state_dict(), 'model_weights_custom.pth')"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Before loading the pretrained model:\n","Test Error: \n"," Accuracy: 15.0%, Avg loss: 2.293067 \n","\n","After loading the pretrained model:\n","Test Error: \n"," Accuracy: 89.0%, Avg loss: 0.326776 \n","\n"]}],"source":["# create a new model\n","new_model = NeuralNetwork()\n","print('Before loading the pretrained model:')\n","test_loop(test_dataloader, new_model, loss_fn)\n","\n","# load the model\n","new_model.load_state_dict(torch.load('model_weights_custom.pth'))\n","print('After loading the pretrained model:')\n","test_loop(test_dataloader, new_model, loss_fn)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"vscode":{"interpreter":{"hash":"3547e4de56e94baed6895a62efc214c7acc075f2a4ab60ef2d16710b3bde95e4"}}},"nbformat":4,"nbformat_minor":0}
