{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise 5: Debugging a BERT Model Training Script 🐞\n",
        "\n",
        "In this exercise, you will debug a script designed to train a [BERT](https://arxiv.org/abs/1810.04805) model using a portion of the [wikitext](https://huggingface.co/datasets/wikitext) dataset. We don't expect you to find any bug in the code provided in `utils`.\n",
        "\n",
        "🔍 **Mission:** Identify and correct at least 8 issues (🐞) in this notebook. Found more? Awesome – tell us about them!😄\n",
        "\n",
        "### Debugging Tips:\n",
        "\n",
        "**Memory Usage:** Identify which parameters impact the model's memory usage. The finalized script should run on Colab using a free account.\n",
        "\n",
        "**Input/Output Dimensions:** How are the input and output dimensions defined in a transformer?\n",
        "\n",
        "**Training Metrics:** Is the model learning? Why not?\n",
        "\n",
        "**Evaluation Metrics:** Are they behaving as expected?\n",
        "\n",
        "**Start Small:** Use a smaller dataset initially to identify and solve issues faster and without using extensive computational resources. Two script-running options are provided to help you. Running 200 epochs on the large dataset will take more than an hour. No need to do it – but if you are up to, go for it!\n",
        "\n",
        "**Performance Indicator:** A training perplexity around 20 after 50 epochs in `DEBUG` mode means you are on the right path.\n",
        "\n",
        "Happy debugging! 🚀"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ENABLE IF RUNNING ON GOOGLE COLAB\n",
        "\n",
        "# !pip install transformers\n",
        "# !pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertConfig, DataCollatorForLanguageModeling\n",
        "from datasets import load_dataset\n",
        "\n",
        "from utils import visualization, models\n",
        "\n",
        "import math\n",
        "import random\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8e991ba546b4d48ae2cd428a91ba17c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Dropdown(description='Mode:', options=('DEBUG', 'RUN'), value='DEBUG')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Set seeds\n",
        "def reset_seed():\n",
        "  seed = 5\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device(\"mps\" if torch.has_mps else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "mode_dropdown = visualization.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "\n",
        "class Ex5BertEmbeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from learnable word and position embeddings.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id, device=config.device)\n",
        "        self.positional_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size, device=config.device)\n",
        "\n",
        "        self.norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "        # Compute possible positions\n",
        "        self.positions= torch.arange(config.max_position_embeddings, dtype=torch.long, device=config.device)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.LongTensor] = None\n",
        "    ) -> torch.Tensor:\n",
        "        # Compute word embeddings\n",
        "        word_embeddings = self.word_embeddings(input_ids)\n",
        "        \n",
        "        # Compute positional embeddings\n",
        "        position_ids = torch.cat([self.positions] * input_ids.shape[0]).reshape(input_ids.shape)\n",
        "        pos_embeddings = self.positional_embeddings(position_ids)\n",
        "\n",
        "        # Sum word and positional embeddings\n",
        "        embeddings = word_embeddings + pos_embeddings\n",
        "\n",
        "        embeddings = self.norm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Ex5BertSelfAttentionLinearOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.intermediate_size)\n",
        "        self.norm = nn.LayerNorm(config.intermediate_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
        "        hidden_states = self.dense(hidden_states) # (B, S, H)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.norm(hidden_states + input_tensor)\n",
        "        return hidden_states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom Debug / Run configurations setup\n",
        "RUN_MODE = mode_dropdown.value\n",
        "\n",
        "dataset_config = 'dataset'\n",
        "model_config = 'model'\n",
        "training = 'training'\n",
        "\n",
        "DEBUG_CONFIG =  {\n",
        "        dataset_config : { 'train' : 5, 'validation': 5},\n",
        "        model_config: {'num_heads': 6, 'num_layers': 1},\n",
        "        training: {'track_every' : 10, 'lr': 0.001, 'epochs': 200} # Fix 4: Reduce learning rate to 0.001\n",
        "    }\n",
        "RUN_CONFIG = {\n",
        "        dataset_config : { 'train' : 36718, 'validation': 1000},\n",
        "        model_config: {'num_heads': 6, 'num_layers': 3},\n",
        "        training: {'track_every' : 1, 'lr': DEBUG_CONFIG[training]['lr'] / 10, 'epochs': 70 }\n",
        "    }\n",
        "RUN_CONFIG_DEFAULTS =  {\n",
        "    'DEBUG' : DEBUG_CONFIG,\n",
        "    'RUN' : RUN_CONFIG\n",
        "  }\n",
        "CONFIG = RUN_CONFIG_DEFAULTS[RUN_MODE]\n",
        "\n",
        "reset_seed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0010 train cost = 7.500e-02 train ppl = 1.078e+00 train acc = 0.089 eval cost = 7.500e-02 eval ppl = 1.078e+00 eval acc = 0.105\n",
            "Epoch: 0020 train cost = 5.530e-02 train ppl = 1.057e+00 train acc = 0.093 eval cost = 5.530e-02 eval ppl = 1.057e+00 eval acc = 0.052\n",
            "Epoch: 0030 train cost = 3.419e-02 train ppl = 1.035e+00 train acc = 0.075 eval cost = 3.419e-02 eval ppl = 1.035e+00 eval acc = 0.120\n",
            "Epoch: 0040 train cost = 2.894e-02 train ppl = 1.029e+00 train acc = 0.278 eval cost = 2.894e-02 eval ppl = 1.029e+00 eval acc = 0.231\n",
            "Epoch: 0050 train cost = 2.662e-02 train ppl = 1.027e+00 train acc = 0.368 eval cost = 2.662e-02 eval ppl = 1.027e+00 eval acc = 0.400\n",
            "Epoch: 0060 train cost = 2.354e-02 train ppl = 1.024e+00 train acc = 0.513 eval cost = 2.354e-02 eval ppl = 1.024e+00 eval acc = 0.444\n",
            "Epoch: 0070 train cost = 2.193e-02 train ppl = 1.022e+00 train acc = 0.477 eval cost = 2.193e-02 eval ppl = 1.022e+00 eval acc = 0.471\n",
            "Epoch: 0080 train cost = 1.702e-02 train ppl = 1.017e+00 train acc = 0.565 eval cost = 1.702e-02 eval ppl = 1.017e+00 eval acc = 0.562\n",
            "Epoch: 0090 train cost = 9.133e-03 train ppl = 1.009e+00 train acc = 0.718 eval cost = 9.133e-03 eval ppl = 1.009e+00 eval acc = 0.679\n",
            "Epoch: 0100 train cost = 1.119e-02 train ppl = 1.011e+00 train acc = 0.644 eval cost = 1.119e-02 eval ppl = 1.011e+00 eval acc = 0.617\n",
            "Epoch: 0110 train cost = 7.705e-03 train ppl = 1.008e+00 train acc = 0.595 eval cost = 7.705e-03 eval ppl = 1.008e+00 eval acc = 0.610\n",
            "Epoch: 0120 train cost = 9.538e-03 train ppl = 1.010e+00 train acc = 0.620 eval cost = 9.538e-03 eval ppl = 1.010e+00 eval acc = 0.565\n",
            "Epoch: 0130 train cost = 4.565e-03 train ppl = 1.005e+00 train acc = 0.742 eval cost = 4.565e-03 eval ppl = 1.005e+00 eval acc = 0.627\n",
            "Epoch: 0140 train cost = 5.033e-03 train ppl = 1.005e+00 train acc = 0.743 eval cost = 5.033e-03 eval ppl = 1.005e+00 eval acc = 0.662\n",
            "Epoch: 0150 train cost = 8.141e-03 train ppl = 1.008e+00 train acc = 0.648 eval cost = 8.141e-03 eval ppl = 1.008e+00 eval acc = 0.685\n",
            "Epoch: 0160 train cost = 4.281e-03 train ppl = 1.004e+00 train acc = 0.750 eval cost = 4.281e-03 eval ppl = 1.004e+00 eval acc = 0.753\n",
            "Epoch: 0170 train cost = 4.541e-03 train ppl = 1.005e+00 train acc = 0.872 eval cost = 4.541e-03 eval ppl = 1.005e+00 eval acc = 0.872\n",
            "Epoch: 0180 train cost = 4.084e-03 train ppl = 1.004e+00 train acc = 0.826 eval cost = 4.084e-03 eval ppl = 1.004e+00 eval acc = 0.857\n",
            "Epoch: 0190 train cost = 2.362e-03 train ppl = 1.002e+00 train acc = 0.912 eval cost = 2.362e-03 eval ppl = 1.002e+00 eval acc = 0.933\n",
            "Epoch: 0200 train cost = 2.570e-03 train ppl = 1.003e+00 train acc = 0.950 eval cost = 2.570e-03 eval ppl = 1.003e+00 eval acc = 0.951\n"
          ]
        }
      ],
      "source": [
        "# load and create dataset\n",
        "train_wikitext_subset = load_dataset('wikitext', 'wikitext-2-v1', split=f'train[:{CONFIG[dataset_config][\"train\"]}]')\n",
        "train_text_data = train_wikitext_subset['text']\n",
        "\n",
        "validation_wikitext_subset = load_dataset('wikitext', 'wikitext-2-v1', split=f'train[:{CONFIG[dataset_config][\"validation\"]}]')\n",
        "validation_text_data = validation_wikitext_subset['text']\n",
        "\n",
        "max_len = 1024\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', max_len=max_len)\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length=max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.input_ids = []\n",
        "        self.attention_masks = []\n",
        "        for text in texts:\n",
        "            encoded_text = tokenizer(text, max_length=max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
        "            self.input_ids.append(encoded_text.input_ids)\n",
        "            self.attention_masks.append(encoded_text.attention_mask)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {'input_ids': self.input_ids[idx].squeeze(), 'attention_mask': self.attention_masks[idx].squeeze()}\n",
        "\n",
        "train_dataset = TextDataset(train_text_data, tokenizer)\n",
        "validation_dataset = TextDataset(validation_text_data, tokenizer)\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=data_collator)\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size=32, shuffle=False, collate_fn=data_collator)\n",
        "\n",
        "\n",
        "# BERT model setup\n",
        "config = BertConfig(\n",
        "        hidden_size=300, # Embedding dimensions (Fix 1: Change to multiple of num_heads=6)\n",
        "        max_position_embeddings=max_len,\n",
        "        type_vocab_size=1,\n",
        "        num_attention_heads=CONFIG[model_config]['num_heads'],\n",
        "        hidden_act=\"gelu\",\n",
        "        intermediate_size=300, # dimension of feedforward expansion (Fix 2: Change to same as hidden_size=60)\n",
        "        num_hidden_layers=CONFIG[model_config]['num_layers'],\n",
        "        initializer_range=0.02,\n",
        "        device=device\n",
        ")\n",
        "model = models.Ex5BertForMaskedLM(config=config, embeddings=Ex5BertEmbeddings, selfoutput=Ex5BertSelfAttentionLinearOutput)\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer set up\n",
        "optimizer = optim.AdamW(model.parameters(), lr=CONFIG[training]['lr'])\n",
        "\n",
        "# Train loop\n",
        "epochs, train_losses, val_losses = [], [], []\n",
        "for epoch in range(CONFIG[training]['epochs']):\n",
        "    epoch_loss, total_masked_tokens, correct_masked_predictions = 0.0, 0, 0\n",
        "    model.train()\n",
        "    for batch in train_dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Fix 3: Clear the gradients before computing gradients w.r.t loss\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        predictions = outputs.logits\n",
        "        masked_positions = (labels != -100)\n",
        "        total_masked_tokens += torch.sum(masked_positions)\n",
        "        predicted_tokens = torch.argmax(F.softmax(predictions, dim=-1), dim=-1)\n",
        "        correct_masked_predictions += torch.sum(predicted_tokens[masked_positions] == labels[masked_positions])\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    # Train and Validation tracking\n",
        "    if (epoch + 1) % CONFIG[training]['track_every'] == 0:\n",
        "      model.eval()\n",
        "      train_accuracy = correct_masked_predictions.float() / total_masked_tokens.float()\n",
        "      avg_train_loss = epoch_loss / len(train_dataloader)\n",
        "      train_perplexity = math.exp(avg_train_loss)\n",
        "\n",
        "      total_eval_loss = 0\n",
        "      for batch in validation_dataloader:\n",
        "        with torch.no_grad():\n",
        "          input_ids = batch['input_ids'].to(device)\n",
        "          attention_mask = batch['attention_mask'].to(device)\n",
        "          labels = batch['labels'].to(device)\n",
        "\n",
        "          outputs = model(\n",
        "              input_ids=input_ids,\n",
        "              attention_mask=attention_mask,\n",
        "              labels=labels)\n",
        "          total_eval_loss += loss.item()\n",
        "\n",
        "          predictions = outputs.logits\n",
        "          masked_positions = (labels != -100)\n",
        "          total_masked_tokens += torch.sum(masked_positions)\n",
        "          predicted_tokens = torch.argmax(F.softmax(predictions, dim=-1), dim=-1)\n",
        "          correct_masked_predictions += torch.sum(predicted_tokens[masked_positions] == labels[masked_positions])\n",
        "      avg_eval_loss = total_eval_loss / len(validation_dataloader)\n",
        "      eval_perplexity = math.exp(avg_eval_loss)\n",
        "      eval_accuracy = correct_masked_predictions.float() / total_masked_tokens.float()\n",
        "      \n",
        "      epochs.append(epoch + 1)\n",
        "      train_losses.append(avg_train_loss)\n",
        "      val_losses.append(avg_eval_loss)\n",
        "      print(\n",
        "          'Epoch:', '%04d' % (epoch + 1),\n",
        "          'train cost =', '{:.3e}'.format(avg_train_loss),\n",
        "          'train ppl =', '{:.3e}'.format(train_perplexity),\n",
        "          'train acc =', '{:.3f}'.format(train_accuracy),\n",
        "          'eval cost =', '{:.3e}'.format(avg_eval_loss),\n",
        "          'eval ppl =', '{:.3e}'.format(eval_perplexity),\n",
        "          'eval acc =', '{:.3f}'.format(eval_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## HINTS\n",
        "\n",
        "If you're finding the task challenging, hints are available for each bug. We recommend attempting to solve the issues on your own first. Remember, the teaching assistants are also a resource for any questions you have. We're here to assist you!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### HINT 1: Dimension mismatch 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When using multi-head attention, how should the dimensions of the input relate to the number of heads?\n",
        "\n",
        "**Answer**: The input dimension should be divisible by the number of heads as the input is split and then individually processed by each head bfore being concatenated back together. As the number of heads is set to 6 in the configuration, I have changed the input dimension to from `50` to `300`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### HINT 2: Dimension mismatch 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What is the output dimension of the multi-head self-attention layer?\n",
        "\n",
        "**Answer**: The output dimension of the multi-head attention layer is the same as the input dimension, thus $B \\times S \\times D$, where $B$. Thus, we need to set the `intermediate_size` also to `60`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### HINT 3: Flat loss "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Is your model learning anything? Are your update steps correct? Carefully check the training loop...\n",
        "\n",
        "**Answer**: No, in the code we are clearing out the gradients using `optimizer.zero_grad()` right after we have compute the gradient w.r.t. the loss value. Therefore, when calling `optimizer.step()` the parameters do not change and we do not learn. To fix the issue, we simply clear out the gradients before the `loss.backward()` call."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### HINT 4: Oscillating loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Is your loss oscillating or diverging? What parameter settings could be causing this behavior?\n",
        "\n",
        "**Answer**: The learning rate is too high at `0.05`. We can stabilise the training by reducing the learning rate. A sensible defaul for the `AdamW` optimiser is `0.001` ($1e^{-3}$)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### HINT 5: High perplexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Is the loss/perplexity at the first epoch too large? What parameter setting influences the training dynamics at the start of training besides the learning rate?\n",
        "\n",
        "**Answer**: Could be the weight initialisation, the initial embeddings, or the batch size. It seems that the `BertConfig` class from the `transformers` library allows to control the weight initialisation via the `initializer_range` parameter. I have set it to `0.02` as is the default for the BERT model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### HINT 6: Mediocre results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Is your model training, but does not reach a good performance? Is the model architecture correct? Is there some module/element missing in your layers? Note that you can expect to find all bugs in the notebook itself.\n",
        "\n",
        "**Answer**: The embedding module does not any positional information. As a quick fix, we can add a positional embedding using a regular nn.Embedding module which learns a positional embedding for each position in the sequence. We can then add the positional embedding to the token embedding before feeding it into the transformer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### HINT 7: Memory overflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There might be a layer that uses a large amount of memory... Can you think of which one? What parameter could you change to mitigate this problem?\n",
        "\n",
        "**Answer**: The embedding layer `Ex5Embedding` uses a lot of memory as it has to store the embeddings over the entire vocabulary. This is a `VOCAB_SIZE * EMBEDDING_DIM` tensor. We can reduce the memory usage by reducing the size of the vocabulary or the embedding dimension."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### HINT 8: Too good to be true..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Considering the small amout of data used to train our model, does the validation performance behave as expected? When does this happen?\n",
        "\n",
        "**Answer**: We are achieving a training and validation performance of 100% accuracy. This is too good to be true. We are overfitting easily because we are using a very small dataset. We can fix this by using a larger dataset."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "navigate_num": "#000000",
        "navigate_text": "#333333",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700",
        "sidebar_border": "#EEEEEE",
        "wrapper_background": "#FFFFFF"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "264px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 4,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false,
      "widenNotebook": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
