{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS-502 Homework 2: Graph Neural Networks\n",
    "\n",
    "Author: Mika Senghaas (mika.senghaas@epfl.ch)\n",
    "\n",
    "This homework assignment implements a custom **graph neural network** (GNN) in pure [Pytorch](https://pytorch.org) and perform experiments on the [MUTAG]() dataset for graph classficiation of chemcical compounds. MUTAG consists of a collection of chemcical compounds, each represented as a graph. Here, *nodes* are atoms and identified by the atom type, *edges* are chemical bounds between the atoms with features indicating the chemical bond type. Each graph represents a chemical compound and is labelled as either *mutagenic* (positive) or *non-mutagenic* (negative) class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Setup\n",
    "\n",
    "We import the necessary modules and set global parameters. Note, that this notebook was run in the lastest minor release of Python `3.9`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "import json\n",
    "import shutil\n",
    "import random\n",
    "from itertools import product\n",
    "from collections import Counter\n",
    "\n",
    "# External libraries\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "BASE_PATH = os.getcwd()\n",
    "DATA_PATH = os.path.join(BASE_PATH, 'data', 'mutag.jsonl')\n",
    "PLOT_PATH = os.path.join(BASE_PATH, 'plots')\n",
    "\n",
    "# Check if data exists and is in the right place\n",
    "assert os.path.exists(DATA_PATH), f'❌ Error: Please download the data and place it in {DATA_PATH}'\n",
    "\n",
    "# Create plot path if it doesn't exist\n",
    "shutil.rmtree(PLOT_PATH)\n",
    "os.makedirs(PLOT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Set plot styles\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Set float precision for pandas\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "\n",
    "# Test data\n",
    "test_x = torch.eye(3)\n",
    "test_adj = torch.randint(0, 2, (3, 3)).float()\n",
    "test_e = torch.randint(0, 2, (3, 3, 4)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Meta(type):\n",
    "    def __repr__(cls):\n",
    "        return cls.__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implementing Different Graph Convolution and Pooling Layers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Convolution (Graph Convolution)\n",
    "\n",
    "A regular graph convolution in the $l$-th layer computes the embedding of the $v$-th node, $\\mathbf{h}_v$ through\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_v^{(l+1)} = \\sigma\\left( \\mathbf{W}_l \\sum_{u\\in N(v)} \\frac{\\mathbf{h}_u^{(l)}}{|N(v)|} + \\mathbf{B}_l \\mathbf{h}_v^{(l)} \\right).\n",
    "$$\n",
    "\n",
    "Note, that here $\\sigma$ is a non-linearity, $\\mathbf{W}_l$ and $\\mathbf{B}_l$ are both trainable weight matrices with dimension \n",
    "$\\text{in\\_features} \\times \\text{out\\_features}$ and $N(v)$ is the set of adjacent nodes in the graph.\n",
    "\n",
    "We can represent the average over the neighbourhood of node $v$ through a matrix product of the adjacency matrix $\\mathbf{A}$ with the matrix $\\mathbf{H}^{(l)}=\\left[\\mathbf{h}^{(l)}_1, ..., \\mathbf{h}^{(l)}_{|V|}\\right]$ holding all node embeddings in the $l$ layer (corrected by the inverse of the degree $\\tilde{A}$ to obtain an average) as\n",
    "\n",
    "$$\n",
    "H^{(l+1)} = \\sigma\\left( \\tilde{A}H^{(l)}W_l^\\top + H^{(l)}B_l^\\top \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConv(nn.Module, metaclass=Meta):\n",
    "    \"\"\"Basic graph convolutional layer implementing the simple neighborhood aggregation.\"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, activation=None):\n",
    "        \"\"\"\n",
    "        Initialize the graph convolutional layer.\n",
    "        \n",
    "        Args:\n",
    "            in_features (int): number of input node features.\n",
    "            out_features (int): number of output node features.\n",
    "            activation (nn.Module or callable): activation function to apply. (optional)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Save parameters\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # Linear transformation layers\n",
    "        self.weight = nn.Linear(in_features, out_features, bias=False)\n",
    "        self.bias = nn.Linear(in_features, out_features, bias=False)\n",
    "\n",
    "        # Non-linear activation function (optional)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"\n",
    "        Perform graph convolution operation.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input node features of shape (num_nodes, in_features).\n",
    "            adj (Tensor): Adjacency matrix of the graph, shape (num_nodes, num_nodes).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output node features after graph convolution, shape (num_nodes, out_features).\n",
    "        \"\"\"\n",
    "        # Neighbourhood aggregation\n",
    "        adj = adj / adj.sum(1, keepdim=True).clamp(1)\n",
    "        x_agg = adj @ x\n",
    "\n",
    "        # Graph convolution\n",
    "        x = self.weight(x_agg) + self.bias(x)\n",
    "\n",
    "        # Apply non-linear activation if specified\n",
    "        if self.activation:\n",
    "            return self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test graph convolution\n",
    "conv = GraphConv(3, 2)\n",
    "out = conv(test_x, test_adj)\n",
    "\n",
    "assert out.shape == (3, 2), f\"Output shape shold be 3x2 but is {out.shape}\"\n",
    "assert repr(GraphConv) == \"GraphConv\", f\"Class name should be `GraphConv`, but is {repr(GraphConv)}\"\n",
    "\n",
    "print(\"Tests passed. ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphSAGE (Customised Aggregation)\n",
    "\n",
    "GraphSAGE is a generalised version of the regular graph convolution, in which any type of aggregation can be applied to. Instead of adding the result of two matrix products and performing a non-linearity, here the original node embeddings are concatenated with the aggregated neighbourhood embeddings and then linearly transformed. The equation for the GraphSAGE layer is:\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_v^{(l+1)} = \\sigma\\left( \\mathbf{W}_l \\cdot \\mathrm{CONCAT} \\left[\\mathbf{h}_v^{(l)}, \\mathrm{AGG} \\left(\\left\\{\\mathbf{h}_u^{(l)}, \\forall u\\in N(v) \\right\\}\\right) \\right] \\right),\n",
    "$$\n",
    "\n",
    "where $v$ index the node, $l$ the layer, $\\mathbf{h}$ are the node embeddings, $\\sigma$ is a non-linearity, $N(v)$ is the set of neighbor of node $v$, and $\\mathbf{W}$ is the trainable weight matrix of the layer. $\\mathrm{CONCAT}$ is the concatenation operation, while $\\mathrm{AGG}$ is an arbitrary aggregation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregations\n",
    "class MeanAggregation(nn.Module):\n",
    "    \"\"\"Aggregate node features by averaging over the neighborhood.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        adj = adj / adj.sum(1, keepdim=True).clamp(1)\n",
    "\n",
    "        return adj @ x\n",
    "    \n",
    "class SumAggregation(nn.Module):\n",
    "    \"\"\"Aggregate node features by summing over the neighborhood.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        return adj @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGEConv(nn.Module, metaclass=Meta):\n",
    "    \"\"\"GraphSAGE convolutional layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "        in_features, \n",
    "        out_features, \n",
    "        aggregation=SumAggregation,\n",
    "        activation=None):\n",
    "        \"\"\"\n",
    "        Initialize the GraphSAGE convolutional layer.\n",
    "        \n",
    "        Args:\n",
    "            in_features (int): number of input node features.\n",
    "            out_features (int): number of output node features.\n",
    "            aggregation (nn.Module or callable): aggregation function to apply, as x_agg = aggegration(x, adj).\n",
    "            activation (nn.Module or callable): activation function to apply. (optional)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Save parameters\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # Linear transformation layer (no bias)\n",
    "        torch.manual_seed(0)\n",
    "        self.weight = nn.Linear(2*in_features, out_features, bias=False)\n",
    "\n",
    "        # Aggregation function\n",
    "        self.aggregation = aggregation()\n",
    "\n",
    "        # Non-linear activation function (optional)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"\n",
    "        Perform graph convolution operation.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input node features of shape (num_nodes, in_features).\n",
    "            adj (Tensor): Adjacency matrix of the graph, typically sparse, shape (num_nodes, num_nodes).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output node features after graph convolution, shape (num_nodes, out_features).\n",
    "        \"\"\"\n",
    "        # Neighbourhood aggregation\n",
    "        x_agg = self.aggregation(x, adj)\n",
    "\n",
    "        # Concatenate node features and aggregated features\n",
    "        x_cat = torch.cat([x, x_agg], dim=1)\n",
    "        \n",
    "        # Apply linear transformation\n",
    "        x = self.weight(x_cat)\n",
    "\n",
    "        # Apply non-linear activation if specified\n",
    "        if self.activation:\n",
    "            return self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test GraphSAGE\n",
    "conv = GraphSAGEConv(3, 2, aggregation=MeanAggregation)\n",
    "out = conv(test_x, test_adj)\n",
    "\n",
    "assert out.shape == (3, 2), f\"Output shape shold be 3x2 but is {out.shape}\"\n",
    "assert repr(GraphSAGEConv) == \"GraphSAGEConv\", f\"Class name should be `GraphSAGEConv`, but is {repr(GraphSAGEConv)}\"\n",
    "\n",
    "print(\"Tests passed. ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention-based Convolution\n",
    "\n",
    "The attention-based convolution is a generalisation of the regular graph convolution, in which the aggregation of the neighbourhood is weighted by an attention mechanism. The equation for the attention-based convolution is:\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_v^{(l+1)} = \\sigma\\left(\n",
    "    \\sum_{u \\in N(v) \\cup \\{v\\}} \\alpha_{vu}^{(l)} \\mathbf{W}_l \\mathbf{\\tilde{h}}_u^{(l)}\n",
    "    \\right)\n",
    "$$\n",
    "\n",
    "where $N(v)$ is the neighborhood of node $v$, $\\alpha_{vu}^{(l)}$ is the attention weight between node $v$ and $u$ in layer $l$ and is computed as:\n",
    "\n",
    "$$\n",
    "\\alpha_{vu}^{(l)} = \\mathrm{softmax}_{N(v)}\\left(\n",
    "    \\textrm{LeakyReLU}\\left(\n",
    "        \\mathbf{S}^T\\cdot \\textrm{CONCAT}(\\mathbf{\\tilde{h}}_v^{(l)}, \\mathbf{\\tilde{h}}_u^{(l)}\n",
    "        \\right)\n",
    "    \\right)\n",
    "$$\n",
    "\n",
    "and $\\mathbf{\\tilde{h}}_v^{(l)'}$ is the linearly transformed node embedding of node $v$ in layer $l$:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\tilde{h}}_v^{(l)} = \\mathbf{W}_l \\mathbf{h}_v^{(l)}\n",
    "$$\n",
    "\n",
    "This implementation is vectorised and computes the attention weights for all nodes in the graph at once. Note, that this implementation computes the attention weights for all node pairs and then masks the attention weights for non-neighbouring nodes to zero which might be inefficient for large graphs with sparse adjacency matrices. However, given the small size of the MUTAG dataset, this is not a problem here and the training is comparable in speed to the other implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionGraphConvolution(nn.Module, metaclass=Meta):\n",
    "    \"\"\"Attention-based convolutional layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, activation=None):\n",
    "        \"\"\"\n",
    "        Initialize the attention-based convolutional layer.\n",
    "        \n",
    "        Args:\n",
    "            in_features (int): number of input node features.\n",
    "            out_features (int): number of output node features.\n",
    "            activation (nn.Module or callable): activation function to apply. (optional)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Save parameters\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # Linear transformation layer (no bias)\n",
    "        self.weight = nn.Linear(in_features, out_features, bias=False)\n",
    "        self.att = nn.Linear(2*out_features, 1, bias=False)\n",
    "\n",
    "        # Non-linear activation function (optional)\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"\n",
    "        Perform an attention-based graph convolution operation.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input node features of shape (num_nodes, in_features).\n",
    "            adj (Tensor): Adjacency matrix of the graph, typically sparse, shape (num_nodes, num_nodes).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output node features after graph convolution, shape (num_nodes, out_features).\n",
    "        \"\"\"\n",
    "\n",
    "        # Linear transformation \n",
    "        x = self.weight(x)\n",
    "\n",
    "        # Add self-loops to the adjacency matrix\n",
    "        adj = torch.minimum(\n",
    "            adj + torch.eye(adj.shape[0]), \n",
    "            torch.ones_like(adj))\n",
    "\n",
    "        # Attention weights\n",
    "        v = len(adj)\n",
    "        u_indices = torch.cat([\n",
    "            torch.fill(torch.empty(v), i).long() \n",
    "            for i in torch.arange(v)])\n",
    "        v_indices = torch.arange(v).repeat(v)\n",
    "\n",
    "        cc = torch.cat([x[u_indices], x[v_indices]], dim=1)\n",
    "        att = self.leaky_relu(self.att(cc).reshape(v, v))\n",
    "\n",
    "        # Normalise attention weights via softmax on neighbours\n",
    "        adj_mask = torch.where(adj > 0, torch.zeros_like(adj), torch.full_like(adj, -torch.inf))\n",
    "        att = att + adj_mask\n",
    "        att = self.softmax(att)\n",
    "\n",
    "        # Attention-based aggregation\n",
    "        x = att @ x\n",
    "        \n",
    "        # Apply non-linear activation if specified \n",
    "        if self.activation:\n",
    "            return self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test attention-based convolution\n",
    "conv = AttentionGraphConvolution(3, 2)\n",
    "out = conv(test_x, test_adj)\n",
    "\n",
    "assert out.shape == (3,2), f\"Output shape shold be 3x2 but is {out.shape}\"\n",
    "assert repr(AttentionGraphConvolution) == \"AttentionGraphConvolution\", f\"Class name should be `AttentionGraphConvolution`, but is {repr(AttentionGraphConvolution)}\"\n",
    "\n",
    "print(f\"Tests passed. ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Pooling\n",
    "\n",
    "Mean pooling computes a graph level representation $\\mathbf{h}_{\\text{global}}$ as the mean (average) of all node features\n",
    "\n",
    "$$\n",
    "\\textbf{h}_{\\text{global}} = \\frac{1}{N} \\sum_{i=1}^N \\mathbf{X}_i,\n",
    "$$\n",
    "\n",
    "where $\\mathbf{X} \\in \\mathbb{R}^{N \\times D}$ where $N$ is the number of nodes and $D$ is the feature dimension. Finally, $\\mathbf{X}_i$ is node representation of the $i$-th node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module, metaclass=Meta):\n",
    "    \"\"\"Mean pooling layer.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize mean pooling layer.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Computes the average of all node features.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input node features of shape (num_nodes, in_features).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Aggregated node features of shape (in_features).\n",
    "        \"\"\"\n",
    "        return torch.mean(x, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test\n",
    "x = torch.arange(12).reshape(4, 3).float()\n",
    "meanpool = MeanPooling()\n",
    "out = meanpool(x)\n",
    "\n",
    "assert out.shape == (3,), \"Output shape should be (3, ), but is {out.shape}\"\n",
    "assert torch.equal(out, torch.Tensor([4.5, 5.5, 6.5])), f\"Output should be torch.Tensor([9, 10, 11]), but is {out}\"\n",
    "assert repr(MeanPooling) == \"MeanPooling\", f\"Class name should be MaxPooling, but is {repr(MeanPooling)}\"\n",
    "\n",
    "print(\"Tests passed. ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Pooling\n",
    "\n",
    "Max pooling computes a graph level representation $\\mathbf{h}_{\\text{global}}$ by taking the maximum value from each feature dimension across all nodes in the graph-level representation\n",
    "\n",
    "$$\n",
    "\\textbf{h}_{\\text{global}, d} =  \\max_{i=1}^N \\mathbf{X}_{i, d}\n",
    "$$\n",
    "\n",
    "for each feature dimension $d$ and again $\\mathbf{X} \\in \\mathbb{R}^{N \\times D}$ where $N$ is the number of nodes and $D$ is the feature dimension and $\\mathbf{X}_i$ is node representation of the $i$-th node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPooling(nn.Module, metaclass=Meta):\n",
    "    \"\"\"Max pooling layer.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize mean pooling layer.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Computes the max pool of all node features.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input node features of shape (num_nodes, in_features).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Max pooled node features of shape (in_features,).\n",
    "        \"\"\"\n",
    "\n",
    "        return torch.max(x, dim=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test\n",
    "x = torch.arange(12).reshape(4, 3)\n",
    "maxpool = MaxPooling()\n",
    "out = maxpool(x)\n",
    "\n",
    "assert out.shape == (3,), f\"Output shape should be (3,), but is {out.shape}\"\n",
    "assert torch.equal(out, torch.arange(9, 12)), f\"Output should be torch.Tensor([9, 10, 11]), but is {out}\"\n",
    "assert repr(MaxPooling) == \"MaxPooling\", f\"Class name should be MaxPooling, but is {repr(MaxPooling)}\"\n",
    "\n",
    "print(\"Tests passed. ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Custom Network Design with Node Features\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Network Architecture\n",
    "\n",
    "This is a generic graph neural network for binary graph classification. It can be composed of the modules from above that can through user parameters for the number of node features as input, type and number of graph convolutional layers, pooling mechanism, dropout and batch normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    \"\"\"Custom graph neural network model for binary graph prediction.\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "        num_features, \n",
    "        num_layers,\n",
    "        conv_dim,\n",
    "        conv = GraphConv, \n",
    "        pooling= MeanPooling, \n",
    "        activation= nn.LeakyReLU, \n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initialize the GNN model for graph prediction.\n",
    "\n",
    "        Args:\n",
    "            num_features (int): Number of input node features.\n",
    "            num_layers (int): Number of graph convolution layers.\n",
    "            conv_dim (int): Number of hidden features in each graph convolution layers.\n",
    "            conv (nn.Module or callable): Graph convolution layer to use.\n",
    "            pooling (nn.Module or callable): Pooling layer to use.\n",
    "            activation (nn.Module or callable): Activation function to apply.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Create UUID\n",
    "        self.uuid = uuid.uuid4().hex\n",
    "\n",
    "        # Compute dimensions and activations for graph conv layers\n",
    "        conv_dims = [conv_dim] * num_layers\n",
    "        dimensions = [num_features] + conv_dims\n",
    "        in_dimensions = dimensions[:-1]\n",
    "        out_dimensions = dimensions[1:]\n",
    "        activations = [activation] * (len(conv_dims) - 1) + [None]\n",
    "\n",
    "        # Create Graph convolution layers\n",
    "        self.convs = nn.ModuleList([\n",
    "            conv(\n",
    "                in_features, \n",
    "                out_features,\n",
    "                activation=activation() if activation else activation\n",
    "            ) for in_features, out_features, activation in \n",
    "            zip(in_dimensions, out_dimensions, activations)\n",
    "        ])\n",
    "\n",
    "        \"\"\"\n",
    "        # Batch norm layers\n",
    "        self.norms = nn.ModuleList()\n",
    "        for dim in conv_dims:\n",
    "            self.norms.append(nn.BatchNorm1d(dim))\n",
    "        \"\"\"\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.pooling = pooling()\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(conv_dims[-1] if conv_dims else num_features, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"\n",
    "        Perform forward pass for graph prediction.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input node features of shape (num_nodes, num_features).\n",
    "            adj (Tensor): Adjacency matrix of the graph, typically sparse, shape (num_nodes, num_nodes).\n",
    "        \"\"\"\n",
    "\n",
    "        # Graph convolution layers\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, adj)\n",
    "\n",
    "        # Pooling layer\n",
    "        x = self.pooling(x)\n",
    "\n",
    "        # Fully connected layer\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test architecture\n",
    "model = GNN(3, num_layers=2, conv_dim=2)\n",
    "logits = model(test_x, test_adj)\n",
    "\n",
    "assert logits.shape == (1,), f\"Expected shape (1,), but got {logits.shape}\"\n",
    "print(\"Tests passed. ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Partitioning\n",
    "\n",
    "This section loads the input data using the utility classes `torch.utils.data.Dataset`. Note, that this implementation assumes that the entire dataset is stored in JSONL format at the relative path `data/mutag.jsonl`. The dataset is then partitioned into training, validation and testing sets using the utility classes `torch.utils.data.random_split`.\n",
    "\n",
    "Further, some basic statistics and visualisations, such as the number of nodes and edges, average node degree and number of graphs are computed and printed for the entire dataset, in between the two mutagenic classes and for the training, validation and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "class MUTAGDataset(Dataset):\n",
    "    def __init__(self, datapath):\n",
    "        super().__init__()\n",
    "\n",
    "        with open(datapath, \"r\") as f:\n",
    "            raw = f.read()\n",
    "        \n",
    "        self.graphs = [json.loads(line) for line in raw.splitlines()]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of graphs in the dataset\n",
    "        \"\"\"\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a single graph's node features, edge features, adjacency matrix and label\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the graph to return.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Node features of shape (num_nodes, num_features).\n",
    "            Tensor: Adjacency matrix of shape (num_nodes, num_nodes).\n",
    "        \"\"\"\n",
    "        graph = self.graphs[idx]\n",
    "\n",
    "        # Create adjacency matrix\n",
    "        n = graph[\"num_nodes\"]\n",
    "        adj = torch.zeros((n, n))\n",
    "        d_edges = len(graph[\"edge_attr\"][0])\n",
    "        edge_attr = torch.zeros((n, n, d_edges))\n",
    "        for idx, (i, j) in enumerate(zip(graph[\"edge_index\"][0], graph[\"edge_index\"][1])):\n",
    "            adj[i, j] = 1\n",
    "            adj[j, i] = 1\n",
    "            edge_attr[i, j] = torch.Tensor(graph[\"edge_attr\"][idx])\n",
    "            edge_attr[j, i] = torch.Tensor(graph[\"edge_attr\"][idx])\n",
    "\n",
    "        node_feat = torch.Tensor(graph[\"node_feat\"])\n",
    "        label = torch.Tensor(graph[\"y\"])\n",
    "\n",
    "        return node_feat, edge_attr, adj, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset class\n",
    "data = MUTAGDataset(datapath=DATA_PATH)\n",
    "node_feat, edge_attr, adj, label = data[0]\n",
    "\n",
    "assert node_feat.shape == (17, 7), f\"Expected 17 nodes with 7 node features, but got {node_feat.shape}\"\n",
    "assert edge_attr.shape[:2] == adj.shape, f\"Edge attribute shape should be (17, 17), but is {edge_attr.shape}\"\n",
    "\n",
    "print(\"Tests passed. ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define some utility functions to do some EDA on the dataset. The functions are:\n",
    "\n",
    "- `convert_to_graph`: Converts the Tensor representation of a graph to a NetworkX graph object (used for plotting and computing statistics)\n",
    "- `plot_graph`: Plots a NetworkX graph object (with node and edge features)\n",
    "- `plot_categorical_dist`: Plots an empirical distribution of a categorical feature, here the class distribution and the node and edge type distributions\n",
    "- `compute_graph_statistics`: Computes some basic statistics of a NetworkX graph object\n",
    "- `plot_graph_statistics`: Prints the basic statistics of a NetworkX graph object in a nice format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_graph(data):\n",
    "    \"\"\"\n",
    "    Converts a single graph from its tensor representation to a networkx graph.\n",
    "    \n",
    "    Args:\n",
    "        data (tuple): Tuple of node features, edge features, adjacency matrix and label.\n",
    "\n",
    "    Returns:\n",
    "        nx.Graph: Networkx graph.\n",
    "    \"\"\"\n",
    "    # Unpack data\n",
    "    n, e, adj, label = data\n",
    "    node_types = {i: node_type.item() for i, node_type in enumerate(torch.argmax(n, dim=1))}\n",
    "    \n",
    "    # List of tuples of edge indices\n",
    "    edge_index = adj.nonzero().t().contiguous()\n",
    "    edge_index = [tuple(edge) for edge in edge_index.t().tolist()]\n",
    "\n",
    "    # List of edge labels (encode as -1 if no edge)\n",
    "    neg_edges = torch.sum(e, dim=2) == 0\n",
    "    masked_edge_labels = torch.where(neg_edges, -1, torch.argmax(e, dim=2))\n",
    "    edge_labels = [edge_label.item() for edge_label in masked_edge_labels.flatten() if edge_label.item() != -1]\n",
    "    edge_types = {(i, j): edge_type for (i,j), edge_type in zip(edge_index, edge_labels)}\n",
    "\n",
    "    # Build graph and add node and edge features\n",
    "    G = nx.from_numpy_array(adj.numpy())\n",
    "    G.name = label.item()\n",
    "    nx.set_node_attributes(G, node_types, \"node_type\")\n",
    "    nx.set_edge_attributes(G, edge_types, \"edge_type\")\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(graph, ax=None):\n",
    "    \"\"\"\n",
    "    Plot a single graph with node and edge labels. The node colours represent\n",
    "    the class of the graph (mutagenic or not).\n",
    "\n",
    "    Args:\n",
    "        graph (nx.Graph): Networkx graph.\n",
    "        ax (matplotlib.axes.Axes): Axes to plot on. (optional)\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Styles\n",
    "    colors = [\"lightblue\", \"red\"]\n",
    "    styles = {\n",
    "        \"node_size\": 100,\n",
    "        \"edge_color\": \"grey\",\n",
    "        \"with_labels\": True,\n",
    "        \"font_size\": 8,\n",
    "    }\n",
    "\n",
    "    pos = nx.spring_layout(graph)\n",
    "    nx.draw(\n",
    "        graph, \n",
    "        pos=pos, \n",
    "        node_color=colors[int(graph.name)],\n",
    "        labels=nx.get_node_attributes(graph, \"node_type\"),\n",
    "        ax=ax,\n",
    "        **styles,\n",
    "    )\n",
    "\n",
    "    nx.draw_networkx_edge_labels(\n",
    "        graph, \n",
    "        pos=pos, \n",
    "        edge_labels=nx.get_edge_attributes(graph, \"edge_type\"),\n",
    "        font_size=8,\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "    ax.set_title(f\"Class: {int(graph.name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_categorical_dist(x, hue= None, title = None, ax=None):\n",
    "    \"\"\"\n",
    "    Plot the distribution of a categorical variable.\n",
    "\n",
    "    Args:\n",
    "        x (dict): Dictionary of class counts.\n",
    "        title (str): Title of the plot. (optional)\n",
    "        ax (matplotlib.axes.Axes): Axes to plot on. (optional)\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    sns.countplot(x=x, hue=hue, ax=ax, stat=\"count\")\n",
    "    ax.set(\n",
    "        xlabel=\"Class\",\n",
    "        ylabel=\"Count\",\n",
    "        title=\"Distribution of classes\" if title is None else title,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_graph_statistics(graphs):\n",
    "    \"\"\"\n",
    "    Print some basic statistics about a set of graphs.\n",
    "\n",
    "    Args:\n",
    "        graphs (list of nx.Graph): List of graphs.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of graph statistics.\n",
    "    \"\"\"\n",
    "    # Number of graphs\n",
    "    num_graphs = len(graphs)\n",
    "\n",
    "    # Average number of nodes and edges\n",
    "    avg_num_nodes = sum([graph.number_of_nodes() for graph in graphs]) / num_graphs\n",
    "    avg_num_edges = sum([graph.number_of_edges() for graph in graphs]) / num_graphs\n",
    "    compute_avg_degree = lambda graph: sum(dict(graph.degree).values()) / len(graph.degree)\n",
    "    avg_degree = sum([compute_avg_degree(graph) for graph in graphs]) / num_graphs\n",
    "    full_connectivity = sum([nx.is_connected(graph) for graph in graphs]) / num_graphs\n",
    "    \n",
    "    # Class distribution\n",
    "    classes = [int(graph.name) for graph in graphs]\n",
    "    class_dist = Counter(classes)\n",
    "    pos_ratio = class_dist[1] / num_graphs\n",
    "\n",
    "    # Node type distribution\n",
    "    node_types = [node_type for graph in graphs for node_type in nx.get_node_attributes(graph, \"node_type\").values()]\n",
    "    node_type_dist = Counter(node_types)\n",
    "\n",
    "    # Edge type distribution\n",
    "    edge_types = [edge_type for graph in graphs for edge_type in nx.get_edge_attributes(graph, \"edge_type\").values()]\n",
    "    edge_type_dist = Counter(edge_types)\n",
    "\n",
    "    return {\n",
    "        \"#Graphs\": num_graphs,\n",
    "        \"Avg. #Nodes\": avg_num_nodes,\n",
    "        \"Avg. #Edges\": avg_num_edges,\n",
    "        \"Avg. Degree\": avg_degree,\n",
    "        \"Full Connectivity\": full_connectivity,\n",
    "        \"Positive Ratio\": pos_ratio,\n",
    "        \"Classes\": classes,\n",
    "        \"Class Distribution\": class_dist,\n",
    "        \"Node Types\": node_types,\n",
    "        \"Node Type Distribution\": node_type_dist,\n",
    "        \"Edge Types\": edge_types,\n",
    "        \"Edge Type Distribution\": edge_type_dist,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_statistics(*statistics, index):\n",
    "    columns = [\"#Graphs\", \"Avg. #Nodes\", \"Avg. #Edges\", \"Avg. Degree\", \"Full Connectivity\", \"Positive Ratio\"]\n",
    "    return pd.DataFrame(statistics, index=index)[columns].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all graphs in a list\n",
    "all_graphs = [convert_to_graph(data[idx]) for idx in range(len(data))]\n",
    "\n",
    "# Save positive and negative examples in separate lists\n",
    "positive_graphs = [graph for graph in all_graphs if graph.name == 1]\n",
    "negative_graphs = [graph for graph in all_graphs if graph.name == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the dataset\n",
    "fig, ax = plt.subplots(3, 3, figsize=(15, 15))\n",
    "for i in range(9):\n",
    "    # Draw with node colour and edge colour encoding node and edge types\n",
    "    plot_graph(all_graphs[i], ax=ax[i//3, i%3])\n",
    "\n",
    "fig.savefig(os.path.join(PLOT_PATH, \"dataset.png\"), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print positive vs. negative sample side-by-side\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(8, 4))\n",
    "plot_graph(positive_graphs[0], ax=ax[0])\n",
    "plot_graph(negative_graphs[0], ax=ax[1])\n",
    "\n",
    "# Save figure\n",
    "fig.savefig(os.path.join(PLOT_PATH, \"pos_vs_neg.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some basic statistics about the dataset\n",
    "statistics = compute_graph_statistics(all_graphs)\n",
    "\n",
    "# Plot class, node type and edge type distribution\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(20, 5))\n",
    "plot_categorical_dist(statistics[\"Classes\"], ax=ax[0], title=\"Class Type Distribution\")\n",
    "plot_categorical_dist(statistics[\"Node Types\"], ax=ax[1], title=\"Node Type Distribution\")\n",
    "plot_categorical_dist(statistics[\"Edge Types\"], ax=ax[2], title=\"Edge Type Distribution\")\n",
    "\n",
    "# Save plot\n",
    "fig.savefig(os.path.join(PLOT_PATH, \"mutag_statistics.png\"), dpi=300)\n",
    "\n",
    "display_statistics(statistics, index=[\"MUTAG\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some basic statistics about the positive and negative graphs\n",
    "pos_statistics = compute_graph_statistics(positive_graphs)\n",
    "neg_statistics = compute_graph_statistics(negative_graphs)\n",
    "\n",
    "# Plot class, node type and edge type distribution\n",
    "node_types = pos_statistics[\"Node Types\"] + neg_statistics[\"Node Types\"]\n",
    "hue_node_types = [1] * len(pos_statistics[\"Node Types\"]) + [0] * len(neg_statistics[\"Node Types\"])\n",
    "edge_types = pos_statistics[\"Edge Types\"] + neg_statistics[\"Edge Types\"]\n",
    "hue_edge_types = [1] * len(pos_statistics[\"Edge Types\"]) + [0] * len(neg_statistics[\"Edge Types\"])\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(20, 5))\n",
    "plot_categorical_dist(node_types, hue=hue_node_types, ax=ax[0], title=\"Node Type Distribution\")\n",
    "plot_categorical_dist(edge_types, hue=hue_edge_types, ax=ax[1], title=\"Edge Type Distribution\")\n",
    "\n",
    "# Save plot\n",
    "fig.savefig(os.path.join(PLOT_PATH, \"pos_vs_neg_statistics.png\"), dpi=300)\n",
    "\n",
    "display_statistics(pos_statistics, neg_statistics, index=[\"Positive\", \"Negative\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data splitting\n",
    "n_val = int(0.15 * len(data))\n",
    "n_test = int(0.15 * len(data))\n",
    "n_train = len(data) - n_val - n_test\n",
    "split_num_samples = [n_train, n_val, n_test]\n",
    "\n",
    "# Split dataset randomly\n",
    "train_data, val_data, test_data = random_split(data, split_num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick EDA\n",
    "train_statistics = compute_graph_statistics([convert_to_graph(train_graph) for train_graph in train_data])\n",
    "val_statistics = compute_graph_statistics([convert_to_graph(val_graph) for val_graph in val_data])\n",
    "test_statistics = compute_graph_statistics([convert_to_graph(test_graph) for test_graph in test_data])\n",
    "\n",
    "display_statistics(train_statistics, val_statistics, test_statistics, index=[\"Train\", \"Validation\", \"Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = train_statistics[\"Positive Ratio\"]\n",
    "val_ratio = val_statistics[\"Positive Ratio\"]\n",
    "test_ratio = test_statistics[\"Positive Ratio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_size = 1\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "\n",
    "This section trains graph neural networks in different configurations (hyperparameters). To do so, we first define a set of utility functions:\n",
    "\n",
    "- `validate`: Computes relevant classification metrics of a trained model on a validation or test split\n",
    "- `train_epoch`: Trains a model for one epoch on a training split\n",
    "- `train`: Trains a model for a given number of epochs on a training split and evaluates the model on a validation split after each epoch\n",
    "- `plot_training_history`: Plots the training and validation history of a model (loss and macro F1 score) for each epoch\n",
    "- `build_grid`: Builds a grid of hyperparameters to be tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, data_loader, criterion, use_edges=False, verbose=False):\n",
    "    \"\"\"\n",
    "    Test model on data split using common classification accuracy.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Model to be tested.\n",
    "        data_loader (DataLoader): Data loader containing the data.\n",
    "        criterion (nn.Module): Loss function to use.\n",
    "        use_edges (boolean): Whether to use edge features or not.\n",
    "        verbose (boolean): Whether to print classification report or not. (optional)\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the model's performance on the data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    loss = 0.\n",
    "    all_preds, all_targets = [], []\n",
    "    for batch in data_loader:\n",
    "        # Extract node features, edge features, adjacency matrix and labels\n",
    "        node_feats, edges, adjs, labels = batch\n",
    "\n",
    "        # Forward pass\n",
    "        neighs = edges if use_edges else adjs\n",
    "        logits = model(node_feats.squeeze(), neighs.squeeze())\n",
    "        probs = F.sigmoid(logits)\n",
    "        preds = probs.round()\n",
    "\n",
    "        # Compute loss\n",
    "        batch_loss = criterion(logits, labels.reshape(-1))\n",
    "        loss += batch_loss.item()\n",
    "\n",
    "        # Save predictions and targets for later\n",
    "        all_preds.append(preds.item())\n",
    "        all_targets.append(labels.item())\n",
    "\n",
    "    # Compute classification metrics\n",
    "    loss /= len(data_loader)\n",
    "    acc = accuracy_score(all_targets, all_preds)\n",
    "    f1 = f1_score(all_targets, all_preds, zero_division=0., average=\"macro\")\n",
    "    conf_matrix = confusion_matrix(all_targets, all_preds)\n",
    "\n",
    "    # Print classification report (if verbose flag is set)\n",
    "    if verbose:\n",
    "        test_classification_report = classification_report(all_targets, all_preds, zero_division=0.)\n",
    "        print(test_classification_report)\n",
    "\n",
    "    return {\n",
    "        \"loss\": loss,\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"confusion_matrix\": conf_matrix,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, criterion, optimiser, use_edges=False):\n",
    "    \"\"\"\n",
    "    Train model on training data using specified loss function and optimiser.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Model to be tested.\n",
    "        data_loader (DataLoader): Data loader containing the training data.\n",
    "        criterion (nn.Module): Loss function to be optimized.\n",
    "        optimiser (nn.optim.Optimizer): Optimiser to use for training.\n",
    "        use_edges (boolean): Whether to use edge features or not.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tuple containing the training loss and accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set model into training mode\n",
    "    model.train()\n",
    "\n",
    "    train_loss, train_acc, train_f1 = 0., 0., 0.\n",
    "    for batch in data_loader:\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # Extract data features\n",
    "        node_feats, edges, adjs, labels = batch\n",
    "\n",
    "        # Forward pass\n",
    "        neighs = edges if use_edges else adjs\n",
    "        logits = model(node_feats.squeeze(), neighs.squeeze())\n",
    "        probs = F.sigmoid(logits)\n",
    "        preds = probs.round()\n",
    "\n",
    "        # Compute loss value and update weights\n",
    "        batch_loss = criterion(logits, labels.reshape(-1))\n",
    "        batch_loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        # Compute batch accuracy, f1\n",
    "        labels, preds = labels.detach(), preds.detach()\n",
    "        batch_acc = accuracy_score(labels, preds)\n",
    "        batch_f1 = f1_score(labels, preds, zero_division=0., average=\"macro\")\n",
    "\n",
    "        # Update training loss and accuracy\n",
    "        train_loss += batch_loss.item()\n",
    "        train_acc += batch_acc.item()\n",
    "        train_f1 += batch_f1.item()\n",
    "\n",
    "    # Normalise training loss and acc\n",
    "    train_loss /= len(data_loader)\n",
    "    train_acc /= len(data_loader)\n",
    "    train_f1 /= len(data_loader)\n",
    "\n",
    "    return {\n",
    "        \"loss\": train_loss,\n",
    "        \"accuracy\": train_acc,\n",
    "        \"f1\": train_f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(model, train_loader, val_loader, criterion, optimiser, epochs, use_edges=False, verbose=2):\n",
    "    \"\"\"\n",
    "    Train model on training data using specified loss function and optimiser.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Model to be tested.\n",
    "        train_loader (DataLoader): Data loader containing the trainin data.\n",
    "        val_loader (DataLoader): Data loader containing the trainin data.\n",
    "        criterion (nn.Module): Loss function to be optimized.\n",
    "        optimiser (nn.optim.Optimizer): Optimiser to use for training.\n",
    "        epochs (int): Number of epochs to train for.\n",
    "        use_edges (boolean): Whether to use edge features or not.\n",
    "        verbose (int): 0 for no output, 1 for tqdm progress, 2 for batch summaries. (optional)\n",
    "\n",
    "    Returns:\n",
    "        results (dict): Dictionary containing the model's final performance on the data training and validation data and history of loss and accuracy on both splits.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialise training loss and accuracy\n",
    "    metrics = [\"train_loss\", \"train_acc\", \"train_f1\", \"val_loss\", \"val_acc\", \"val_f1\"]\n",
    "    history = {metric: [] for metric in metrics}\n",
    "    pbar = tqdm(range(epochs), disable=verbose != 1)\n",
    "    for epoch in pbar:\n",
    "        # Train model\n",
    "        train_results = train_epoch(model, train_loader, criterion, optimiser, use_edges=use_edges)\n",
    "        train_loss = train_results[\"loss\"]\n",
    "        train_acc = train_results[\"accuracy\"]\n",
    "        train_f1 = train_results[\"f1\"]\n",
    "\n",
    "        # Validate model\n",
    "        val_results = validate(model, val_loader, criterion, use_edges=use_edges)\n",
    "        val_loss = val_results[\"loss\"]\n",
    "        val_acc = val_results[\"accuracy\"]\n",
    "        val_f1 = val_results[\"f1\"]\n",
    "\n",
    "        # Save training/ validation loss and accuracy\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"train_f1\"].append(train_f1)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        history[\"val_f1\"].append(val_f1)\n",
    "\n",
    "        progress= \" | \".join([\n",
    "            f\"{epoch+1}/{epochs}\",\n",
    "            f\"Train {train_loss:.4f} ({(100*train_acc):.1f}%)\",\n",
    "            f\"Val {val_loss:.4f} ({(100*val_acc):.1f}%)\"\n",
    "        ])\n",
    "\n",
    "        # Verbose output\n",
    "        if verbose == 1:\n",
    "            pbar.set_description(progress)\n",
    "        elif verbose == 2:\n",
    "            print(progress)\n",
    "\n",
    "    results = {\n",
    "        \"train_results\": {\n",
    "            \"loss\": history[\"train_loss\"][-1], \n",
    "            \"accuracy\": history[\"train_acc\"][-1]},\n",
    "            \"f1\": history[\"train_f1\"][-1],\n",
    "        \"val_results\": val_results,\n",
    "        \"history\": history,\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(train_results, kwargs={}):\n",
    "    \"\"\"\n",
    "    Plots the training and validation loss (left subplot) and accuracy (right subplot) over the training epochs saved in the history dictionary.\n",
    "\n",
    "    Args:\n",
    "        history (dict): Dictionary containing the training and validation loss and accuracy.\n",
    "\n",
    "    Returns:\n",
    "        fig (matplotlib.pyplot.figure): Figure containing the training history plots (can be used for saving the figure)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create figure with two subplots\n",
    "    fig, axs  = plt.subplots(ncols=3, figsize=(20, 4))\n",
    "\n",
    "    # Extract history of train/val loss and accuracy\n",
    "    history = train_results[\"history\"]\n",
    "\n",
    "    # Plot train/val loss\n",
    "    sns.lineplot(history[\"train_loss\"], label=\"Train Loss\", ax=axs[0])\n",
    "    sns.lineplot(history[\"val_loss\"], label=\"Val Loss\", ax=axs[0])\n",
    "\n",
    "    # Plot train/val accuracy\n",
    "    sns.lineplot(history[\"train_acc\"], label=\"Train Acc\", ax=axs[1])\n",
    "    sns.lineplot(history[\"val_acc\"], label=\"Val Acc\", ax=axs[1])\n",
    "\n",
    "    # Plot train/val f1\n",
    "    sns.lineplot(history[\"train_f1\"], label=\"Train -F1\", ax=axs[2])\n",
    "    sns.lineplot(history[\"val_f1\"], label=\"Val F1\", ax=axs[2])\n",
    "\n",
    "    # Set plot labels\n",
    "    for ax in axs:\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.legend()\n",
    "    axs[0].set_ylabel(\"Loss\")\n",
    "    axs[1].set_ylabel(\"Accuracy\")\n",
    "    axs[2].set_ylabel(\"F1\")\n",
    "    axs[1].set_ylim(0, 1)\n",
    "\n",
    "    params = \",\".join([f\"{k}: {v}\" for k, v in kwargs.items() if k != 'model'])\n",
    "    fig.suptitle(f\"Training History ({params})\")\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_grid(hyperparams):\n",
    "    \"\"\"\n",
    "    Builds a grid of hyperparameters to be tested.\n",
    "\n",
    "    Args:\n",
    "        hyperparams (dict): Dictionary of hyperparameters and their values and an iterable of values to test.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: List of hyperparameter combinations to test, each as dictionary of hyperparameter names and value. Length is the product of the number of values for each hyperparameter.\n",
    "    \"\"\"\n",
    "    return [dict(zip(hyperparams.keys(), values)) for values in product(*hyperparams.values())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each hyperparameter, we define a set of values to try out. Then, we create a grid of all possible combinations of hyperparameters. For each combination, we train a model and evaluate it on the validation set and save the results in a dictionary in order to analyse the results later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_HYPERPARAMS = {\n",
    "    \"num_features\": [7],\n",
    "    \"num_layers\": [3, 5, 7],\n",
    "    \"conv_dim\": [8, 32, 64],\n",
    "    \"conv\": [GraphConv, GraphSAGEConv, AttentionGraphConvolution],\n",
    "    \"pooling\": [MeanPooling, MaxPooling],\n",
    "}\n",
    "TRAIN_HYPERPARAMS = {\n",
    "    \"learning_rate\": [1e-2, 1e-3],\n",
    "    \"epochs\": [100]\n",
    "}\n",
    "\n",
    "# Build grid\n",
    "model_grid = build_grid(MODEL_HYPERPARAMS)\n",
    "train_grid = build_grid(TRAIN_HYPERPARAMS)\n",
    "\n",
    "print(f\"Testing {len(model_grid)} model configurations for {len(train_grid)} training configurations. Total of {len(model_grid) * len(train_grid)} experiments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the experiments for the first three convolutional layers. The results are saved in the dictionary `RESULTS` and are be analysed in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments\n",
    "RESULTS = {}\n",
    "for i, model_hyperparams in enumerate(model_grid):\n",
    "    for j, train_hyperparams in enumerate(train_grid):\n",
    "        model = GNN(**model_hyperparams)\n",
    "        RESULTS[model.uuid] = EXPERIMENT_RESULTS = {}\n",
    "\n",
    "        # Compute number of trainable parameters\n",
    "        num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "        # Save model and training hyperparameters (experiment meta information)\n",
    "        params = {**model_hyperparams, **train_hyperparams, \"num_params\": num_params}\n",
    "\n",
    "        print(f\"\\nModel [{i*len(train_grid)+(j+1)}/{len(model_grid) * len(train_grid)}]\")\n",
    "        print(pd.Series(params))\n",
    "\n",
    "        # Create optimizer and loss\n",
    "        optimiser = torch.optim.Adam(model.parameters(), lr=train_hyperparams[\"learning_rate\"])\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([train_ratio]))\n",
    "\n",
    "        # Train model\n",
    "        start = time.time()\n",
    "        train_results = train(model, train_loader, val_loader, criterion, optimiser, epochs=train_hyperparams[\"epochs\"], use_edges=False, verbose=1)\n",
    "        print(f\"Training complete. ✅ ({(time.time() - start):.1f}s)\")\n",
    "\n",
    "        # Save model and training results\n",
    "        EXPERIMENT_RESULTS[\"model\"] = model\n",
    "        EXPERIMENT_RESULTS[\"train_hyperparams\"] = train_hyperparams\n",
    "        EXPERIMENT_RESULTS[\"model_hyperparams\"] = model_hyperparams\n",
    "        EXPERIMENT_RESULTS[\"train_results\"] = train_results[\"train_results\"]\n",
    "        EXPERIMENT_RESULTS[\"val_results\"] = train_results[\"val_results\"]\n",
    "        EXPERIMENT_RESULTS[\"other\"] = {\"num_params\": num_params}\n",
    "\n",
    "        # Save training history plot, but don't display it inline\n",
    "        fig = plot_training_history(train_results, kwargs={**model_hyperparams, **train_hyperparams})\n",
    "        fig.savefig(os.path.join(PLOT_PATH, f\"training_curve_{model.uuid}.png\"))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation\n",
    "\n",
    "*Note, that the performance evaluation of all models is done after the implementation of the edge convolutional layer to have the entire analysis and evaluation in one place.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Incorporating Edge Features\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy for incorporating edge features\n",
    "\n",
    "The approach taken in this project for including edge features in the graph classification of the mutagenicity of a chemical compound is adapted from the **Edge Graph Convolutional Layer**, called $\\text{EGGN(C)}$ proposed in the paper [Exploiting Edge Features in Graph Neural Networks](https://arxiv.org/pdf/1809.02709.pdf), *Gong et. al*. \n",
    "\n",
    "In a graph with $N$ nodes, we define the node feature matrix $\\mathbf{H}$ as a $N \\times D$ dimensional matrix and the adjancency matrix $\\mathbf{A}$ as a $N \\times N$ binary matrix. Now, we similarly define the edge feature matrix $\\mathbf{E}$ as a $N \\times N \\times P$ dimensional matrix, where the entry at index $(i, j)$ represents the real-valued, $P$-dimensional edge feature vector of the edge between the $i$-th and $j$-th node. Given this notation it becomes clear that the edge feature matrix $\\mathbf{E}$ can be seen as an extension of the adjacency matrix $\\mathbf{A}$, where each entry is a $P$-dimensional vector instead of a scalar.\n",
    "\n",
    "Following the method proposed in the paper and the above notation, we can extend the regular graph convolution in a straight-forward way by treating each dimension of the edge feature vector as a separate *channel* to perform graph convolution over.\n",
    "\n",
    "$$\n",
    "H^{(l+1)} = \\sigma\\left(\\sum_{p=1}^P \\tilde{\\mathbf{E}}_{\\cdot\\cdot p} \\mathbf{H}^{(l)} \\mathbf{W}_l  \\right),\n",
    "$$\n",
    "\n",
    "where $\\tilde{E}_{\\cdot\\cdot p}$ is the $p$-th channel of the normalised edge feature matrix $\\mathbf{E}$ and $W_l$ is the trainable weight matrix of the $l$-th layer. The edge feature matrix is normalised using doubly stochastic normalisation, which is defined as:\n",
    "\n",
    "$$\n",
    "   \\tilde{\\mathbf{E}}_{i,j,p} = \n",
    "   \\frac{\\mathbf{E}_{i,j,p}}{\\sum_{k=1}^N \\mathbf{E}_{ikp}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "  \\tilde{\\mathbf{E}}_{i,j,p} = \n",
    "  \\sum_{k=1}^{N}\n",
    "  \\frac{\\tilde{\\mathbf{E}}_{i,k,p}\\tilde{\\mathbf{E}}_{j,k,p}}{\\sum_{v=1}^N \\mathbf{E}_{vkp}}\n",
    "$$\n",
    "\n",
    "Both the doubly-stochastic normalisation, as well as the edge convolution are implemented in a vectorised way, which makes them more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeGraphConv(nn.Module, metaclass=Meta):\n",
    "    \"\"\"Edge graph convolutional layer, adapted from the paper \"Exploting Edge Features in Graph Neural Networks\" (https://arxiv.org/pdf/1611.08945.pdf).\"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, edge_dim=4, activation=None):\n",
    "        \"\"\"\n",
    "        Initialize the edge graph convolutional layer.\n",
    "        \n",
    "        Args:\n",
    "            in_features (int): number of input node features.\n",
    "            out_features (int): number of output node features.\n",
    "            edge_dim (int): number of edge features. (optional)\n",
    "            activation (nn.Module or callable): activation function to apply. (optional)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Save parameters\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # Linear transformation layers\n",
    "        self.weights = nn.ModuleList([nn.Linear(in_features, out_features, bias=False) for _ in range(edge_dim)])\n",
    "\n",
    "        # Non-linear activation function (optional)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x, e):\n",
    "        \"\"\"\n",
    "        Perform edge graph convolution operation.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input node features of shape (num_nodes, in_features).\n",
    "            e (Tensor): Edge feature matrix of the graph, shape (num_nodes, num_nodes, num_edge_features).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output node features after graph convolution, shape (num_nodes, out_features).\n",
    "        \"\"\"\n",
    "\n",
    "        # Normalise edge feature vectors\n",
    "        e = self._doubly_stochastic_norm(e)\n",
    "\n",
    "        # Neighborhood aggregation based on edge features\n",
    "        x_agg = torch.zeros((x.shape[0], self.out_features))\n",
    "        for p in range(e.shape[2]):\n",
    "            x_agg += self.weights[p](e[:, :, p] @ x)\n",
    "\n",
    "        # Add non-linearity\n",
    "        if self.activation:\n",
    "            return self.activation(x_agg), e\n",
    "\n",
    "        return x_agg, e\n",
    "\n",
    "    def _doubly_stochastic_norm(self, E):\n",
    "        # from: https://stackoverflow.com/questions/70950648/pytorch-doubly-stochastic-normalisation-of-3d-tensor\n",
    "        E = E / torch.sum(E, dim=1, keepdim=True).clamp(1)  # normalised across rows\n",
    "        F = E / torch.sum(E, dim=0, keepdim=True).clamp(1)  # normalised across cols\n",
    "        return (E.permute(2,0,1) @ F.permute(2,1,0)).permute(1,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test EGNN\n",
    "conv = EdgeGraphConv(3, 3, edge_dim=4)\n",
    "x_out, e_out = conv(test_x, test_e)\n",
    "\n",
    "assert x_out.shape == (3, 3), f\"Output shape shold be 3x2 but is {x_out.shape}\"\n",
    "assert repr(EdgeGraphConv) == \"EdgeGraphConv\", f\"Class name should be `EdgeGraphConv`, but is {repr(EdgeGraphConv)}\"\n",
    "\n",
    "print(\"Tests passed. ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the `EdgeGraphConv` layer updates both the node and edge features in each layer, we create a new custom graph convolutional class `EGNN` which inherits from the `GraphConv` class and overrides the `forward` method to include the edge features. The `EGNN` class is then used in the `CustomNetwork` class to create a custom graph neural network with edge features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGNN(nn.Module, metaclass=Meta):\n",
    "    \"\"\"Custom graph neural network model for binary graph prediction.\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "        num_features, \n",
    "        num_layers,\n",
    "        conv_dim,\n",
    "        conv = EdgeGraphConv,\n",
    "        pooling = MeanPooling, \n",
    "        activation = nn.LeakyReLU, \n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initialize the EGNN model for graph prediction.\n",
    "\n",
    "        Args:\n",
    "            num_features (int): Number of input node features.\n",
    "            num_layers (int): Number of graph convolution layers.\n",
    "            conv_dim (int): Number of hidden features in each graph convolution layers.\n",
    "            conv_dims (list of int): Number of hidden features in each graph convolution layers.\n",
    "            activation (nn.Module or callable): Activation function to apply.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Create UUID\n",
    "        self.uuid = uuid.uuid4().hex\n",
    "\n",
    "        # Compute dimensions and activations for graph conv layers\n",
    "        conv_dims = [conv_dim] * num_layers\n",
    "        dimensions = [num_features] + conv_dims\n",
    "        in_dimensions = dimensions[:-1]\n",
    "        out_dimensions = dimensions[1:]\n",
    "        activations = [activation] * (len(conv_dims) - 1) + [None]\n",
    "\n",
    "        # Create Graph convolution layers\n",
    "        self.convs = nn.ModuleList([\n",
    "            conv(\n",
    "                in_features, \n",
    "                out_features,\n",
    "                activation=activation() if activation else activation\n",
    "            ) for in_features, out_features, activation in \n",
    "            zip(in_dimensions, out_dimensions, activations)\n",
    "        ])\n",
    "\n",
    "        \"\"\"\n",
    "        # Batch norm layers\n",
    "        self.norms = nn.ModuleList()\n",
    "        for dim in conv_dims:\n",
    "            self.norms.append(nn.BatchNorm1d(dim))\n",
    "        \"\"\"\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.pooling = pooling()\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(conv_dims[-1] if conv_dims else num_features, 1)\n",
    "\n",
    "    def forward(self, x, e):\n",
    "        \"\"\"\n",
    "        Perform forward pass for graph prediction using edge features.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input node features of shape (num_nodes, num_features).\n",
    "            e (Tensor): Edge feature matrix\n",
    "        \"\"\"\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x, e = conv(x, e)\n",
    "\n",
    "        # Pooling layer\n",
    "        x = self.pooling(x)\n",
    "\n",
    "        # Fully connected layer\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test EGNN\n",
    "model = EGNN(3, num_layers=2, conv_dim=2)\n",
    "out = model(test_x, test_e)\n",
    "\n",
    "assert out.shape == (1,), f\"Expected shape (1,), but got {out.shape}\"\n",
    "assert repr(EGNN) == \"EGNN\", f\"Class name should be `EGNN`, but is {repr(EGNN)}\"\n",
    "\n",
    "print(\"Tests passed. ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "\n",
    "Let's extend the training results by running the same hyperparameter tuning as before, but now only with the edge convolutional layer. We will append the results to the previous results and compare the performance of the two approaches in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update model hyperparameter grid to include only EGGN\n",
    "MODEL_HYPERPARAMS[\"conv\"] = [EdgeGraphConv]\n",
    "model_grid_2 = build_grid(MODEL_HYPERPARAMS)\n",
    "\n",
    "print(f\"Testing an additional {len(model_grid_2)} model configurations with {len(train_grid)} training configurations. Total of {len(model_grid_2) * len(train_grid)} experiments.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model_hyperparams in enumerate(model_grid_2):\n",
    "    for j, train_hyperparams in enumerate(train_grid):\n",
    "        # Initialise model and results\n",
    "        model = EGNN(**model_hyperparams)\n",
    "        RESULTS[model.uuid] = EXPERIMENT_RESULTS = {}\n",
    "\n",
    "        # Compute number of trainable parameters\n",
    "        num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        params = {**model_hyperparams, **train_hyperparams, \"num_params\": num_params}\n",
    "\n",
    "        print(f\"\\nModel [{i*j + (j+1)}/{len(model_grid_2) * len(train_grid)}]\")\n",
    "        print(pd.Series(params))\n",
    "\n",
    "        # Create optimizer and loss\n",
    "        optimiser = torch.optim.Adam(model.parameters(), lr=train_hyperparams[\"learning_rate\"])\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([train_ratio]))\n",
    "\n",
    "        # Train model\n",
    "        start = time.time()\n",
    "        try:\n",
    "            train_results = train(model, train_loader, val_loader, criterion, optimiser, epochs=train_hyperparams[\"epochs\"], use_edges=True, verbose=1)\n",
    "            print(f\"Training complete. ✅ ({(time.time() - start):.1f}s)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Training failed. ❌ ({(time.time() - start):.1f}s)\")\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "        # Save model and training results\n",
    "        EXPERIMENT_RESULTS[\"model\"] = model\n",
    "        EXPERIMENT_RESULTS[\"train_hyperparams\"] = train_hyperparams\n",
    "        EXPERIMENT_RESULTS[\"model_hyperparams\"] = model_hyperparams\n",
    "        EXPERIMENT_RESULTS[\"train_results\"] = train_results[\"train_results\"]\n",
    "        EXPERIMENT_RESULTS[\"val_results\"] = train_results[\"val_results\"]\n",
    "        EXPERIMENT_RESULTS[\"other\"] = {\"num_params\": num_params}\n",
    "\n",
    "        # Create training curve\n",
    "        fig = plot_training_history(train_results, {**model_hyperparams, **train_hyperparams})\n",
    "        fig.savefig(os.path.join(PLOT_PATH, f\"training_curve_{model.uuid}.png\"))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation\n",
    "\n",
    "This section evaluates the performance of the different trained models. First, we convert the collected training results to a `pd.DataFrame` and visualise the validation performance (both F1 score and accuracy) as a function of the hyperparameters we have defined, i.e. the number of layers (total number of parameters) and the type of convolutional and pooling layer.\n",
    "\n",
    "After selecting the best performing model, we retrain the same model configuration on the combined training and validation split and report the final performance on the test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_df(training_results, include_columns):\n",
    "    \"\"\"\n",
    "    Utility function for putting collected training results in a multi-indexed pd.DataFrame.\n",
    "\n",
    "    Args:\n",
    "        training_results (dict): Dictionary of training results.\n",
    "        include_columns (dict): Dictionary of columns to include in the dataframe. Keys are model names and values are lists of columns to include.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Multi-indexed dataframe containing the training results.\n",
    "    \"\"\"\n",
    "\n",
    "    # Transform dictionary\n",
    "    data = {}\n",
    "    for m in training_results.keys():\n",
    "        data[m] = {}\n",
    "        for c in training_results[m].keys():\n",
    "            if c in include_columns:\n",
    "                for p in training_results[m][c].keys():\n",
    "                    if p in include_columns[c]:\n",
    "                        data[m][(c, p)] = training_results[m][c][p]\n",
    "\n",
    "    # Create multi-indexed dataframe\n",
    "    training_results = pd.DataFrame.from_dict(data, orient=\"index\")\n",
    "    multi_indexed_columns = pd.MultiIndex.from_tuples(training_results.columns)\n",
    "    training_results.columns = multi_indexed_columns\n",
    "\n",
    "    # Sort by validation F1 score\n",
    "    training_results = training_results.sort_values(by=(\"val_results\", \"f1\"), ascending=False)\n",
    "\n",
    "    # Filter columns\n",
    "    include_columns = [(k, c) for k in include_columns.keys() for c in include_columns[k]]\n",
    "    training_results = training_results[include_columns]\n",
    "\n",
    "    return training_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_latex(results):\n",
    "    \"\"\"\n",
    "    Utility function for converting training results DataFrame to LaTeX table.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplot(df, y, x, hue=None, ax=None):\n",
    "    \"\"\"\n",
    "    Utility function for plotting boxplot of training results.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the training results.\n",
    "        y (str): Column name of the y-axis variable.\n",
    "        x (str): Column name of the x-axis variable.\n",
    "        hue (str): Column name of the hue variable. (optional)\n",
    "        ax (matplotlib.axes.Axes): Axes to plot on. (optional)\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    sns.boxplot(df, x=x, y=y, hue=hue, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(df, y, x, hue, style, ax=None):\n",
    "    \"\"\"\n",
    "    Plot scatter of performance metric vs. hyperparameter.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.scatterplot(df, x=x, y=y, hue=hue, style=style, markers=True, s=50, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(df, y, x, values, ax=None):\n",
    "    \"\"\"\n",
    "    Plot heatmap of from multi-indexed pd.DataFrame given x, y and values columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Multi-indexed DataFrame.\n",
    "        y (tuple): Tuple of column names for y-axis.\n",
    "        x (tuple): Tuple of column names for x-axis.\n",
    "        values (tuple): Tuple of column names for values.\n",
    "        ax (matplotlib.pyplot.axis): Axis to plot on. (optional)\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Pivot table\n",
    "    df_pivot = pd.concat([df[x[0]][x[1]], df[y[0]][y[1]], df[values[0]][values[1]]], axis=1).pivot_table(index=x[1], columns=y[1], values=values[1], aggfunc=\"mean\")\n",
    "    df_pivot.sort_index(ascending=False, inplace=True)\n",
    "\n",
    "    # Plot heatmap\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(figsize=(8, 5))\n",
    "    sns.heatmap(df_pivot, annot=True, cmap=\"Greens\", fmt=\".2f\", ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the training results first by showing a subset of the recorded meta information and performance metrics for all runs in a `pd.DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_columns = {\n",
    "    \"train_hyperparams\": [\"learning_rate\", \"epochs\"],\n",
    "    \"model_hyperparams\": [\"num_features\", \"num_layers\", \"conv_dim\", \"conv\", \"pooling\"],\n",
    "    \"train_results\": [\"loss\", \"accuracy\"],\n",
    "    \"val_results\": [\"loss\", \"accuracy\", \"f1\"],\n",
    "    \"other\": [\"num_params\"]\n",
    "}\n",
    "\n",
    "training_results_df = results_to_df(RESULTS, include_columns=include_columns)\n",
    "display(training_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframe to LaTeX table for report\n",
    "columns = [(\"model_hyperparams\", \"num_layers\"), (\"model_hyperparams\", \"conv_dim\"), (\"train_hyperparams\", \"learning_rate\"), (\"model_hyperparams\", \"conv\"), (\"model_hyperparams\", \"pooling\"), (\"val_results\", \"f1\"), (\"val_results\", \"accuracy\")]\n",
    "results = training_results_df[columns]\n",
    "results = results.reset_index().drop(columns=\"index\")\n",
    "\n",
    "# Rename columns\n",
    "shorten_conv = {\n",
    "    \"GraphConv\": \"GraphConv\",\n",
    "    \"GraphSAGEConv\": \"GraphSAGE\",\n",
    "    \"AttentionGraphConvolution\": \"GraphAttention\",\n",
    "    \"EdgeGraphConv\": \"EdgeConv\",\n",
    "}\n",
    "shorten_pool = {\n",
    "    \"MeanPooling\": \"Mean\",\n",
    "    \"MaxPooling\": \"Max\",\n",
    "}\n",
    "\n",
    "results[(\"model_hyperparams\", \"conv\")] = results[(\"model_hyperparams\", \"conv\")].apply(lambda x: shorten_conv[x.__name__])\n",
    "results[(\"model_hyperparams\", \"pooling\")] = results[(\"model_hyperparams\", \"pooling\")].apply(lambda x: shorten_pool[x.__name__])\n",
    "\n",
    "results[(\"val_results\", \"f1\")] = 100 * results[(\"val_results\", \"f1\")]\n",
    "results[(\"val_results\", \"accuracy\")] = 100 * results[(\"val_results\", \"accuracy\")]\n",
    "\n",
    "results = results.style.format(precision=2)\n",
    "\n",
    "print(results.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! Let's try to visualise the performance of the best performing model as a function of the hyper-parameters.\n",
    "\n",
    "We start by looking at each hyper-parameter individually and plot the aggregated validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with subplots\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(10, 4))\n",
    "\n",
    "# Plot validation performance as function of number of convolutional layer type\n",
    "plot_boxplot(training_results_df, y=(\"val_results\", \"f1\"), x=(\"model_hyperparams\", \"conv\"), ax=axs[0])\n",
    "plot_boxplot(training_results_df, y=(\"val_results\", \"f1\"), x=(\"model_hyperparams\", \"pooling\"), ax=axs[1])\n",
    "\n",
    "# Add plot labels and styles\n",
    "axs[0].set(\n",
    "    xlabel=\"Convolutional Layer Type\",\n",
    "    ylabel=\"Validation F1 Score\",\n",
    "    title=\"Performance By Convolutional Layer Type\",\n",
    ")\n",
    "axs[0].set_xticks(range(4))\n",
    "axs[0].set_xticklabels([\"GraphSage\", \"GraphConv\", \"GAT\", \"EdgeConv\"])\n",
    "\n",
    "axs[1].set(\n",
    "    xlabel=\"Pooling Layer Type\",\n",
    "    ylabel=\"Validation F1 Score\",\n",
    "    title=\"Performance By Pooling Layer Type\",\n",
    ")\n",
    "\n",
    "# Save figure\n",
    "fig.savefig(os.path.join(PLOT_PATH, \"perf_vs_conv_pool.png\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Let's try to look at a more dense representation using a scatter plot that relates the number of parameters to the validation performance. Additionally, we encode the convolutional layer type through hue and the global pooling type through different shapes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation performance scatter\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "\n",
    "plot_scatter(training_results_df,\n",
    "             x=(\"other\", \"num_params\"), \n",
    "             y=(\"val_results\", \"f1\"), \n",
    "             hue=(\"model_hyperparams\", \"conv\"), \n",
    "             style=(\"model_hyperparams\", \"pooling\"),\n",
    "             ax=ax)\n",
    "\n",
    "ax.set(\n",
    "    xlabel=\"Number of Trainable Parameters\",\n",
    "    ylabel=\"Validation F1 Score\",\n",
    ");\n",
    "\n",
    "# Save figure\n",
    "fig.savefig(os.path.join(PLOT_PATH, \"perf_vs_hyperparms_scatter.png\"), bbox_inches=\"tight\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the model complexity (number of layers and hidden dimension in each layer) relates to the validation performance. To investigate, we plot a heatmap with the number of layers on the x-axis, the hidden dimension on the y-axis and the validation performance as the color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot heatmap of validation performance for number of layers and hidden dimension\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(9, 3))\n",
    "plot_heatmap(training_results_df, \n",
    "             y=(\"model_hyperparams\", \"num_layers\"), x=(\"model_hyperparams\", \"conv_dim\"), values=(\"val_results\", \"f1\"), ax=axs[0])\n",
    "plot_heatmap(training_results_df, \n",
    "             y=(\"model_hyperparams\", \"num_layers\"), x=(\"model_hyperparams\", \"conv_dim\"), values=(\"val_results\", \"accuracy\"), ax=axs[1])\n",
    "\n",
    "axs[0].set(\n",
    "    title=\"Validation F1 Score\",\n",
    "    xlabel=\"Number of Hidden Features\",\n",
    "    ylabel=\"Number of Layers\",\n",
    ");\n",
    "axs[1].set(\n",
    "    title=\"Validation Accuracy\",\n",
    "    xlabel=\"Hidden Dimension\",\n",
    "    ylabel=\"Number of Layers\",\n",
    ");\n",
    "\n",
    "fig.savefig(os.path.join(PLOT_PATH, \"perf_vs_layers_dims.png\"), bbox_inches=\"tight\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot heatmap of training and validation accuracy for number of layers and hidden dimension\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(9, 3))\n",
    "plot_heatmap(training_results_df, \n",
    "             y=(\"model_hyperparams\", \"num_layers\"), x=(\"model_hyperparams\", \"conv_dim\"), values=(\"train_results\", \"accuracy\"), ax=axs[0])\n",
    "plot_heatmap(training_results_df, \n",
    "             y=(\"model_hyperparams\", \"num_layers\"), x=(\"model_hyperparams\", \"conv_dim\"), values=(\"val_results\", \"accuracy\"), ax=axs[1])\n",
    "\n",
    "axs[0].set(\n",
    "    title=\"Training Accuracy\",\n",
    "    xlabel=\"Number of Hidden Features\",\n",
    "    ylabel=\"Number of Layers\",\n",
    ");\n",
    "axs[1].set(\n",
    "    title=\"Validation Accuracy\",\n",
    "    xlabel=\"Hidden Dimension\",\n",
    "    ylabel=\"Number of Layers\",\n",
    ");\n",
    "\n",
    "fig.savefig(os.path.join(PLOT_PATH, \"train_vs_val_accuracy.png\"), bbox_inches=\"tight\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model\n",
    "\n",
    "Finally, let's use the best performing model and evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model based on validation F1\n",
    "best_model_idx = training_results_df[(\"val_results\", \"f1\")].argmax()\n",
    "best_model_info = training_results_df.iloc[best_model_idx]\n",
    "\n",
    "# Save train and model hyperparameters\n",
    "train_params = best_model_info[\"train_hyperparams\"]\n",
    "model_params = best_model_info[\"model_hyperparams\"]\n",
    "\n",
    "# Print best model's hyperparameters\n",
    "best_model_info[[\"train_hyperparams\", \"model_hyperparams\", \"val_results\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrain the model on the full training data and evaluate it on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train and validation data\n",
    "train_val_data = train_data + val_data\n",
    "train_val_loader = DataLoader(train_val_data, batch_size=1, shuffle=True)\n",
    "\n",
    "# Initialise best model\n",
    "best_model = GNN(**model_params)\n",
    "\n",
    "optimser = torch.optim.Adam(best_model.parameters(), lr=train_params[\"learning_rate\"])\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([train_ratio]))\n",
    "\n",
    "train_results = train(best_model, train_val_loader, test_loader, criterion, optimser, epochs=train_params[\"epochs\"], use_edges=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test best model\n",
    "test_results = validate(best_model, test_loader, criterion, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print numeric test results\n",
    "print(\"Test results\")\n",
    "pd.Series(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out heatmap of confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "sns.heatmap(test_results[\"confusion_matrix\"], annot=True, fmt=\"d\", cmap=\"Greens\", cbar=False, ax=ax);\n",
    "\n",
    "fig.savefig(os.path.join(PLOT_PATH, \"confusion_matrix.png\"), dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs502",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
