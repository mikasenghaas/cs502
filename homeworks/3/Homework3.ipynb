{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - Name, SCIPER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, we are going to work with the transformer. There are three parts of this homework.\n",
    "\n",
    "- In the first part, we are going to implement **positional encoding** and **self-attention**  and test them on a simple text dataset which contains around 100 sentences. We will use a small transformer in this task.\n",
    "\n",
    "- In the second part, we will detect promoters from the DNA sequences. The main difference compared to the previous task is to tokenize the DNA sequence. Thus, our task here is to build the **tokenizer** to tokenize the DNA sequence. For the model, we will continue using the small transformer.\n",
    "\n",
    "- In the third part, we will use a **foundation model** DNABERT to perform promoter detection. In this part, you do not need to train the transformer. Instead, you need to find and load the correct pre-trained model and then use it to get the embedding of the DNA sequence. Then, you will build a simple classifier to perform promoter detection based on the DNA embedding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Initialization\n",
    "\n",
    "Import the packages you are going to use here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "# from torchmetrics.classification import BinaryF1Score\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from types import SimpleNamespace\n",
    "from utils import data, evaluation, models, visualization, text_exercise\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "seed = 128\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Positional Encoding and Self-Attention (7 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Sinusoidal Positional Encoding (1 pt)\n",
    "\n",
    "In this section, you are going to implement the sinusoidal positional encoding. The formula is as the following:\n",
    "\n",
    "<div>\n",
    "<img src=\"./imgs/positional embedding.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "where $t$ is the desired position in the input and $\\mathsf{\\omega}_k$ follows:\n",
    "\n",
    "<div>\n",
    "<img src=\"./imgs/omega.png\" width=\"200\"/>\n",
    "</div>\n",
    "\n",
    "To see the details of sinusoidal positional encoding, you can check this [link](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\"Returns the positional embedding for inputs of a maximum length and dimension.\"\"\"\n",
    "\n",
    "    def __init__(self, max_position_embeddings, hidden_size, device):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Initialises the PositionalEmbedding class.\n",
    "\n",
    "        Args:\n",
    "            max_position_embeddings (int): maximum length of the input - related to t in the previous formula\n",
    "            hidden_size (int): encoding dimension - d in the previous formula\n",
    "            device (torch.device): device on which to store the positional embedding\n",
    "        \"\"\"\n",
    "\n",
    "        # Save parameters\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Compute positional embeddings\n",
    "        t = torch.arange(max_position_embeddings).unsqueeze(1)\n",
    "        w = 1 / 10000 ** (torch.arange(0, hidden_size, 2) / hidden_size)\n",
    "        self.positional_embedding = torch.zeros(max_position_embeddings, hidden_size, device=device)\n",
    "        self.positional_embedding[:, 0::2] = torch.sin(w * t)\n",
    "        self.positional_embedding[:, 1::2] = torch.cos(w * t)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.positional_embedding\n",
    "\n",
    "    def embedding(self):\n",
    "        return self.positional_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can visualize your positional encoding. If you implement everything correctly, you can get a figure that is similar to Figure 2 in this [link](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    visualize_embedding,\n",
    "    dimension_selector,\n",
    "    max_len_selector,\n",
    ") = visualization.display_positional_encoding(PositionalEmbedding)\n",
    "ui = widgets.HBox([max_len_selector, dimension_selector])\n",
    "out = widgets.interactive_output(\n",
    "    visualize_embedding, {\"max_len\": max_len_selector, \"dimension\": dimension_selector}\n",
    ")\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Self-Attention Mechanism (5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you are going to implement the self-attention mechanism. Please check the section 'Self-Attention in Detail' in this [link](https://jalammar.github.io/illustrated-transformer/) for the details of self-attention mechanism. (We encourage you to carefully go through the link since it is a very good tutorial for transformer.)\n",
    "\n",
    "The specific steps will be provided in the comments of the following code. (The steps are only for reference. You do need to follow the steps if you have a better way to implement it.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will split the implementation of the `BertSelfAttention` class into several chunks to make the code more readable and modular. The multi-headed self-attention essentially consists of several parts: `ScaledDotProductAttention`, `SingleHeadAttention`, `MultiHeadAttention` and `SelfAttention` in that order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n",
    "            )\n",
    "\n",
    "        # Save configuration parameters\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.output_attentions = config.output_attentions\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        # Initialise attention weights matrices of all heads in a single linear layer\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        # Add dropout layer\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        \"\"\"\n",
    "        Permutes an input tensor from (batch_size, seq_len, all_head_size) to\n",
    "        (batch_size, num_heads, seq_len, head_size)\n",
    "        \"\"\"\n",
    "        new_x_shape = x.size()[:-1] + (\n",
    "            self.num_attention_heads,\n",
    "            self.attention_head_size,\n",
    "        )\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "    ):\n",
    "        # The parameter encoder_hidden_states and encoder_attention_mask is for cross-attention.\n",
    "        # We do not use them in this homework.\n",
    "\n",
    "        # Compute the query, key, value matrices in all heads\n",
    "        # Output dimension: (num_batches, seq_len, num_heads * head_size) = (num_batches, seq_len, all_head_size)\n",
    "        mixed_key_layer = self.query(hidden_states)\n",
    "        mixed_query_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        # Transpose K, Q, V to get the queries, keys and values in each head\n",
    "        # Output dimension: (num_batches, num_heads, seq_len, head_size)\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        # Compute attention scores through scaled dot product attention\n",
    "        attention_scores = (\n",
    "            query_layer @ key_layer.transpose(-1, -2) / math.sqrt(self.hidden_size)\n",
    "        )\n",
    "\n",
    "        # You do not need to change this part.\n",
    "        # Apply attention mask\n",
    "        if attention_mask is not None:\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities (+ dropout)\n",
    "        attention_probs = self.dropout(F.softmax(attention_scores, dim=-1))\n",
    "\n",
    "        # You do not need to change this part.\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        # Compute output as weighted sum of value by the score\n",
    "        context_layer = attention_probs @ value_layer\n",
    "\n",
    "        # Permute context layer to get the original shape\n",
    "        B, _, _, H = context_layer.shape\n",
    "        context_layer = context_layer.view(B, -1, H * self.num_attention_heads)\n",
    "\n",
    "        # Get the output\n",
    "        outputs = (\n",
    "            (context_layer, attention_probs)\n",
    "            if self.output_attentions\n",
    "            else (context_layer,)\n",
    "        )\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test your implementation using simple text data! First, let's load the data.\n",
    "\n",
    "We use a small dataset in this homework for a shorter training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT generated text data about BERT\n",
    "text = text_exercise.get()\n",
    "sentences_df, vocab = data.to_sentence_df(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the data, you can train your model. Here we train our model using masked token prediction.\n",
    "\n",
    "Hint: The final model accuracy should be higher than 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_max_len = 11\n",
    "\n",
    "text_config = SimpleNamespace(\n",
    "    vocab_size=len(vocab),\n",
    "    hidden_size=60,\n",
    "    max_position_embeddings=text_max_len,\n",
    "    type_vocab_size=1,\n",
    "    layer_norm_eps=1e-12,\n",
    "    hidden_dropout_prob=0.0,\n",
    "    attention_probs_dropout_prob=0.0,\n",
    "    num_attention_heads=1,\n",
    "    hidden_act=\"gelu\",\n",
    "    intermediate_size=160,\n",
    "    num_hidden_layers=1,\n",
    "    is_decoder=False,\n",
    "    output_attentions=True,\n",
    "    output_hidden_states=False,\n",
    "    pruned_heads={},\n",
    "    initializer_range=0.02,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "tokenizer = data.TextTokenizer(vocab)\n",
    "(\n",
    "    input_ids,\n",
    "    segment_ids,\n",
    "    masked_lm_labels,\n",
    "    labels_idx,\n",
    "    labels,\n",
    "    attention_masks,\n",
    ") = data.generate_masked_data(\n",
    "    sentences_df, tokenizer, k=1, max_len=text_max_len, noise_rate=0.4\n",
    ")\n",
    "\n",
    "model = models.BertForMaskedLM(\n",
    "    config=text_config,\n",
    "    positional_embedding=PositionalEmbedding,\n",
    "    attention=BertSelfAttention,\n",
    ")\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "print(\n",
    "    f\"Number of trainable model parameters: {models.number_of_model_parameters(model)}\"\n",
    ")\n",
    "\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    loss, outputs, attentions = model(\n",
    "        input_ids=input_ids,\n",
    "        token_type_ids=segment_ids,\n",
    "        masked_lm_labels=masked_lm_labels,\n",
    "        attention_mask=attention_masks,\n",
    "    )\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(\"Epoch:\", \"%04d\" % (epoch + 1), \"loss =\", \"{:.6f}\".format(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\n",
    "    f\"Final model accuracy: {evaluation.masked_label_accuracy(labels, labels_idx, outputs.data)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Visualize Attention (1 pt)\n",
    "\n",
    "Here, you can visualize the self-attention. \n",
    "\n",
    "Question: Can you interpret the visualization of the self-attention?\n",
    "\n",
    "**Write down you answer here (1 pt):** The visualisation shows a heatmap of the attention weight matrix in a head within an self-attention layer for some sample. The matrices' row-sum is $1$ and we can interpret the element $a_{uv}$ as the importance of the $v$-th token to the $u$-th token. The darker the color, the higher the importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_attention, sample_id_selector = visualization.display_attantion(\n",
    "    attentions=attentions, input_ids=input_ids, tokenizer=tokenizer\n",
    ")\n",
    "widgets.interactive(visualize_attention, sample_id=sample_id_selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Train on small Wikitext Dataset\n",
    "\n",
    "Here, you can **optionally** test your model on the smallest wikitext dataset. You should get an test accuracy around 0.4 after training 50 epochs.\n",
    "\n",
    "This part is only for you to test your code. You can choose to run it or not. It takes around 1 hour to train the model for 50 epochs on the smallest wikitext dataset with Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't train\n",
    "# text_exercise.train_wikitext(device, positional_embedding=PositionalEmbedding, attention=BertSelfAttention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Promoter detection (7 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we detect promoter in DNA sequence.\n",
    "\n",
    "A promoter is a region of DNA upstream of a gene where relevant proteins (such as RNA polymerase and transcription factors) bind to initiate transcription of that gene. Promoter detection is to identify if there are promoter regions in the given DNA sequence. We have covered this in the lecture. (If you are interested in the promoter, you can check this [link](https://www.genome.gov/genetics-glossary/Promoter) for more details.)\n",
    "\n",
    "Here, we use a transformer and a classifier. The transformer first embeds the DNA sequences into features, and then the classifier detects the promoter based on the features.\n",
    "\n",
    "The main difference between text and DNA sequence is how to tokenize the sequence. Thus, you need to implement a tokenizer for the DNA sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. DNA Tokenizer (1 pts)\n",
    "\n",
    "Here, you will implement the DNA tokenizer the same as in DNABERT. Please check this [paper](https://academic.oup.com/bioinformatics/article/37/15/2112/6128680) for implementation details. Also, you need to check the data type and shape for both input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNATokenizer(data.Tokenizer):\n",
    "    def __init__(self, k, vocab, unknown=\"[UNK]\"):\n",
    "        \"\"\"\n",
    "        DNA tokenizer that splits a DNA sequence into k-mers. Inherits from\n",
    "        data.Tokenizer which implements functionality for mapping the parsed tokens\n",
    "        to indices and vice versa.\n",
    "        \"\"\"\n",
    "        super().__init__(vocab, unknown)\n",
    "        # Save parameters\n",
    "        self.k = k\n",
    "\n",
    "    def _parse_text(self, text):\n",
    "        \"\"\"\n",
    "        Parse a text into a list of tokens.\n",
    "\n",
    "        Args:\n",
    "            text (str): text to parse\n",
    "\n",
    "        Returns:\n",
    "            list[str]: list of k-mer token strings\n",
    "        \"\"\"\n",
    "        return [text[i : i + self.k] for i in range(len(text) - self.k + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Test BERT on DNA Sequence\n",
    "\n",
    "In this section, you will train BERT on DNA sequence to learn the embedding of DNA sequence. The code is provided below and you do not need to write anything.\n",
    "\n",
    "Hint: the final evaluation accuracy should be higher than 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmer = 3\n",
    "mask_length = kmer\n",
    "VOCAB_3MER = [\n",
    "    \"[PAD]\",\n",
    "    \"[UNK]\",\n",
    "    \"[CLS]\",\n",
    "    \"[SEP]\",\n",
    "    \"[MASK]\",\n",
    "    \"AAA\",\n",
    "    \"AAT\",\n",
    "    \"AAC\",\n",
    "    \"AAG\",\n",
    "    \"ATA\",\n",
    "    \"ATT\",\n",
    "    \"ATC\",\n",
    "    \"ATG\",\n",
    "    \"ACA\",\n",
    "    \"ACT\",\n",
    "    \"ACC\",\n",
    "    \"ACG\",\n",
    "    \"AGA\",\n",
    "    \"AGT\",\n",
    "    \"AGC\",\n",
    "    \"AGG\",\n",
    "    \"TAA\",\n",
    "    \"TAT\",\n",
    "    \"TAC\",\n",
    "    \"TAG\",\n",
    "    \"TTA\",\n",
    "    \"TTT\",\n",
    "    \"TTC\",\n",
    "    \"TTG\",\n",
    "    \"TCA\",\n",
    "    \"TCT\",\n",
    "    \"TCC\",\n",
    "    \"TCG\",\n",
    "    \"TGA\",\n",
    "    \"TGT\",\n",
    "    \"TGC\",\n",
    "    \"TGG\",\n",
    "    \"CAA\",\n",
    "    \"CAT\",\n",
    "    \"CAC\",\n",
    "    \"CAG\",\n",
    "    \"CTA\",\n",
    "    \"CTT\",\n",
    "    \"CTC\",\n",
    "    \"CTG\",\n",
    "    \"CCA\",\n",
    "    \"CCT\",\n",
    "    \"CCC\",\n",
    "    \"CCG\",\n",
    "    \"CGA\",\n",
    "    \"CGT\",\n",
    "    \"CGC\",\n",
    "    \"CGG\",\n",
    "    \"GAA\",\n",
    "    \"GAT\",\n",
    "    \"GAC\",\n",
    "    \"GAG\",\n",
    "    \"GTA\",\n",
    "    \"GTT\",\n",
    "    \"GTC\",\n",
    "    \"GTG\",\n",
    "    \"GCA\",\n",
    "    \"GCT\",\n",
    "    \"GCC\",\n",
    "    \"GCG\",\n",
    "    \"GGA\",\n",
    "    \"GGT\",\n",
    "    \"GGC\",\n",
    "    \"GGG\",\n",
    "]\n",
    "\n",
    "raw_training_data = data.load_csv(\"./data/train.csv\")\n",
    "raw_test_data = data.load_csv(\"./data/test.csv\")\n",
    "\n",
    "dna_max_len = 298\n",
    "batch_size = 128\n",
    "max_dna_mask = 100\n",
    "dataset_size = 1000\n",
    "num_layers = 3\n",
    "num_heads = 6\n",
    "dna_config = SimpleNamespace(\n",
    "    vocab_size=len(VOCAB_3MER),\n",
    "    hidden_size=60,\n",
    "    max_position_embeddings=dna_max_len,\n",
    "    type_vocab_size=1,\n",
    "    layer_norm_eps=1e-12,\n",
    "    hidden_dropout_prob=0.0,\n",
    "    attention_probs_dropout_prob=0.0,\n",
    "    num_attention_heads=num_heads,\n",
    "    hidden_act=\"gelu\",\n",
    "    intermediate_size=160,\n",
    "    num_hidden_layers=num_layers,\n",
    "    is_decoder=False,\n",
    "    output_attentions=True,\n",
    "    output_hidden_states=True,\n",
    "    pruned_heads={},\n",
    "    initializer_range=0.02,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "tokenizer = DNATokenizer(k=kmer, vocab=VOCAB_3MER)\n",
    "(\n",
    "    input_ids,\n",
    "    segment_ids,\n",
    "    masked_lm_labels,\n",
    "    labels_idx,\n",
    "    labels,\n",
    "    attention_masks,\n",
    ") = data.generate_masked_data(\n",
    "    raw_training_data,\n",
    "    tokenizer,\n",
    "    max_len=dna_max_len,\n",
    "    max_mask=max_dna_mask,\n",
    "    k=mask_length,\n",
    "    mask_rate=0.05,\n",
    "    max_size=dataset_size,\n",
    ")\n",
    "(\n",
    "    test_input_ids,\n",
    "    test_segment_ids,\n",
    "    test_masked_lm_labels,\n",
    "    test_labels_idx,\n",
    "    test_labels,\n",
    "    test_attention_masks,\n",
    ") = data.generate_masked_data(\n",
    "    raw_test_data,\n",
    "    tokenizer,\n",
    "    max_len=dna_max_len,\n",
    "    max_mask=max_dna_mask,\n",
    "    k=mask_length,\n",
    "    mask_rate=0.05,\n",
    "    max_size=dataset_size,\n",
    ")\n",
    "\n",
    "model = models.BertForMaskedLM(\n",
    "    config=dna_config,\n",
    "    positional_embedding=PositionalEmbedding,\n",
    "    attention=BertSelfAttention,\n",
    ").to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.002)\n",
    "print(\n",
    "    f\"Number of trainable model parameters: {models.number_of_model_parameters(model)}\"\n",
    ")\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    input_ids, segment_ids, masked_lm_labels, labels_idx, labels, attention_masks\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(\n",
    "    test_input_ids,\n",
    "    test_segment_ids,\n",
    "    test_masked_lm_labels,\n",
    "    test_labels_idx,\n",
    "    test_labels,\n",
    "    test_attention_masks,\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for epoch in range(50):\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "    for (\n",
    "        batch_input_ids,\n",
    "        batch_segment_ids,\n",
    "        batch_masked_lm_labels,\n",
    "        _,\n",
    "        _,\n",
    "        batch_attention_mask,\n",
    "    ) in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss, outputs, hidden_states, _ = model(\n",
    "            input_ids=batch_input_ids.to(device),\n",
    "            token_type_ids=batch_segment_ids.to(device),\n",
    "            masked_lm_labels=batch_masked_lm_labels.to(device),\n",
    "            attention_mask=batch_attention_mask.to(device),\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        model.eval()\n",
    "        total_eval_loss = 0\n",
    "        for (\n",
    "            batch_input_ids,\n",
    "            batch_segment_ids,\n",
    "            batch_masked_lm_labels,\n",
    "            _,\n",
    "            _,\n",
    "            batch_attention_mask,\n",
    "        ) in test_loader:\n",
    "            with torch.no_grad():\n",
    "                loss, outputs, hidden_states, _ = model(\n",
    "                    input_ids=batch_input_ids.to(device),\n",
    "                    token_type_ids=batch_segment_ids.to(device),\n",
    "                    masked_lm_labels=batch_masked_lm_labels.to(device),\n",
    "                    attention_mask=batch_attention_mask.to(device),\n",
    "                )\n",
    "                if batch_attention_mask.sum() - torch.numel(batch_attention_mask) > 0:\n",
    "                    print(\"found patting\", batch_attention_mask.sum())\n",
    "                total_eval_loss += loss.item()\n",
    "        avg_eval_loss = total_eval_loss / len(test_loader)\n",
    "        print(\n",
    "            \"Epoch:\",\n",
    "            \"%04d\" % (epoch + 1),\n",
    "            \"train cost =\",\n",
    "            \"{:.6f}\".format(avg_train_loss),\n",
    "            \"eval cost =\",\n",
    "            \"{:.6f}\".format(avg_eval_loss),\n",
    "        )\n",
    "\n",
    "average_train_acc, _ = evaluation.model_masked_label_accuracy(\n",
    "    model, train_loader, device\n",
    ")\n",
    "average_test_acc, last_test_attention = evaluation.model_masked_label_accuracy(\n",
    "    model, test_loader, device\n",
    ")\n",
    "print(\n",
    "    \"Train Acc =\",\n",
    "    \"{:.6f}\".format(average_train_acc),\n",
    "    \"Eval Acc =\",\n",
    "    \"{:.6f}\".format(average_test_acc),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Visualize the Attentions (1 pt)\n",
    "\n",
    "Here, you can visualize the self-attention. \n",
    "\n",
    "Question: compare the visualization to Section 1.3, what can you find here? How do you explain it?\n",
    "\n",
    "**Write down you answer here (1 pt):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    visualize_attention,\n",
    "    sample_id_selector,\n",
    "    layer_selector,\n",
    "    head_selector,\n",
    ") = visualization.display_multi_attantion(\n",
    "    attentions=last_test_attention,\n",
    "    tokenizer=tokenizer,\n",
    "    input_ids=input_ids,\n",
    "    layers=range(1, num_layers + 1),\n",
    "    heads=range(1, num_heads + 1),\n",
    ")\n",
    "ui = widgets.HBox([sample_id_selector, layer_selector, head_selector])\n",
    "out = widgets.interactive_output(\n",
    "    visualize_attention,\n",
    "    {\"sample_id\": sample_id_selector, \"layer\": layer_selector, \"head\": head_selector},\n",
    ")\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Use your pretrained model for promoter detection (5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You already have the embeddings for the DNA sequence. Now, you are going to build a classifier based on the DNA embeddings. The classifier is to perform promoter detection. Specifically, the DNA sequence will be classified into *'contains promoter'* or *'does not contain promoter'*.\n",
    "\n",
    "Hint: \n",
    "- We now want to annotate data (get the label for each sample), not predict masked data anymore!\n",
    "- You can reuse some parts of the code in the previous sections, e.g. dataloader and training pipeline in Section 2.2.\n",
    "- If you implement the previous section correctly (the Eval Acc > 0.2 in Section 2.2), you already have an pre-trained object named 'model' of class models.BertForMaskedLM. You can directly use it.\n",
    "- The evaluation accuracy of this task should be around 0.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a new model `PromoterDetector` which is a binary classification head that is \"stacked\" on top of a pre-trained DNABERT model. The model assumes that the BERT model is pre-trained and all weights are frozen for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromoterDetector(nn.Module):\n",
    "    \"\"\"PromoterDetector classification head on top of DNA BERT.\"\"\"\n",
    "    def __init__(self, model):\n",
    "        \"\"\"\n",
    "        Classification head to stack on top of a pre-trained BERT model for promoter detection.\n",
    "\n",
    "        Args:\n",
    "            model (nn.Module): A pre-trained encoder model\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Save parameters\n",
    "        self.model = model\n",
    "        self.hidden_size = model.config.hidden_size\n",
    "\n",
    "        # Initialise linear layer from class token\n",
    "        self.linear = nn.Linear(self.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        Feed forward function of the classification head. Returns the logits of the classification\n",
    "        of each sequence in the batch. Assumes that the hidden states are returned by BERT model\n",
    "        as the second output. And that the last tensor in the hidden state list corresponds to the\n",
    "        hidden states in the last encoder block (i.e. the final output of the encoder) and that the\n",
    "        first token in the sequence is the class token.\n",
    "\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): Input tensor of shape (batch_size, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, )\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute BERT output\n",
    "        _, hidden_states, _ = self.model(input_ids=input_ids)\n",
    "        \n",
    "        # Extract last hidden state\n",
    "        last_hidden_state = hidden_states[-1]\n",
    "\n",
    "        # Get the embedding of the first token (class token)\n",
    "        class_tokens = last_hidden_state[:, 0, :]\n",
    "\n",
    "        # Get the classification logits\n",
    "        logits = self.linear(class_tokens)\n",
    "\n",
    "        return logits.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define new dataset classes and loaders which sample the tokenised input indices and sequence labels. We can use the utility function `data.generate_labeled_data` and PyTorch functions for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate labelled training data\n",
    "train_input_ids, train_labels = data.generate_labeled_data(\n",
    "    raw_training_data,\n",
    "    tokenizer,\n",
    "    max_len=dna_max_len,\n",
    "    max_size=dataset_size,\n",
    ")\n",
    "\n",
    "# Generate labelled test data\n",
    "test_input_ids, test_labels = data.generate_labeled_data(\n",
    "    raw_test_data,\n",
    "    tokenizer,\n",
    "    max_len=dna_max_len,\n",
    "    max_size=dataset_size,\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_dataset = TensorDataset(train_input_ids, train_labels)\n",
    "test_dataset = TensorDataset(test_input_ids, test_labels)\n",
    "\n",
    "# Create batched data loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialise the `PromotorDetector` from a the pre-trained DNABERT model with frozen weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all BERT parameters\n",
    "for params in model.parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "# Initialise the classifier\n",
    "clf = PromoterDetector(model).to(device)\n",
    "optimizer = optim.AdamW(clf.parameters(), lr=0.002)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(f\"Number of model parameters: {models.number_of_model_parameters(clf)}\")\n",
    "print(f\"Number of trainable model parameters: {sum([p.numel() for p in clf.parameters() if p.requires_grad])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model for `50` epochs and print the training and validation loss and accuracy every other epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(50):\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "    for (input_ids, labels) in train_loader:\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Move data to device\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).float()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = clf(input_ids)\n",
    "        loss = criterion(logits.squeeze(), labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        model.eval()\n",
    "        total_eval_loss = 0\n",
    "        for (input_ids, labels) in test_loader:\n",
    "            with torch.no_grad():\n",
    "                # Move data to device\n",
    "                input_ids = input_ids.to(device)\n",
    "                labels = labels.to(device).float()\n",
    "\n",
    "                # Forward pass and compute loss\n",
    "                logits = clf(input_ids)\n",
    "                loss = criterion(logits.squeeze(), labels)\n",
    "\n",
    "                total_eval_loss += loss.item()\n",
    "\n",
    "        avg_eval_loss = total_eval_loss / len(test_loader)\n",
    "        print(\n",
    "            \"Epoch:\",\n",
    "            \"%04d\" % (epoch + 1),\n",
    "            \"train cost =\",\n",
    "            \"{:.6f}\".format(avg_train_loss),\n",
    "            \"eval cost =\",\n",
    "            \"{:.6f}\".format(avg_eval_loss),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the model on the test set using accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "y_pred, y_true = [], []\n",
    "for (input_ids, labels) in test_loader:\n",
    "    with torch.no_grad():\n",
    "        # Move data to device\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).float()\n",
    "\n",
    "        # Forward pass and compute loss\n",
    "        logits = clf(input_ids)\n",
    "        probs = F.sigmoid(logits.squeeze())\n",
    "        preds = torch.round(probs)\n",
    "\n",
    "        y_pred.extend(preds.tolist())\n",
    "        y_true.extend(labels.tolist())\n",
    "\n",
    "y_pred = torch.Tensor(y_pred)\n",
    "y_true = torch.Tensor(y_true)\n",
    "eval_acc = (y_pred == y_true).float().mean()\n",
    "\n",
    "print(\"Eval Acc =\", \"{:.6f}\".format(eval_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Additional question (1 pt)\n",
    "\n",
    "Now we change mask_length = 1 (already changed, you do not need to implement anything).\n",
    "Let's run the code below and check the accuracy.\n",
    "\n",
    "Question: What is the final masked token prediction accuracy? How do you explain this?\n",
    "\n",
    "**Write down you answer here (1 pt):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmer = 3\n",
    "mask_length = 1\n",
    "\n",
    "dna_max_len = 298\n",
    "batch_size = 128\n",
    "max_dna_mask = 100\n",
    "dataset_size = 1000\n",
    "num_layers = 3\n",
    "num_heads = 6\n",
    "dna_config = SimpleNamespace(\n",
    "    vocab_size=len(VOCAB_3MER),\n",
    "    hidden_size=60,\n",
    "    max_position_embeddings=dna_max_len,\n",
    "    type_vocab_size=1,\n",
    "    layer_norm_eps=1e-12,\n",
    "    hidden_dropout_prob=0.0,\n",
    "    attention_probs_dropout_prob=0.0,\n",
    "    num_attention_heads=num_heads,\n",
    "    hidden_act=\"gelu\",\n",
    "    intermediate_size=160,\n",
    "    num_hidden_layers=num_layers,\n",
    "    is_decoder=False,\n",
    "    output_attentions=True,\n",
    "    output_hidden_states=True,\n",
    "    pruned_heads={},\n",
    "    initializer_range=0.02,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "tokenizer = DNATokenizer(k=3, vocab=VOCAB_3MER)\n",
    "(\n",
    "    input_ids,\n",
    "    segment_ids,\n",
    "    masked_lm_labels,\n",
    "    labels_idx,\n",
    "    labels,\n",
    "    attention_masks,\n",
    ") = data.generate_masked_data(\n",
    "    raw_training_data,\n",
    "    tokenizer,\n",
    "    max_len=dna_max_len,\n",
    "    max_mask=max_dna_mask,\n",
    "    k=mask_length,\n",
    "    mask_rate=0.05,\n",
    "    max_size=dataset_size,\n",
    ")\n",
    "(\n",
    "    test_input_ids,\n",
    "    test_segment_ids,\n",
    "    test_masked_lm_labels,\n",
    "    test_labels_idx,\n",
    "    test_labels,\n",
    "    test_attention_masks,\n",
    ") = data.generate_masked_data(\n",
    "    raw_test_data,\n",
    "    tokenizer,\n",
    "    max_len=dna_max_len,\n",
    "    max_mask=max_dna_mask,\n",
    "    k=mask_length,\n",
    "    mask_rate=0.05,\n",
    "    max_size=dataset_size,\n",
    ")\n",
    "\n",
    "model = models.BertForMaskedLM(\n",
    "    config=dna_config,\n",
    "    positional_embedding=PositionalEmbedding,\n",
    "    attention=BertSelfAttention,\n",
    ").to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.002)\n",
    "print(\n",
    "    f\"Number of trainable model parameters: {models.number_of_model_parameters(model)}\"\n",
    ")\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    input_ids, segment_ids, masked_lm_labels, labels_idx, labels, attention_masks\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(\n",
    "    test_input_ids,\n",
    "    test_segment_ids,\n",
    "    test_masked_lm_labels,\n",
    "    test_labels_idx,\n",
    "    test_labels,\n",
    "    test_attention_masks,\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for epoch in range(50):\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "    for (\n",
    "        batch_input_ids,\n",
    "        batch_segment_ids,\n",
    "        batch_masked_lm_labels,\n",
    "        _,\n",
    "        _,\n",
    "        batch_attention_mask,\n",
    "    ) in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss, outputs, hidden_states, _ = model(\n",
    "            input_ids=batch_input_ids.to(device),\n",
    "            token_type_ids=batch_segment_ids.to(device),\n",
    "            masked_lm_labels=batch_masked_lm_labels.to(device),\n",
    "            attention_mask=batch_attention_mask.to(device),\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        model.eval()\n",
    "        total_eval_loss = 0\n",
    "        for (\n",
    "            batch_input_ids,\n",
    "            batch_segment_ids,\n",
    "            batch_masked_lm_labels,\n",
    "            _,\n",
    "            _,\n",
    "            batch_attention_mask,\n",
    "        ) in test_loader:\n",
    "            with torch.no_grad():\n",
    "                loss, outputs, hidden_states, _ = model(\n",
    "                    input_ids=batch_input_ids.to(device),\n",
    "                    token_type_ids=batch_segment_ids.to(device),\n",
    "                    masked_lm_labels=batch_masked_lm_labels.to(device),\n",
    "                    attention_mask=batch_attention_mask.to(device),\n",
    "                )\n",
    "                if batch_attention_mask.sum() - torch.numel(batch_attention_mask) > 0:\n",
    "                    print(\"found patting\", batch_attention_mask.sum())\n",
    "                total_eval_loss += loss.item()\n",
    "        avg_eval_loss = total_eval_loss / len(test_loader)\n",
    "        print(\n",
    "            \"Epoch:\",\n",
    "            \"%04d\" % (epoch + 1),\n",
    "            \"train cost =\",\n",
    "            \"{:.6f}\".format(avg_train_loss),\n",
    "            \"eval cost =\",\n",
    "            \"{:.6f}\".format(avg_eval_loss),\n",
    "        )\n",
    "\n",
    "average_train_acc, _ = evaluation.model_masked_label_accuracy(\n",
    "    model, train_loader, device\n",
    ")\n",
    "average_test_acc, last_test_attention = evaluation.model_masked_label_accuracy(\n",
    "    model, test_loader, device\n",
    ")\n",
    "print(\n",
    "    \"Train Acc =\",\n",
    "    \"{:.6f}\".format(average_train_acc),\n",
    "    \"Eval Acc =\",\n",
    "    \"{:.6f}\".format(average_test_acc),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using foundation model (5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Introduction\n",
    "\n",
    "In this section, we aim to use a foundation model, DNABERT, to perform promoter detection.\n",
    "A foundation model is a model pretrained on large datasets. Foundation models serve as the foundational building blocks upon which various applications can be constructed.\n",
    "\n",
    "Here, we use DNABERT as the foundation model. We first apply it on DNA sequence to get the embedding. Then, we train a classifier on the embedding as in Section 2. Please follow this [link](https://github.com/Zhihan1996/DNABERT_2) to load the foundation model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Implementation\n",
    "\n",
    "**Consider this situation:** You get a dataset about promoter detection, and you build your model to perform the task as in Section 2. However, the performance is not good since the model is not strong enough. Suddenly, you think we can use a large pre-trained model to embed DNA sequences. Then, you search online and find the pre-trained model [DNABERT](https://github.com/Zhihan1996/DNABERT_2). Now, you want to perform promoter detection using the pre-trained DNABERT.\n",
    "\n",
    "There is no coding framework in this section. Just make things work (get good test accuracy) using the pre-trained model!\n",
    "\n",
    "Hint: \n",
    "- We encourage you to create a **new environment** following the instructions of Section 3 in this [link](https://github.com/Zhihan1996/DNABERT_2). (When you face the error \"The model class you are passing has a config_class attribute that is not consistent with the config class you passed ...\", creating a new environment can save you.)\n",
    "- Section 4 in this [link](https://github.com/Zhihan1996/DNABERT_2) shows you how to load and use the pre-trained foundation model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Switch the environment at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from utils import data\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train and test datasets from train.csv and test.csv\n",
    "raw_training_data = data.load_csv(\"./data/train.csv\")\n",
    "raw_test_data = data.load_csv(\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Get the embeddings of the DNA sequences using pretrained model.\n",
    "\n",
    "Hint: \n",
    "- This step can take some time. Thus, you can start with a small sample size, and then increase it when you have made sure that everything works correctly.\n",
    "- After getting the embeddings, you can save them so that you can directly load them next time without running the foundation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: Load the pretrained DNABERT model and use this to get the embeddings of the train and test DNA sequences.\n",
    "# Load pre-trained DNABERT model and tokeniser\n",
    "tokeniser = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's familiarise ourselves with the API provided by the pre-trained `AutoTokenizer` and `AutoModel` class. We can pass a `str` into the `AutoTokenizer` to get the tokenised input indices and pass the tokenised input indices into the `AutoModel` to get the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dna = \"ACGTAGCATCGGATCTATCTATCGACACTTGGTTATCGATCTACGAGCATCTCGTTAGC\"\n",
    "input_ids = tokeniser(dna, return_tensors = 'pt')[\"input_ids\"] # (B, L)\n",
    "hidden_states, _ = model(input_ids) # (B, L, H)\n",
    "\n",
    "# Compute sequence embedding using mean pooling\n",
    "embedding = torch.mean(hidden_states[0], dim=0)\n",
    "\n",
    "print(f\"input_ids: {input_ids.shape}\")\n",
    "print(f\"hidden_states: {hidden_states.shape}\")\n",
    "print(f\"embedding: {embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can pass batched input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batched input to tokeniser\n",
    "input_ids = tokeniser([dna, dna], return_tensors = 'pt')[\"input_ids\"] # (B, L)\n",
    "hidden_states, _ = model(input_ids) # (B, L, H)\n",
    "\n",
    "# Compute sequence embedding using mean pooling\n",
    "embedding = torch.mean(hidden_states[0], dim=0)\n",
    "\n",
    "print(f\"input_ids: {input_ids.shape}\")\n",
    "print(f\"hidden_states: {hidden_states.shape}\")\n",
    "print(f\"embedding: {embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works as expected. We can now iterate over the DNA sequences in our training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path):\n",
    "    return torch.load(path)\n",
    "\n",
    "def save_embeddings(embeddings, path):\n",
    "    return torch.save(embeddings, path)\n",
    "\n",
    "def get_embeddings(model, tokeniser, sequences, batch_size = 128):\n",
    "    embeddings = []\n",
    "    for sequence in tqdm.tqdm(sequences):\n",
    "        input_ids = tokeniser(sequence, return_tensors = 'pt')[\"input_ids\"]\n",
    "        hidden_states, _ = model(input_ids) # (B, L, H)\n",
    "        embedding = torch.mean(hidden_states[0], dim=0)\n",
    "\n",
    "        embeddings.append(embedding.tolist()) # (B, H)\n",
    "\n",
    "    return torch.Tensor(embeddings)\n",
    "\n",
    "# Extract training and testing sequences\n",
    "# sub = 1000\n",
    "train_sequences = raw_training_data[\"sequence\"].tolist()\n",
    "test_sequences = raw_test_data[\"sequence\"].tolist()\n",
    "train_labels = raw_training_data[\"label\"].tolist()\n",
    "test_labels = raw_test_data[\"label\"].tolist()\n",
    "\n",
    "# Set paths for saving and loading embeddings\n",
    "data_path = os.path.join(os.getcwd(), \"data\")\n",
    "train_embeddings_path = os.path.join(data_path, \"train_embeddings.pt\")\n",
    "test_embeddings_path = os.path.join(data_path, \"test_embeddings.pt\")\n",
    "\n",
    "# Compute and save embeddings if they do not exist\n",
    "if not os.path.exists(train_embeddings_path) and not os.path.exists(test_embeddings_path):\n",
    "    train_embeddings = get_embeddings(model, tokeniser, train_sequences)\n",
    "    test_embeddings = get_embeddings(model, tokeniser, test_sequences)\n",
    "\n",
    "    save_embeddings(train_embeddings, train_embeddings_path)\n",
    "    save_embeddings(test_embeddings, test_embeddings_path)\n",
    "\n",
    "# Load embeddings from disk\n",
    "train_embeddings = load_embeddings(train_embeddings_path)\n",
    "test_embeddings = load_embeddings(test_embeddings_path)\n",
    "\n",
    "print(\"Loaded embeddings ✅.\")\n",
    "print(f\"train_embeddings: {train_embeddings.shape}\")\n",
    "print(f\"test_embeddings: {test_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo: Using tsne or umap to visualize the embedding space.\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from umap.umap_ import UMAP\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "# Hint: you can import other packages here for visualization.\n",
    "tsne = TSNE(n_components=2)\n",
    "umap = UMAP(n_components=2)\n",
    "\n",
    "# Use a subset for visualisation\n",
    "sub = 1000\n",
    "sub_train_embeddings = train_embeddings[:sub]\n",
    "sub_train_labels = train_labels[:sub]\n",
    "\n",
    "# Standardize embeddings for better visualisation\n",
    "scaler = StandardScaler()\n",
    "scaled_train_embeddings = scaler.fit_transform(sub_train_embeddings)\n",
    "\n",
    "# Compute t-SNE and UMAP embeddings\n",
    "tsne_train_embeddings = tsne.fit_transform(scaled_train_embeddings)\n",
    "umap_train_embeddings = umap.fit_transform(scaled_train_embeddings)\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(10, 5))\n",
    "sns.scatterplot(\n",
    "    x=tsne_train_embeddings[:, 0], \n",
    "    y=tsne_train_embeddings[:, 1],\n",
    "    hue=sub_train_labels,\n",
    "    s=50,\n",
    "    ax=axs[0]\n",
    "    );\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=umap_train_embeddings[:, 0], \n",
    "    y=umap_train_embeddings[:, 1],\n",
    "    hue=sub_train_labels,\n",
    "    s=50,\n",
    "    ax=axs[1]\n",
    "    );\n",
    "\n",
    "axs[0].set_title(\"t-SNE of DNABERT embeddings\");\n",
    "axs[1].set_title(\"UMAP of DNABERT embeddings\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Train a classifier.\n",
    "\n",
    "Hint: It is easy to overfit on the training set. Try to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \"\"\"Predicts the promoter sequence from the embedding of the sequence.\"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 hidden_dims = [256, 128, 64],\n",
    "                 activation = nn.ReLU(),\n",
    "                 dropout = 0.1):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialise linear layers\n",
    "        dims = [768] + hidden_dims + [1]\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(dims[i], dims[i+1]) for i in range(len(dims) - 1)\n",
    "        ])\n",
    "\n",
    "        # Initialise dropout and activation\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Feed forward function of the classification head. Returns the logits of the classification\n",
    "        of each sequence in the batch.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Tensor of sequence embeddings of shape (batch_size, embedding_size)\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, )\n",
    "        \"\"\"\n",
    "\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.activation(x)\n",
    "\n",
    "        x = self.layers[-1](x)\n",
    "\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data classes\n",
    "batch_size = 128\n",
    "\n",
    "# Create PyTorch dataset from embeddings and labels\n",
    "train_labels = torch.Tensor(raw_training_data[\"label\"].tolist())\n",
    "test_labels = torch.Tensor(raw_test_data[\"label\"].tolist())\n",
    "\n",
    "train_dataset = TensorDataset(train_embeddings, train_labels)\n",
    "test_dataset = TensorDataset(test_embeddings, test_labels)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "epochs = 10\n",
    "lr = 0.001\n",
    "\n",
    "# Initialise model and optimiser\n",
    "model = Classifier().to(device)\n",
    "optimiser = optim.AdamW(model.parameters(), lr=lr)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0.\n",
    "\n",
    "    for batch_embeddings, batch_labels in train_loader:\n",
    "        # Zero gradients\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # Move data to device\n",
    "        batch_embeddings = batch_embeddings.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(batch_embeddings)\n",
    "        loss = criterion(logits, batch_labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        model.eval()\n",
    "        total_eval_loss = 0.\n",
    "        for batch_embeddings, batch_labels in test_loader:\n",
    "            # Move data to device\n",
    "            batch_embeddings = batch_embeddings.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(batch_embeddings)\n",
    "            loss = criterion(logits, batch_labels)\n",
    "\n",
    "            total_eval_loss += loss.item()\n",
    "\n",
    "        avg_eval_loss = total_eval_loss / len(test_loader)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] | Train Loss {avg_train_loss:.4f} | Eval Loss {avg_eval_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance\n",
    "def validate(model, loader, device):\n",
    "    model.to(device)\n",
    "    y_true, y_pred = [], []\n",
    "    for batch_embeddings, batch_labels in loader:\n",
    "        # Move data to device\n",
    "        batch_embeddings = batch_embeddings.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(batch_embeddings)\n",
    "        probs = F.sigmoid(logits)\n",
    "        preds = torch.round(probs)\n",
    "\n",
    "        y_true.extend(batch_labels.tolist())\n",
    "        y_pred.extend(preds.tolist())\n",
    "\n",
    "    y_true = torch.Tensor(y_true)\n",
    "    y_pred = torch.Tensor(y_pred)\n",
    "\n",
    "    # Compute accuracy\n",
    "    acc = (preds == batch_labels).float().mean()\n",
    "\n",
    "    return {\n",
    "        \"acc\": acc\n",
    "    }\n",
    "\n",
    "train_results = validate(model, train_loader, device)\n",
    "test_results = validate(model, test_loader, device)\n",
    "\n",
    "print(f\"Train Acc: {train_results['acc']:.4f}\")\n",
    "print(f\"Test Acc: {test_results['acc']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "264px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
